### 1.认识微服务

随着互联网行业的发展，对服务的要求也越来越高，服务架构也从单体架构逐渐演变为现在流行的微服务架构。

#### 1.1 单体架构

`单体架构`：将业务的所有功能集中在一个项目中开发，打成一个包部署。

![image-20210713202807818](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903120529.png) 

单体架构的优缺点：

**优点：**

- 架构简单
- 部署成本低

**缺点：**

- 耦合度高(维护困难、升级困难)

#### 1.2 分布式架构

`分布式架构`：根据业务功能对系统做拆分，每个业务功能模块作为独立项目开发，称为一个服务。

![image-20210713203124797](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903120537.png) 

分布式架构的优缺点：

**优点：**

- 降低服务耦合
- 有利于服务升级和拓展

**缺点：**

- 服务调用关系错综复杂

分布式架构虽然降低了服务耦合，但是服务拆分时也有很多问题需要思考：

- 服务拆分的粒度如何界定？
- 服务之间如何调用？
- 服务的调用关系如何管理？

所以需要制定一套行之有效的标准来约束分布式架构。

#### 1.3 微服务架构

微服务的架构特征：

- `单一职责`：微服务拆分粒度更小，每一个服务都对应唯一的业务能力，做到单一职责；
- `自治`：团队独立、技术独立、数据独立，独立部署和交付；
- `面向服务`：服务提供统一标准的接口，与语言和技术无关；
- `隔离性强`：服务调用做好隔离、容错、降级，避免出现级联问题。

![image-20210713203753373](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903120550.png) 

微服务的上述特性其实是在给分布式架构制定一个标准，进一步降低服务之间的耦合度，提供服务的独立性和灵活性。做到高内聚，低耦合。

因此，可以认为**微服务**是一种经过良好架构设计的**分布式架构方案**。以下是微服务较为完整的架构图：

![微服务技术架构](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225113.jpg) 

但方案该怎么落地？微服务带来的各种问题怎么解决？选用什么样的技术栈？全球的互联网公司都在积极尝试自己的微服务落地方案，其中在Java领域最引人注目的就是SpringCloud提供的方案了，SpringCloud其实是一套解决微服务方案的技术整合。

#### 1.4 SpringCloud

[SpringCloud](https://spring.io/projects/spring-cloud)是目前国内使用最广泛的微服务框架，它集成了各种微服务功能组件，并基于SpringBoot实现了这些组件的自动装配，从而提供了良好的开箱即用体验。

其中常见的组件包括：

![image-20210713204155887](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903120601.png) 

SpringCloud底层是依赖于SpringBoot的，并且[官网](https://spring.io/projects/spring-cloud)也提供了版本的兼容关系，如下：

| Release Train                                                | Boot Version                          |
| :----------------------------------------------------------- | :------------------------------------ |
| [2020.0.x](https://github.com/spring-cloud/spring-cloud-release/wiki/Spring-Cloud-2020.0-Release-Notes) aka Ilford | 2.4.x, 2.5.x (Starting with 2020.0.3) |
| [Hoxton](https://github.com/spring-cloud/spring-cloud-release/wiki/Spring-Cloud-Hoxton-Release-Notes) | 2.2.x, 2.3.x (Starting with SR5)      |
| [Greenwich](https://github.com/spring-projects/spring-cloud/wiki/Spring-Cloud-Greenwich-Release-Notes) | 2.1.x                                 |
| [Finchley](https://github.com/spring-projects/spring-cloud/wiki/Spring-Cloud-Finchley-Release-Notes) | 2.0.x                                 |
| [Edgware](https://github.com/spring-projects/spring-cloud/wiki/Spring-Cloud-Edgware-Release-Notes) | 1.5.x                                 |
| [Dalston](https://github.com/spring-projects/spring-cloud/wiki/Spring-Cloud-Dalston-Release-Notes) | 1.5.x                                 |

#### 1.5 微服务技术对比

![image-20210903122239457](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903122252.png) 

#### 1.6 总结

- 单体架构：简单方便，高度耦合，扩展性差，适合小型项目。例如：学生管理系统
- 分布式架构：松耦合，扩展性好，但架构复杂，难度大。适合大型互联网项目，例如：京东、淘宝
- 微服务：一种良好的分布式架构方案
  - 优点：拆分粒度更小、服务更独立、耦合度更低
  - 缺点：架构非常复杂，运维、监控、部署难度提高
- SpringCloud是微服务架构的一站式解决方案，集成了各种优秀微服务功能组件

### 2.服务拆分和远程调用

任何分布式架构都离不开服务的拆分，微服务也是一样。

#### 2.1 服务拆分原则

微服务拆分时要遵循以下几个原则：

- 不同微服务，不要重复开发相同业务；
- 微服务数据独立，不要访问其它微服务的数据库；
- 微服务可以将自己的业务暴露为接口，供其它微服务调用。

![image-20210903162502053](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225130.png) 

#### 2.2 服务拆分示例

这里以微服务项目[cloud-parent](https://gitee.com/gongcqq/others/attach_files/821170/download/cloud-parent.zip)为例演示服务拆分，其结构如下：

![image-20210903214759456](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225153.png) 

`cloud-parent`：父工程，管理依赖。

- `order-service`：订单微服务，负责订单相关业务。
- `user-service`：用户微服务，负责用户相关业务。

要求：

- 订单微服务和用户微服务都必须有各自的数据库，相互独立；
- 订单服务和用户服务都对外暴露Restful的接口；
- 订单服务如果需要查询用户信息，只能调用用户服务的Restful接口，不能查询用户数据库。

##### 2.2.1 数据库设计

首先创建两个库，分别为**cloud_order**和**cloud_user**，然后不同库中执行以上`cloud-parent`项目中提供的不同sql脚本即可。

执行脚本后，cloud_user库中的t_user表的初始数据如下：

![image-20210903220430580](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225210.png) 

cloud_order库中的t_order表的初始数据如下：

![image-20210903220539317](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225215.png) 

> **说明**：t_order表中的user_id字段对应t_user表的id字段。

##### 2.2.2 导入微服务工程

首先将以上提供的代码导入到IDEA工程中，然后打开`Run Dashboard`面板，如下所示：

![image-20210903221334282](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225225.png) 

如果IDEA中选不到`Run Dashboard`面板(有的版本的IDEA是`Services`面板)，在项目的`.idea`目录的workspace.xml文件中添加以下内容后重启工程即可：

```xml
<option name="configurationTypes">
  <set>
    <option value="SpringBootApplicationConfigurationType" />
  </set>
</option>
```

![image-20210903222035404](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225235.png) 

生效后的`Run Dashboard`面板如下所示：

![image-20210903222418187](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225240.png) 

> **注意**：项目中用到的java的版本是jdk1.8。

#### 2.3 实现远程调用

订单模块的项目中是在启动类中使用`RestTemplate`类实现远程调用的，首先是使用如下代码将该类注入到Spring容器：

```java
@Bean
public RestTemplate restTemplate(){
    return new RestTemplate();
}
```

然后当订单模块的Service层获取订单信息的时候，使用如下方法通过远程调用将用户信息填充到Order对象中返回：

```java
@Override
public Order queryOrderById(Long orderId) {
    //1.查询订单
    Order order = orderMapper.findById(orderId);
    //2.利用RestTemplate发起http请求，查询用户
    //2.1 url路径
    String url = "http://localhost:8080/user/" + order.getUserId();
    //2.2 发送http请求，实现远程调用
    User user = restTemplate.getForObject(url, User.class);
    //3.封装user到Order中
    order.setUser(user);
    //4.返回
    return order;
}
```

在IDEA中分别启动订单模块和用户模块，然后浏览器中直接进行访问即可：

```http
http://localhost:8081/order/101
```

![image-20210903223928146](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225254.png) 

#### 2.4 提供者与消费者

在服务调用关系中，会有两个不同的角色：

`服务提供者`：一次业务中，被其它微服务调用的服务(提供接口给其它微服务)；

`服务消费者`：一次业务中，调用其它微服务的服务(调用其它微服务提供的接口)。

![image-20210903224606793](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225301.png) 

但是，服务提供者与服务消费者的角色并不是绝对的，而是相对于业务而言。

如果服务A调用了服务B，而服务B又调用了服务C，那么服务B的角色是什么？

- 对于A调用B的业务而言：A是服务消费者，B是服务提供者；
- 对于B调用C的业务而言：B是服务消费者，C是服务提供者。

因此，服务B既可以是服务提供者，也可以是服务消费者。

### 3.Eureka注册中心

#### 3.1 Eureka的结构

在上面远程调用的例子中，我们是把调用地址通过如下的一行代码写死在了代码中的：

```java
String url = "http://localhost:8080/user/" + order.getUserId();
```

直接写死就会有很多问题，比如，服务提供者如果是一个集群的话，那这个地址该写成什么合适？我们又如何实现远程调用呢？这时就需要用到注册中心了，我们可以先把各自的微服务注册到注册中心上，需要的时候通过注册中心完成远程调用即可，这一章节主要介绍的是Eureka注册中心的使用。

**Eureka注册中心的结构如下：**

![20210904200248](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904200602.jpg) 

**结构图解：**

1. user-service服务实例启动后，将自己的信息注册到eureka-server(Eureka服务端)，这个是`服务注册`；
2. eureka-server保存服务名称到服务实例地址列表的映射关系中，然后order-service根据服务名称，拉取实例地址列表，这个是`服务发现`或`服务拉取`；
3. 然后order-service从实例列表中利用`负载均衡`算法选中一个实例地址，并向该实例地址发起`远程调用`；
4. user-service会每隔一段时间(默认30秒)向eureka-server发起请求，报告自己的状态，称为**心跳**；当超过一定时间没有发送心跳时，eureka-server会认为对应的微服务实例出现了故障，便会将该实例从服务列表中剔除，这样一来，当order-service拉取服务时，就能将有故障的实例排除掉了。

> **说明**：一个微服务，既可以是服务提供者，又可以是服务消费者，因此eureka将服务注册、服务发现等功能统一封装到了eureka-client端。

#### 3.2 搭建Eureka服务端

Eureka服务端的搭建，需要创建一个独立的微服务，下面就在cloud-parent父工程下创建一个名为eureka-server的SpringBoot工程，目录结构如下所示：

![image-20210904204235259](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221633.png) 

然后我们需要在eureka-server工程对应的pom文件中引入以下依赖：

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>
</dependency>
```

之后我们需要在启动类中添加`@EnableEurekaServer`注解：

```java
package com.eureka;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;

@EnableEurekaServer
@SpringBootApplication
public class EurekaApplication {
    public static void main(String[] args) {
        SpringApplication.run(EurekaApplication.class, args);
    }
}
```

最后就是在application.yml文件中添加对应的配置，比如下面这样：

```yaml
server:
  port: 10086 # 服务端口
spring:
  application:
    name: eurekaserver # eureka的服务名称，自定义，不重复即可
eureka:
  client:
    service-url:  # eureka的地址信息
      defaultZone: http://localhost:10086/eureka
```

> **说明**：eureka自己也是一个微服务，所以eureka在启动的时候，会将自己也注册到eureka上，所以上面也定义了它自己的服务名称。

接下来就可以在eureka-server的启动类中启动eureka的服务端了，启动成功后直接在浏览器访问如下地址即可：

```http
http://localhost:10086/
```

![image-20210904210416023](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221642.png) 

#### 3.3 服务注册

接下来将我们的user-service服务和order-service服务都注册到Eureka上。

首先在各自工程的pom文件中都引入如下依赖：

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
</dependency>
```

然后在user-service工程的application.yml文件中添加如下内容：

```yaml
spring:
  application:
    name: userservice # user-service服务的服务名称
eureka:
  client:
    service-url:
      defaultZone: http://127.0.0.1:10086/eureka
```

而在order-service工程的application.yml文件中添加以下内容：

```yaml
spring:
  application:
    name: orderservice # order-service服务的服务名称
eureka:
  client:
    service-url:
      defaultZone: http://127.0.0.1:10086/eureka
```

然后就可以启动user-service服务以及order-service服务了，服务都启动成功后，我们再到eureka的服务端查看的时候就可以发现，已注册的服务名称中，除了eurekaserver(eureka自身服务的服务名)外，还包含了userservice和orderservice，说明我们自己的服务都已经成功注册到eureka上了。

![image-20210904214759080](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221649.png) 

我们也可以给自己的服务启多个实例，下面就以user-service服务为例，演示一下多实例的效果。本地同一个服务直接启动多个实例的话会端口冲突，所以我们需要变更一下端口，比如`-Dserver.port=8082`这样，步骤如下：

![image-20210904220144332](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221701.png) 

![image-20210904220643931](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221707.png) 

![image-20210904220851962](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221720.png) 

![image-20210904221046962](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221728.png) 

![image-20210904221431546](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221735.png) 

#### 3.4 服务拉取和负载均衡

在前面的步骤中，已经把user-service服务和order-service服务都注册到eureka注册中心了，接下来就从eureka注册中心拉取user-service服务的实例列表并实现负载均衡。

首先在order-service的OrderApplication类中，给RestTemplate这个Bean添加一个`@LoadBalanced`注解：

```java
package com.order;

import org.mybatis.spring.annotation.MapperScan;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.client.loadbalancer.LoadBalanced;
import org.springframework.context.annotation.Bean;
import org.springframework.web.client.RestTemplate;

@MapperScan("com.order.mapper")
@SpringBootApplication
public class OrderApplication {

    public static void main(String[] args) {
        SpringApplication.run(OrderApplication.class, args);
    }

    @Bean
    @LoadBalanced
    public RestTemplate restTemplate(){
        return new RestTemplate();
    }
}
```

然后修改`com.order.service.impl.OrderServiceImpl#queryOrderById`方法中调用user-service服务的地址，把之前的地址和端口的部分改成user-service服务注册到eureka上的服务名，即`userservice`，如下所示：

```java
package com.order.service.impl;

import com.order.mapper.OrderMapper;
import com.order.pojo.Order;
import com.order.pojo.User;
import com.order.service.OrderService;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;
import org.springframework.web.client.RestTemplate;

/**
 * @Author: gongsl
 * @Date: 2021-09-02 23:35
 */
@Service
@Transactional
public class OrderServiceImpl implements OrderService {

    @Autowired
    private OrderMapper orderMapper;

    @Autowired
    private RestTemplate restTemplate;

    @Override
    public Order queryOrderById(Long orderId) {
        Order order = orderMapper.findById(orderId);
        //String url = "http://localhost:8080/user/" + order.getUserId();
        String url = "http://userservice/user/" + order.getUserId();
        User user = restTemplate.getForObject(url, User.class);
        order.setUser(user);
        return order;
    }
}
```

完成以上修改后，重启order-service服务，然后在浏览器访问以下地址：

```http
http://localhost:8080/order/102
```

![image-20210904223942005](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904225750.png) 

通过以上结果可知，已经通过eureka注册中心获取到了用户信息，我们可以访问多次，来测试负载均衡的效果，我访问了六次后，以下是控制台日志打印的结果：

![image-20210904224427752](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904225757.png) 

![image-20210904224815466](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904225806.png) 

![image-20210904225013553](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904225816.png) 

> **说明**：启动两个user-service服务并且都注册到eureka上后，通过以上截图可知，当我们通过远程调用的方式去访问user-service服务的时候，每个user-service服务都有被访问到，说明确实实现了负载均衡。上面在进行访问测试的时候只访问了六次，我们也可以多访问几次进行测试，可以发现，默认使用的负载均衡策略是轮询策略。

### 4.Ribbon负载均衡

在上面3.4章节中，我们只是使用了一个`@LoadBalanced`注解，就实现了负载均衡的效果，这里面的原理是什么呢？我们如何改变负载均衡的策略呢？这一章节就是来回答这些问题的。

#### 4.1 负载均衡原理

SpringCloud的底层其实是利用了一个名为**Ribbon**的组件，来实现负载均衡功能的。

前面我们明明在代码中将调用用户模块的地址写成了类似`http://userservice/user/1`这种形式(其中`userservice`是user-service服务注册到eureka上的服务名称)，可是为什么还能轮询调到对应的user-service服务呢？下图就是负载均衡的原理图：

![image-20210713224517686](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135316.png) 

#### 4.2 源码分析

我们并没有通过具体的IP和端口，而是通过服务名就可以找到对应的user-service服务，主要依靠的是SpringCloud源码中的`org.springframework.cloud.client.loadbalancer.LoadBalancerInterceptor`类。

我们在代码中是使用如下`@LoadBalanced`注解的方式实现负载均衡的：

```java
@Bean
@LoadBalanced
public RestTemplate restTemplate(){
    return new RestTemplate();
}
```

当我们进行请求访问的时候，`LoadBalancerInterceptor`类就会对RestTemplate的请求进行拦截，然后从Eureka上根据服务名称获取服务列表，随后利用负载均衡算法得到真实的服务地址信息，替换服务名称。

现在我们在浏览器访问`http://localhost:8080/order/102`，然后通过打断点的方式进行源码跟踪：

##### 4.2.1 LoadBalancerInterceptor

首先在`LoadBalancerInterceptor`类的`intercept`方法上打断点，然后浏览器开始访问：

![image-20210905113104180](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135323.png) 

可以看到这里的intercept方法，已经拦截了用户的HttpRequest请求，并做了几件事：

- `request.getURI()`：获取请求的uri；
- `originalUri.getHost()`：获取url路径中的服务名称，比如userservice；
- `this.loadBalancer.execute()`：处理服务名称和用户请求。

这里的`this.loadBalancer`是`LoadBalancerClient`类型，我们继续跟入到`execute()`方法内部。

##### 4.2.2 RibbonLoadBalancerClient

接下来会进入到`org.springframework.cloud.netflix.ribbon.RibbonLoadBalancerClient`类中，该类是上面提到的`LoadBalancerClient`接口的实现类。

![image-20210905113828250](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135329.png) 

![image-20210905120003916](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135333.png) 

通过以上截图可知，这次访问获取到的是端口为8081的user-service服务，我们放行后再次访问，发现这次访问到的服务是端口为8082的了，说明确实实现了负载均衡。

![image-20210905120418054](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135338.png) 

##### 4.2.3 IRule

在上面截图的RibbonLoadBalancerClient类的execute方法中，可以发现，是使用如下两行代码来做负载均衡的：

```java
ILoadBalancer loadBalancer = getLoadBalancer(serviceId);
Server server = getServer(loadBalancer, hint);
```

那么我们就跟到`getServer()`方法中，看看到底使用的是哪种负载均衡策略：

![image-20210905121634843](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135345.png) 

![image-20210905121925079](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135352.png) 

然后我们会进入到`com.netflix.loadbalancer.BaseLoadBalancer`类的`chooseServer()`方法中，如下所示：

![image-20210905122603652](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135358.png) 

我们可以看下上图中成员变量rule的值，由下图可知，rule其实是`IRule`类型，默认值是`RoundRobinRule`对象。

![image-20210905123433641](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135405.png) 

如果我们打断点的话，可以发现，上面rule的值最终其实是`ZoneAvoidanceRule`对象，它和`RoundRobinRule`一样都是采用的轮询策略，具体它们有什么区别，下文会进行对比介绍。

##### 4.2.5 总结

SpringCloudRibbon的底层采用了一个拦截器，拦截了RestTemplate发出的请求，对地址做了修改。用一幅图来总结的话，就是下面这样：

![image-20210713224724673](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135435.png) 

#### 4.3 负载均衡策略

##### 4.3.1 负载均衡策略类型

负载均衡的规则都定义在了IRule接口中，而IRule有很多不同的实现类：

![image-20210905162919415](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905164155.png) 

不同策略类型的含义如下：

![image-20210905164005682](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905164200.png) 

> **说明**：这里默认的实现是`ZoneAvoidanceRule`，它是一种轮询方案。

##### 4.3.2 自定义使用的策略

通过定义IRule的实现可以修改负载均衡的规则，从而改变我们所使用的负载均衡的策略，主要有以下两种方式：

1. **代码的方式。**我们可以在order-service工程的OrderApplication类(或配置类)中，定义一个新的IRule：

   ```java
   @Bean
   public IRule randomRule(){
       //将负载均衡策略从默认的轮询改为随机
       return new RandomRule();
   }
   ```

2. **配置文件的方式。**我们可以在order-service工程的application.yml中针对指定服务配置指定负载均衡策略：

```yaml
userservice: # 给某个微服务配置负载均衡规则，这里是userservice服务
  ribbon:
    NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule # 负载均衡规则 
```

> **注意**：我们一般使用默认的负载均衡策略即可，确需修改的，上述两种方式是有根本区别的。第一种通过代码的方式是针对全局生效的，如果order-service服务需要调用多个不同微服务的话，第一种方式就会将调用所有微服务的负载均衡策略都改为随机策略；第二种方式则是指定服务设置策略，也就是说，如果调用多个不同的服务的话，按照第二种方式进行配置，除了调用user-service服务会使用随机策略外，调用其余服务都还是使用默认的轮询策略。

#### 4.4 饥饿加载

Ribbon默认是采用懒加载，即第一次访问时才会去创建LoadBalanceClient，所以第一次请求访问时间会较长。而饥饿加载则会在项目启动时就创建，这样可以降低第一次访问的耗时。如果有需要，我们可以通过以下方式开启饥饿加载：

```yaml
# 在order-service工程的application.yml中加入以下内容
ribbon:
  eager-load:
    enabled: true # 开启饥饿加载
    clients:  
      - userservice # 指定对userservice这个服务饥饿加载
```

> **说明**：截止到目前为止的代码可以点击[cloud-parent](https://gitee.com/gongcqq/others/attach_files/821855/download/cloud-parent.zip)进行下载。

### 5.Nacos的使用

#### 5.1 Nacos注册中心

[Nacos](https://nacos.io/zh-cn/)是阿里巴巴的产品，它是[SpringCloudAlibaba](https://spring.io/projects/spring-cloud-alibaba)中的一个组件，相比于[Eureka](https://github.com/Netflix/eureka)而言功能更加丰富，在国内受欢迎程度更高。

##### 5.1.1 Nacos简介

Nacos致力于帮助我们发现、配置和管理微服务。它提供了一组简单易用的特性集，帮助我们快速实现动态服务发现、服务配置、服务元数据及流量管理。

Nacos也帮助我们更敏捷和容易地构建、交付和管理微服务平台，它是构建以"服务"为中心的现代应用架构(例如微服务范式、云原生范式)的服务基础设施。

Nacos的关键特性主要包括以下几点:

- 服务发现和服务健康监测；
- 动态配置服务；
- 动态DNS服务；
- 服务及其元数据管理。

##### 5.1.2 Nacos的安装

为了更贴近真实场景，这里介绍的是Nacos在linux下的安装使用。

1. 下载安装包并解压：

   ```shell
   # 下载安装包(2.0以上版本可能还存在一些bug，所以暂时使用1.4.1版本)
   wget https://github.com/alibaba/nacos/releases/download/1.4.1/nacos-server-1.4.1.tar.gz
   
   # 解压安装包
   tar -zxvf nacos-server-1.4.1.tar.gz
   ```

   解压后目录结构如下：

   ```shell
   [root@master ~]# tree nacos
   nacos
   ├── bin
   │   ├── shutdown.cmd
   │   ├── shutdown.sh
   │   ├── startup.cmd
   │   └── startup.sh
   ├── conf
   │   ├── 1.4.0-ipv6_support-update.sql
   │   ├── application.properties
   │   ├── application.properties.example
   │   ├── cluster.conf.example
   │   ├── nacos-logback.xml
   │   ├── nacos-mysql.sql
   │   └── schema.sql
   ├── LICENSE
   ├── NOTICE
   └── target
       └── nacos-server.jar
   ```

2. 启动nacos：

   ```shell
   # standalone代表着单机模式运行，非集群模式
   cd nacos/bin/ && sh startup.sh -m standalone
   ```

   ![image-20210905201114733](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224343.png) 

   > **说明**：启动nacos需要依赖java环境，并且版本要求是jdk1.8及其以上。

3. 访问nacos控制台：

   ```shell
   # nacos默认端口是8848，所以浏览器输入以下地址(ip要换成自己的主机ip)进行访问即可
   http://192.168.68.11:8848/nacos
   ```

   进入到界面中后，输入用户名密码进行登录即可，默认用户名密码都是`nacos`。

   ![image-20210905201725368](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224354.png) 

4. 关闭nacos服务：

   ```shell
   # 如果我们想关闭nacos的话，直接执行以下命令即可
   sh shutdown.sh
   ```

> **说明**：如果想了解更多关于nacos的安装以及详细的使用说明，可以参考[nacos官网](https://nacos.io/zh-cn/docs/quick-start.html)。

##### 5.1.3 服务注册到Nacos上

1. 在cloud-parent父工程的pom文件中的`<dependencyManagement>`标签内引入SpringCloudAlibaba的依赖：

   ```xml
   <dependency>
       <groupId>com.alibaba.cloud</groupId>
       <artifactId>spring-cloud-alibaba-dependencies</artifactId>
       <version>2.2.6.RELEASE</version>
       <type>pom</type>
       <scope>import</scope>
   </dependency>
   ```

2. 然后在user-service和order-service中的pom文件中引入nacos-discovery依赖：

   ```xml
   <dependency>
       <groupId>com.alibaba.cloud</groupId>
       <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
   </dependency>
   ```

   > **说明**：由于我们已经用不到eureka了，所以我们可以注释掉eureka的依赖。

3. 在user-service和order-service的application.yml中添加nacos地址信息：

   ```yaml
   spring:
     cloud:
       nacos:
         server-addr: 192.168.68.11:8848 # nacos服务的地址和端口
   ```

4. 启动user-service服务和order-service服务，然后在nacos控制台的`服务管理`下的`服务列表`中就可以看到我们注册到nacos上的服务信息了： 

   ![image-20210905211727576](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224406.png)   

   ![image-20210905210421700](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224413.png) 

   > **说明**：这时我们可以在浏览器输入类似`http://localhost:8080/order/101`这样的地址进行远程调用，看看服务都注册到nacos上后，order-service是否还能调到user-service，我这边测试是可以的。

##### 5.1.4 服务分级存储模型

一个**服务**可以有多个**实例**，例如我们的user-service，可以有：

- 127.0.0.1:8081
- 127.0.0.1:8082
- 127.0.0.1:8083

假如这些实例分布于全国各地的不同机房，例如：

- 127.0.0.1:8081，在上海机房
- 127.0.0.1:8082，在上海机房
- 127.0.0.1:8083，在杭州机房

Nacos会将同一机房内的实例，划分为一个**集群**。

也就是说，user-service是服务，一个服务可以包含多个集群，如杭州、上海，每个集群下可以有多个实例，形成分级模型，如图：

<img src="https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224421.png" alt="image-20210713232522531" style="zoom: 50%;" /> 

微服务互相访问时，应该尽可能访问同集群实例，因为本地访问速度更快。当本集群内不可用时，才访问其它集群。例如：

<img src="https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224430.png" alt="image-20210713232658928" style="zoom: 60%;" /> 

> **说明**：杭州机房内的order-service应该优先访问同机房的user-service。

###### 5.1.4.1 给微服务配置集群

修改user-service的application.yml文件，添加集群配置：

```yaml
spring:
  cloud:
    nacos:
      server-addr: 192.168.68.11:8848
      discovery:
        cluster-name: HZ # 集群名称，自定义，不重复即可
```

重启两个user-service实例后，我们可以在nacos控制台看到下面的结果：

![image-20210905223846857](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224438.png)  

我们也可以配置多集群，可以再复制一个user-service启动配置，添加属性：

```shell
-Dserver.port=8083 -Dspring.cloud.nacos.discovery.cluster-name=SH
```

![image-20210905223533361](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224443.png) 

![image-20210905224157724](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224450.png) 

启动UserApplication3后再次查看nacos控制台：

![image-20210905223942762](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224458.png) 

###### 5.1.4.2 同集群优先的负载均衡

前面在讲Ribbon的负载均衡策略的时候，并没有根据同集群优先来实现负载均衡的策略。

因此Nacos中提供了一个`com.alibaba.cloud.nacos.ribbon.NacosRule`的实现，可以优先从同集群中挑选实例，下面就进行相关配置和演示。

1. 首先修改order-service的application.yml文件，添加集群配置：

   ```yaml
   spring:
     cloud:
       nacos:
         server-addr: 192.168.68.11:8848
         discovery:
           cluster-name: HZ # 这里将order-service添加到HZ集群
   ```

2. 再次修改order-service的application.yml文件，改变负载均衡的规则：

   ```yaml
   userservice:
     ribbon:
       NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule # 负载均衡规则 
   ```

3. 重启order-service服务进行访问测试：

   ![20210905230543](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905234326.gif) 

   由于我们将order-service添加到了HZ集群，并使用`NacosRule`作为负载均衡策略。所以正常情况下，当我们在浏览器访问order-service服务时，只会远程调用到同处于HZ集群的user-service服务，以上[动图](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905234326.gif)也确实印证了这一点。在获取到同集群的user-service服务列表后，会再通过随机负载均衡策略挑选一个服务进行远程调用。

   如果我们停掉同是HZ集群下的所有user-service服务，就会调用到SH集群下的user-service服务了，具体演示过程如以下[动图](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905235713.gif)：

   ![20210905235623](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905235713.gif) 

##### 5.1.5 Nacos的权重配置

服务器设备的性能是有差异的，不排除会出现部分实例所在机器性能较好，另一些较差的情况。有时我们会希望性能好的机器承担更多的用户请求，但是如果我们使用同集群优先的负载均衡策略，则默认情况下`NacosRule`是同集群内随机挑选服务器，这样就不会考虑到机器性能的问题。

因此，Nacos提供了权重配置来控制访问频率，权重越大则访问频率越高，我们可以的将性能较好的机器的权重配置高一些，以便更多地承担用户请求。

我们可以直接在nacos的控制台上进行实例的权重修改，权重的范围是`0~1`，如果将实例的权重设置为`0`，就相当于将该实例下线，则该实例将永远不会再承担任何用户请求。

下面进行修改权重的操作演示：

![image-20211021210151843](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211021211731.png) 

![image-20211021210913378](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211021211738.png) 

![image-20211021211054987](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211021211746.png) 

上述设置完成之后，我们可以在浏览器访问`http://localhost:8080/order/101`进行测试，可以明显发现，进入到权重值配置为`0.2`的实例的流量明显少于另一个权重值为`1`的实例，说明权重的配置是生效的。

##### 5.1.6 Nacos环境隔离

Nacos提供了namespace来实现环境隔离功能。

- nacos中可以有多个namespace；
- namespace下可以有group、service等；
- 不同namespace之间相互隔离，不同namespace的服务互相是不可见的。

![image-20211023160642391](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023160659.png) 

###### 5.1.6.1 创建namespace

默认情况下，所有service、data、group都在同一个namespace下，名为`public`：

![image-20211023161010870](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023162152.png) 

我们可以点击页面新增按钮，添加一个namespace：

![image-20211023161853795](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023162209.png) 

![image-20211023162049377](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023162216.png) 

![image-20211023162129556](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023162232.png) 

###### 5.1.6.2 给微服务配置namespace

给微服务配置namespace只能通过修改配置来实现。

例如，修改order-service的application.yml文件(增加`namespace`属性)：

```yaml
spring:
  cloud:
    nacos:
      server-addr: 192.168.68.11:8848
      discovery:
        cluster-name: HZ
        namespace: 21fa5899-e0e5-4263-915e-a4ec7959c497 # 命名空间，填页面上的命名空间ID
```

启动order-service服务以及user-service服务，访问Nacos控制台，会看到下面的结果：

![image-20211023164407539](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023171657.png) 

![image-20211023164459207](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023171702.png) 

![image-20211023164539651](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023171706.png) 

可以发现，order-service服务已经被归类到`dev`命名空间下了，而user-service服务还是在默认的`public`命名空间下，由于不同命名空间下的服务是不可见的，所以即便在`public`命名空间下有三个user-service服务，当我们在浏览器通过`http://localhost:8080/order/101`进行访问测试的时候，还是会因为order-service找不到user-service服务而报错，比如下面这样：

![image-20211023170616230](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023171712.png) 

![image-20211023170632944](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023171719.png) 

> **说明**：如果我们修改user-service服务的配置文件，将它也放到`dev`命名空间下的话，order-service服务就能访问到它了。

##### 5.1.7 Nacos的实例类型

Nacos的服务实例主要分为两种类型：

- `临时实例`：这种实例如果宕机超过一定时间，会被从服务列表中剔除，这个是默认的实例类型；
- `非临时实例`：这种实例如果宕机，不会被从服务列表剔除，也可以叫永久实例。

我们可以在实例详情中查看一个实例是否为临时实例，如下所示：

![image-20211023213118649](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023235224.png) 

我们也可以通过修改服务配置的方式将服务设置为非临时实例，比如要将order-service服务修改为非临时实例，可以修改该服务的application.yml配置，如下所示：

```yaml
spring:
  cloud:
    nacos:
      discovery:
        ephemeral: false # 设置为非临时实例
```

![image-20211023222144747](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023235231.png) 

##### 5.1.8 Nacos与Eureka的区别

Nacos和Eureka整体结构类似，服务注册、服务拉取、心跳等待，但是也存在一些差异：

![image-20211023223400549](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023235236.png) 

Nacos与eureka的共同点：

- 都支持服务注册和服务拉取；
- 都支持服务提供者心跳方式做健康检测。

Nacos与Eureka的区别：

- Nacos支持服务端主动检测提供者状态：临时实例采用心跳模式，非临时实例采用主动检测模式；
- 临时实例心跳不正常会被剔除，非临时实例则不会被剔除；
- Nacos支持服务列表变更的消息推送模式，服务列表更新更及时；
- Nacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式；Eureka采用AP方式。

#### 5.2 Nacos配置管理

Nacos除了可以做注册中心，同样可以用作配置管理。

##### 5.2.1 统一配置管理

当微服务部署的实例越来越多，达到数十、数百时，逐个修改微服务配置就会很麻烦，而且很容易出错。我们需要有一种统一的配置管理方案，可以集中管理所有实例的配置。

![image-20211024220612632](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211024220705.png) 

Nacos一方面可以将配置集中管理，另一方可以在配置变更时，及时通知微服务，实现配置的热更新。

###### 5.2.1.1 在nacos中添加配置文件

![image-20211024221117079](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211024223529.png) 

![image-20211024222035124](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211024223534.png) 

页面上配置的内容如下：

```yaml
pattern:
  dateformat: yyyy-MM-dd HH:mm:ss
```

> **注意**：并不是所有配置都应放在nacos上，像项目中可能会变动的配置以及需要热更新的配置才有放到nacos上进行管理的必要，一些像数据库的连接信息等基本不会变更的配置还是保存在微服务本地比较好。

###### 5.2.1.2 微服务拉取nacos上的配置

微服务需要拉取nacos中管理的配置，并且与本地的application.yml配置合并，才能完成项目的启动。但如果尚未读取本地的application.yml文件，我们又无法获取nacos的地址。所以这时候就需要用到`bootstrap.yml`文件了，这个文件是Spring引入的一种新的配置文件，其优先级高于application.yml。也就是说，项目在启动的时候，会优先加载该文件，所以我们只需将nacos的一些相关配置放到`bootstrap.yml`文件中即可。

![image-20211024223518936](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211024223545.png) 

1. 引入nacos-config依赖

   这里以user-service服务为例，首先在user-service服务中引入nacos-config的客户端依赖：

   ```xml
   <dependency>
       <groupId>com.alibaba.cloud</groupId>
       <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
   </dependency>
   ```

2. 在user-service项目的`resources`目录下添加一个bootstrap.yaml文件，内容如下：

   ```yaml
   spring:
     application:
       name: userservice # 服务名称，这里添加后，之前在application.yml中添加的就可以注释掉了
     profiles:
       active: dev # 开发环境，这里是dev
     cloud:
       nacos:
         server-addr: 192.168.68.11:8848 # 服务地址，记得注释掉在application.yml中重复添加的地址
         config:
           file-extension: yaml #对应nacos上配置文件的后缀名
   ```

   > **说明**：项目启动后，会根据以上服务名、环境以及后缀名去nacos上读取配置，在本例中，到nacos上读取到的配置名即为`userservice-dev.yaml`。

3. 通过代码方式验证是否成功读取到nacos上的配置：

   在user-service项目的com.user.controller.UserController类中添加如下业务逻辑：

   ```java
   @Value("${pattern.dateformat}")
   private String dateformat;
   
   @GetMapping("/now")
   public String now(){
       //根据指定格式对当前时间进行格式化处理
       log.info("日期格式：{}",dateformat);
       return LocalDateTime.now().format(DateTimeFormatter.ofPattern(dateformat));
   }
   ```

   ![image-20211024225108159](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211024225310.png) 

   启动项目后，浏览器访问`http://localhost:8081/user/now`进行验证： 

   ![image-20211024225818134](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211024225824.png) 

   ![image-20211024230120944](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211024230127.png) 

   > **说明**：通过以上浏览器访问到的结果可知，我们已经读取到naocs上的配置了，并将当前时间通过nacos上指定的配置进行了格式化展示。

##### 5.2.2 配置的热更新

修改nacos中的配置后，我们的服务无需重启即可让配置生效，这就是==配置的热更新==。

我们对配置的使用方式不同，会有不同的实现配置热更新的方式，主要有以下两种方式：

1. 使用`@Value`注解加`@RefreshScope`注解的方式；
2. 直接使用`@ConfigurationProperties`注解。

如果我们是通过`@Value`注解获取nacos配置管理中的某个配置，那么想要实现热更新的话，只需要在使用`@Value`注解所在的类上加上`@RefreshScope`注解即可，如下所示：

![image-20211026195236024](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211026195309.png) 

如果我们是在配置类中注入nacos配置管理中的配置的话，直接使用配置类的`@ConfigurationProperties`注解即可。下面创建一个配置类，其内容如下：

```java
package com.user.config;

import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.stereotype.Component;

/**
 * @Author: gongsl
 * @Date: 2021-10-26 20:01
 */
@Data
@Component
@ConfigurationProperties(prefix = "pattern")
public class ConfigProperties {
    
    private String dateformat;
}
```

然后测试类中直接使用`@Autowired`注解注入ConfigProperties类进行使用即可，比如下面这样：

```java
package com.user.controller;

import com.user.config.ConfigProperties;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.*;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;

/**
 * @Author: gongsl
 * @Date: 2021-09-02 22:13
 */
@Slf4j
@RestController
@RequestMapping("/user")
public class UserController {

    @Autowired
    private ConfigProperties properties;

    @GetMapping("/now")
    public String now(){
        //根据指定格式对当前时间进行格式化处理
        log.info("日期格式：{}",properties.getDateformat());
        DateTimeFormatter format = DateTimeFormatter.ofPattern(properties.getDateformat());
        return LocalDateTime.now().format(format);
    }
}
```

最后直接在浏览器进行访问测试就可以了。而且当我们修改nacos配置管理中的配置时，可以发现，我们不用重启自己的微服务就可以实现热更新，修改的配置是可以实时生效的。

##### 5.2.3 配置共享

###### 5.2.3.1 配置共享的案例演示

以下是在`bootstrap.yml`文件中的配置：

```yaml
spring:
  application:
    name: userservice
  profiles:
    active: dev
  cloud:
    nacos:
      server-addr: 192.168.68.11:8848
      config:
        file-extension: yaml
```

这么配置以后，当我们启动自己的服务时，不仅会读取nacos配置管理中`userservice-dev.yaml`文件里的配置，其实默认也会读取nacos配置管理中`userservice.yaml`文件里的配置，只不过我们没有在nacos上增加`userservice.yaml`文件，所以看不到有什么区别。

而`userservice.yaml`文件是不包含环境的，因此是可以被多个环境共享的，所以我们如果有什么配置是不同环境都会用到的，那就可以将配置放在`userservice.yaml`文件中。下面通过案例的方式来演示下配置共享。

1. 在nacos上增加一个环境共享的配置文件，这里是`userservice.yaml`文件，内容如下：

   ```yaml
   pattern:
     envSharedValue: 多环境共享属性值
   ```

   ![image-20211027115848780](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027115931.png) 

   ![image-20211027150730333](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027153648.png)  

2. 在项目中之前新增的`ConfigProperties`配置类里面加入上面新增的`envSharedValue`属性：

   ![image-20211027150638436](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027153655.png)  

   具体代码如下：

   ```java
   package com.user.config;
   
   import lombok.Data;
   import org.springframework.boot.context.properties.ConfigurationProperties;
   import org.springframework.stereotype.Component;
   
   @Data
   @Component
   @ConfigurationProperties(prefix = "pattern")
   public class ConfigProperties {
   
       private String dateformat;
       private String envSharedValue;
   }
   ```

3. 然后我们在测试类中新增一个方法以便进行测试，比如下面这个`envSharedTest`方法：

   ```java
   package com.user.controller;
   
   import com.user.config.ConfigProperties;
   import lombok.extern.slf4j.Slf4j;
   import org.springframework.beans.factory.annotation.Autowired;
   import org.springframework.web.bind.annotation.*;
   
   @Slf4j
   @RestController
   @RequestMapping("/user")
   public class UserController {
   
       @Autowired
       private ConfigProperties properties;
   
       @GetMapping("/test")
       public ConfigProperties envSharedTest(){
           return properties;
       }
   }
   ```

4. 接下来我们就可以启动user-service服务进行访问测试了，不过启动前还需要进行一些配置：

   为了验证`userservice.yaml`文件中的配置是可以在不同环境下共享的，我们将会启动两个user-service服务，端口为8081的服务使用的是==dev==环境，端口为8082的使用的是==test==环境。

   用目前的代码启动服务的话，默认使用的是dev环境，所以我们在使用8082端口启动服务的时候，需要通过IDEA的配置来改变环境，操作步骤如下：

   ![image-20211027144823842](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027153703.png)   

   ![image-20211027144617937](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027153719.png) 

5. 按照以上操作配置好之后，启动服务，然后浏览器进行访问测试即可：

   浏览器访问`http://localhost:8081/user/test`的结果如下：

   ![image-20211027151157208](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027153726.png)   

   **说明**：以上使用8081端口启动的服务使用的是dev环境，所以默认会读取nacos上的userservice-dev.yaml文件和userservice.yaml文件，所以==dateformat==字段和==envSharedValue==字段都能获取到对应配置中的值。

   当浏览器访问`http://localhost:8082/user/test`的时候，结果如下：

   ![image-20211027151232062](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027153733.png) 

   **说明**：以上使用8082端口启动的服务使用的是test环境，默认会去读取nacos上的userservice-test.yaml文件和userservice.yaml文件，但是nacos上我们并没有新增过userservice-test.yaml文件，所以这个文件是读取不到的。由于不会去读取userservice-dev.yaml文件，所以==dateformat==字段是获取不到值的，以上该字段的值为null是正常现象。由于也会读取userservice.yaml文件的内容，所以==envSharedValue==字段能获取到对应配置中的值。

###### 5.2.3.2 配置共享的优先级

当nacos中的配置和本地配置相同时，优先级顺序如下图所示：

![image-20211027152701337](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027153739.png) 

如果以user-service服务为例的话，优先级顺序就是下面这样：

```java
userservice-dev.yaml > userservice.yaml > application.yml
```

> **说明**：我们也可以在以上三个配置文件中都配置相同的属性及属性值，然后在代码的配置类中也增加该属性，之后浏览器进行访问测试，看看浏览器返回的该属性的属性值是哪个配置文件中的值就知道谁的优先级高了。另外，截止到目前为止的代码可以点击[cloud-parent](https://gitee.com/gongcqq/others/attach_files/863906/download/cloud-parent.zip)进行下载。

#### 5.3 Nacos集群的搭建

下面是官方给出的Nacos集群图：

![image-20210409210621117](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194542.png) 

其中包含3个nacos节点，然后一个负载均衡器代理3个nacos，这里负载均衡器我们可以使用nginx。正常而言，最起码的集群结构应该是下面这样：

![image-20210409211355037](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194548.png) 

但是为了方便起见，数据库就不再使用集群方式了，而且mysql、nginx和三个nacos都使用一台主机进行部署，这台主机的主机IP是`192.168.68.11`。为了避免端口冲突，三个nacos的端口将被分别设置为==8845==、==8846==和==8847==。然后下面就开始进行nacos的集群搭建。

##### 5.3.1 初始化数据库

Nacos默认数据存储在内嵌数据库Derby中，不属于生产可用的数据库，这里我们将nacos的数据存储到mysql数据库中。

登录到mysql数据库中后，我们首先新增一个库，这里起名为`nacos`，如下所示，可以看到我们新增的数据库：

```sql
mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| nacos              |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.00 sec)
```

然后我们在nacos库中执行初始化脚本即可。我们下载的nacos压缩包，解压后的`nacos/conf/`目录下就有这个初始化的脚本文件，文件名为`nacos-mysql.sql`，或者也可以通过如下命令进行获取，文件内容都是一样的：

```bash
wget https://gitee.com/gongcqq/others/raw/master/nacos-mysql.sql
```

执行完初始化脚本后，我们可以看到，nacos库下多了如下这些表：

```sql
mysql> show tables;
+----------------------+
| Tables_in_nacos      |
+----------------------+
| config_info          |
| config_info_aggr     |
| config_info_beta     |
| config_info_tag      |
| config_tags_relation |
| group_capacity       |
| his_config_info      |
| permissions          |
| roles                |
| tenant_capacity      |
| tenant_info          |
| users                |
+----------------------+
12 rows in set (0.00 sec)
```

##### 5.3.2 配置nacos集群

1. 使用如下命令下载并解压nacos：

   ```bash
   # 下载nacos
   wget https://github.com/alibaba/nacos/releases/download/1.4.1/nacos-server-1.4.1.tar.gz
   
   # 解压nacos
   tar -zxvf nacos-server-1.4.1.tar.gz
   ```

2. 重命名解压后文件中的集群配置文件：

   ```bash
   # 进入到配置文件目录中
   cd nacos/conf/
   
   # 将cluster.conf.example文件重命名为cluster.conf
   mv cluster.conf.example cluster.conf
   ```

3. 打开cluster.conf文件，将里面举例的主机集群配置删除掉，配置上自己的集群信息，即：

   ```xml
   192.168.68.11:8845
   192.168.68.11:8846
   192.168.68.11:8847
   ```

   ![image-20211027182647549](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194556.png) 

4. 然后修改application.properties文件，添加如下数据库配置：

   ```properties
   spring.datasource.platform=mysql
   
   db.num=1
   
   # 数据库的地址、用户名、密码要和自己的数据库保持一致
   db.url.0=jdbc:mysql://192.168.68.11:3306/nacos?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useUnicode=true&useSSL=false&serverTimezone=UTC
   db.user.0=root
   db.password.0=root
   ```

5. 接下来将解压的这个nacos再复制两份：

   ```bash
   # 由于前面已经进入到"nacos/conf/"目录下了，所以这里回到nacos的根目录
   cd ../../
   
   # 再复制两份nacos
   cp -r nacos nacos2 && cp -r nacos nacos3
   ```

6. 修改nacos的application.properties中`server.port`属性的值并启动：

   ```bash
   # 将第一个nacos的端口改为8845
   vim nacos/conf/application.properties
   
   # 将第二个nacos的端口改为8846
   vim nacos2/conf/application.properties
   
   # 将第三个nacos的端口改为8847
   vim nacos3/conf/application.properties
   
   # 端口修改完成之后，启动这三个nacos
   sh nacos/bin/startup.sh && sh nacos2/bin/startup.sh && sh nacos3/bin/startup.sh
   ```

7. 启动完成后，可以在浏览器分别访问如下地址，三个nacos都能访问到，说明都启动成功了：

   ```xml
   http://192.168.68.11:8845/nacos/
   http://192.168.68.11:8846/nacos/
   http://192.168.68.11:8847/nacos/
   ```

##### 5.3.3 配置nginx负载

1. 下载并解压nginx压缩包：

   ```bash
   # 下载nginx
   wget http://nginx.org/download/nginx-1.18.0.tar.gz
   
   # 解压nginx
   tar -zxvf nginx-1.18.0.tar.gz
   ```

2. 安装nginx：

   ```bash
   # 首先安装依赖包
   yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel
   
   # 然后依次执行以下命令
   cd nginx-1.18.0/ && ./configure
   make
   make install
   
   # 我这边防火墙已经关闭了，没有关闭的可以执行以下命令进行关闭
   systemctl stop firewalld
   ```

3. 安装nginx后的默认位置是`/usr/local/nginx/conf/`，下面修改nginx的`nginx.conf`文件，增加如下内容：

   ```xml
   upstream nacos-cluster {
       server 192.168.68.11:8845;
   	server 192.168.68.11:8846;
   	server 192.168.68.11:8847;
   }
   
   server {
       listen       80;
       server_name  localhost;
   
       location /nacos {
           proxy_pass http://nacos-cluster;
       }
   }
   ```

   ![image-20211027185846656](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115231833.png) 

4. 配置修改完成后，启动nginx：

   ```bash
   # 执行以下命令启动nginx
   /usr/local/nginx/sbin/nginx
   ```

5. 浏览器访问`http://192.168.68.11/nacos`看能不能跳转到nacos的页面，我这边是可以的，如下所示：

   ![image-20211027191302925](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194605.png) 

##### 5.3.4 修改本地配置

既然nacos已经采用集群的方式进行搭建了，那么本地工程中的`application.yml`配置或者`bootstrap.yml`配置中关于nacos的地址肯定就要进行修改了。我这边地址是配置在`bootstrap.yml`配置文件中的，之前是这么配置的：

```yaml
spring:
  application:
    name: userservice
  profiles:
    active: dev
  cloud:
    nacos:
      server-addr: 192.168.68.11:8848
      config:
        file-extension: yaml
```

现在nacos集群是使用nginx做负载的，所以以上`server-addr`属性的值要配置成nginx的地址，修改后的配置如下：

```yaml
spring:
  application:
    name: userservice
  profiles:
    active: dev
  cloud:
    nacos:
      server-addr: 192.168.68.11:80
      config:
        file-extension: yaml
```

修改好本地配置后直接启动userservice服务，然后浏览器通过集群地址访问nacos，观察服务注册情况：

![image-20211027192533801](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194611.png) 

![image-20211027192727861](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194616.png) 

##### 5.3.5 演示数据同步

我们也可以在配置列表中增加一项配置，看看配置是否同步到三个nacos中了，以及数据库中是否也有保存。

当我们使用`http://192.168.68.11/nacos`地址访问nacos页面后，在页面新增一个配置，名为`userservice.yaml`，内容如下：

```yaml
cluster: nginx
```

![image-20211027193332393](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194621.png) 

然后当我们访问以下任意一个地址时，该配置都会展示在`配置列表`中，说明配置确实是同步的：

```xml
http://192.168.68.11:8845/nacos/
http://192.168.68.11:8846/nacos/
http://192.168.68.11:8847/nacos/
```

![image-20211027193604733](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194627.png) 

![image-20211027193637908](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194640.png) 

![image-20211027193705168](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194648.png) 

我们到数据库中，也可以查到这条配置的配置信息：

```sql
mysql> select id,data_id,content,gmt_create,src_ip,type from config_info;         
+----+------------------+----------------+---------------------+---------------+------+
| id | data_id          | content        | gmt_create          | src_ip        | type |
+----+------------------+----------------+---------------------+---------------+------+
|  1 | userservice.yaml | cluster: nginx | 2021-10-27 11:28:28 | 192.168.68.11 | yaml |
+----+------------------+----------------+---------------------+---------------+------+
1 row in set (0.00 sec)
```

### 6.openFeign远程调用

以前使用RestTemplate进行远程调用时，代码是这样的：

![image-20211027222511417](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115231856.png) 

使用以上的方式进行远程调用会存在以下问题：

- 代码可读性差，编程体验不统一；
- 参数复杂URL难以维护。

而[openFeign](https://github.com/OpenFeign/feign)则是一个声明式的http客户端，其作用就是帮助我们优雅地实现http请求的发送，解决上面提到的问题。

#### 6.1 使用案例

1. 首先在order-service服务的pom文件中引入相关依赖：

   ```xml
   <dependency>
       <groupId>org.springframework.cloud</groupId>
       <artifactId>spring-cloud-starter-openfeign</artifactId>
   </dependency>
   ```

2. 然后在order-service的启动类上添加`@EnableFeignClients`注解以开启Feign的功能：

   ```java
   package com.order;
   
   import org.mybatis.spring.annotation.MapperScan;
   import org.springframework.boot.SpringApplication;
   import org.springframework.boot.autoconfigure.SpringBootApplication;
   import org.springframework.cloud.openfeign.EnableFeignClients;
   
   @EnableFeignClients
   @MapperScan("com.order.mapper")
   @SpringBootApplication
   public class OrderApplication {
   
       public static void main(String[] args) {
           SpringApplication.run(OrderApplication.class, args);
       }
   }
   ```

3. 之后我们在order-service中新建一个client包，并在该包下新增一个`UserClient`接口，作为Feign的客户端：

   ```java
   package com.order.client;
   
   import com.order.pojo.User;
   import org.springframework.cloud.openfeign.FeignClient;
   import org.springframework.web.bind.annotation.GetMapping;
   import org.springframework.web.bind.annotation.PathVariable;
   
   /**
    * @Author: gongsl
    * @Date: 2021-10-27 22:02
    */
   @FeignClient("userservice")
   public interface UserClient {
   
       @GetMapping("/user/{id}")
       User findByUserId(@PathVariable("id") Long id);
   }
   ```

   这个客户端主要基于SpringMVC的注解来声明远程调用的信息，比如：

   - `服务名称`：userservice

   - `请求方式`：GET
   - `请求路径`：/user/{id}
   - `请求参数`：id
   - `返回值类型`：User

   这样依赖，openFeign就可以帮助我们发送http请求了，就无需使用RestTemplate来发送了。

4. 接下来我们修改order-service中OrderServiceImpl类的queryOrderById方法内容，以便使用openFeign来实现远程调用：

   ```java
   package com.order.service.impl;
   
   import com.order.client.UserClient;
   import com.order.mapper.OrderMapper;
   import com.order.pojo.Order;
   import com.order.pojo.User;
   import com.order.service.OrderService;
   import org.springframework.beans.factory.annotation.Autowired;
   import org.springframework.stereotype.Service;
   import org.springframework.transaction.annotation.Transactional;
   
   /**
    * @Author: gongsl
    * @Date: 2021-09-02 23:35
    */
   @Service
   @Transactional
   public class OrderServiceImpl implements OrderService {
   
       @Autowired
       private OrderMapper orderMapper;
       
       @Autowired
       UserClient userClient;
   
       @Override
       public Order queryOrderById(Long orderId) {
           Order order = orderMapper.findById(orderId);
           User user = userClient.findByUserId(order.getUserId());
           order.setUser(user);
           return order;
       }
   }
   ```

   ![image-20211027224756563](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027235001.png) 

5. 最后启动order-service服务和user-service服务，之后浏览器输入`http://localhost:8080/order/101`进行访问测试：

   ![image-20211027225703397](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027235008.png) 

   ![image-20211027225800563](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027235015.png) 

   > **说明**：在openFeign中默认是集成了Ribbon的，所以如果有多个服务提供者的话，是会实现负载均衡的。

#### 6.2 自定义配置

openFeign可以支持很多的自定义配置，如下表所示：

| 类型                 | 作用             | 说明                                                         |
| -------------------- | ---------------- | ------------------------------------------------------------ |
| `feign.Logger.Level` | 修改日志级别     | 包含四种不同的级别：==NONE==、==BASIC==、==HEADERS==、==FULL== |
| feign.codec.Decoder  | 响应结果的解析器 | http远程调用的结果做解析，例如解析json字符串为java对象       |
| feign.codec.Encoder  | 请求参数编码     | 将请求参数编码，便于通过http请求发送                         |
| feign. Contract      | 支持的注解格式   | 默认是SpringMVC的注解                                        |
| feign. Retryer       | 失败重试机制     | 请求失败的重试机制，默认是没有，不过会使用Ribbon的重试       |

> **说明**：一般情况下，默认值就能满足我们的使用，如果要自定义时，代码方式的话，只需要创建自定义的@Bean覆盖默认Bean即可。

下面以修改日志级别为例来演示如何自定义配置，openFeign的日志级别主要分为四种：

- `NONE`：不记录任何日志信息，这是默认值；
- `BASIC`：仅记录请求的方法，URL以及响应状态码和执行时间；
- `HEADERS`：在BASIC的基础上，额外记录了请求和响应的头信息；
- `FULL`：记录所有请求和响应的明细，包括头信息、请求体、元数据。

##### 6.2.1 配置文件的方式

我们可以在order-service服务的application.yml文件中加入以下内容：

```yaml
feign:
  client:
    config:
      default: # 这里用default就是全局配置，如果是写服务名称，则是针对某个微服务的配置
        loggerLevel: FULL # 日志级别
```

假如我们只想针对userservice服务修改日志级别，可以写成下面这样：

```yaml
feign:  
  client:
    config: 
      userservice: # 针对某个微服务的配置
        loggerLevel: FULL # 日志级别
```

配置完成之后，启动项目，然后浏览器输入`http://localhost:8080/order/101`进行访问测试即可。如果控制台没有打印对应的日志，那么我们可以在order-service服务的application.yml文件中再加上如下配置：

```yaml
logging:
  level:
    com:
      order: debug # com.order是包路径
```

![image-20211027233735489](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027235026.png) 

##### 6.2.2 java代码的方式

我们先创建一个config包，然后创建一个类，最后再声明一个Logger.Level的对象：

```java
package com.order.config;

import feign.Logger;
import org.springframework.context.annotation.Bean;

/**
 * @Author: gongsl
 * @Date: 2021-10-27 23:39
 */
public class DefaultFeignConfiguration {
    @Bean
    public Logger.Level feignLogLevel(){
        // 日志级别设置为BASIC
        return Logger.Level.BASIC;
    }
}

```

如果要**全局生效**，就需要将其放到启动类的`@EnableFeignClients`这个注解中：

```java
@EnableFeignClients(defaultConfiguration = DefaultFeignConfiguration.class)
```

![image-20211027234541209](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027235032.png) 

如果只是要**局部生效**，则把它放到对应的`@FeignClient`这个注解中：

```java
@FeignClient(value = "userservice", configuration = DefaultFeignConfiguration .class) 
```

![image-20211027234738933](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027235036.png) 

如果java代码书写没问题，但控制台没有打印对应的日志，那么我们可以在order-service服务的application.yml文件中再加上如下配置：

```yaml
logging:
  level:
    com:
      order: debug # com.order是包路径
```

![image-20211027234926132](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027235042.png) 

#### 6.3 使用优化

openFeign底层发起http请求，是依赖于其它框架的，其底层客户端实现有以下三种选择：

- `URLConnection`：这个是默认实现，但不支持连接池；
- `Apache HttpClient`：支持连接池；
- `OKHttp`：支持连接池。

因此提高openFeign的性能主要手段就是使用**连接池**代替默认的`URLConnection`，这里我们用Apache的`HttpClient`来进行演示。

1. 在order-service中引入httpClient的依赖：

   ```xml
   <dependency>
       <groupId>io.github.openfeign</groupId>
       <artifactId>feign-httpclient</artifactId>
   </dependency>
   ```

2. 在order-service的application.yml中添加连接池等配置：

   ```yaml
   feign:
     httpclient:
       enabled: true # 开启feign对HttpClient的支持
       max-connections: 200 # 最大的连接数
       max-connections-per-route: 50 # 每个路径的最大连接数
   ```

3. 在`org.springframework.cloud.openfeign.FeignClientFactoryBean#loadBalance`上打断点，然后Debug方式启动order-service服务，可以发现，其底层client已经变成了Apache HttpClient：

   ![image-20211029221200734](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211029231533.png) 

#### 6.4 最佳实践

所谓最佳实践，就是使用过程中总结的经验，最好的一种使用方式。

仔细观察可以发现，openFeign的客户端与服务提供者的controller代码非常相似：

**UserClient：**

![image-20211029225507561](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211029231541.png) 

**UserController：**

![image-20211029225704610](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211029231545.png) 

那么有没有办法简化这种重复的代码编写呢？以下主要提供两种方法：

##### 6.4.1 继承方式

一样的代码可以通过继承来共享：

1. 定义一个API接口，利用定义方法，并基于SpringMVC注解做声明；
2. openFeign的客户端和Controller都集成该接口。

![image-20211029225946848](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211029231552.png)

**说明**：以上方式的优点是用简单的方式实现了代码共享， 但是也有明显的缺点，那就是服务提供方、服务消费方紧耦合，并且参数列表中的注解映射并不会继承，因此Controller中必须再次声明方法、参数列表、注解等。

##### 6.4.2 抽取方式

这种方式是指将openFeign的客户端抽取为独立模块，并把接口相关的实体类、openFeign相关的配置等都放到这个模块中，提供给所有服务消费者使用。例如，将UserClient、User、Feign的默认配置都抽取到一个`feign-api`工程中，打成包后，所有微服务引用该依赖包，即可直接使用。

![image-20211029230523851](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211029231925.png) 

以上方式可能会出现一个问题，以UserClient为例，抽取为独立工程后，包路径可能和之前未抽取的不一定，在这种情况下，order-service的`@EnableFeignClients`注解将无法扫描到`UserClient`，基于该问题，有以下两种解决方式：

**方式一：**

注解中指定指定要扫描的包路径：

```java
//假设UserClient的包路径是"cn.feign.clients"，order-service服务中的注解就可以这么写
@EnableFeignClients(basePackages = "cn.feign.clients")
```

**方式二：**

注解中指定需要加载的Client接口：

```java
//直接加载指定的接口，可以指定多个，如果只有一个，下面的大括号也可以省略
@EnableFeignClients(clients = {UserClient.class})
```

### 7.Gateway服务网关

Spring Cloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0以及 Project Reactor 等响应式编程和事件流技术开发的网关，它旨在为微服务架构提供一种简单有效的统一API路由管理方式。

#### 7.1 网关的作用

Gateway网关是我们服务的守门神，是所有微服务的统一入口，它的核心功能特性包括==请求路由==、==权限控制==和==限流==。

`权限控制`：网关作为微服务入口，需要校验用户是否有请求资格，如果没有则进行拦截。

`路由和负载均衡`：一切请求都必须先经过gateway网关，但网关不处理业务，而是根据某种规则，把请求转发到某个微服务，这个过程叫做路由。当然路由的目标服务有多个时，还需要做负载均衡。

`限流`：当请求流量过高时，在网关中按照下游的微服务能够接受的速度来放行请求，避免服务压力过大。

增加网关后的服务架构图：

![image-20211030161700586](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211031165353.png)

> **说明**：在SpringCloud中网关的实现包括`gateway`和`zuul`两种，Zuul是基于Servlet的实现，属于阻塞式编程。而SpringCloud Gateway则是基于Spring5中提供的WebFlux，属于响应式编程的实现，具备更好的性能。

#### 7.2 gateway快速入门

下面就演示下网关的基本路由功能。基本步骤如下：

- 创建SpringBoot工程gateway，引入网关依赖；
- 编写启动类；
- 编写基础配置和路由规则；
- 启动网关服务进行测试。

1. 创建gateway服务，引入依赖：

   创建一个子工程，名为**gateway**，然后在该工程的pom文件中引入以下依赖：

   ```xml
   <dependency>
       <groupId>org.springframework.cloud</groupId>
       <artifactId>spring-cloud-starter-gateway</artifactId>
   </dependency>
   
   <dependency>
       <groupId>com.alibaba.cloud</groupId>
       <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
   </dependency>
   ```

2. 编写启动类，内容如下：

   ```java
   package com.gateway;
   
   import org.springframework.boot.SpringApplication;
   import org.springframework.boot.autoconfigure.SpringBootApplication;
   
   /**
    * @Author: gongsl
    * @Date: 2021-10-30 16:32
    */
   @SpringBootApplication
   public class GatewayApplication {
   
       public static void main(String[] args) {
           SpringApplication.run(GatewayApplication.class, args);
       }
   }
   ```

3. 创建application.yml文件，编写基础配置和路由规则：

   ```yaml
   server:
     port: 10010 # 网关端口
   spring:
     application:
       name: gateway # 服务名称
     cloud:
       nacos:
         server-addr: 192.168.68.11:8848 # nacos地址
       gateway:
         routes: # 网关路由配置
           - id: user-service # 路由id，自定义，只要唯一即可
             # uri: http://127.0.0.1:8081 # 路由的目标地址，http就是固定地址
             uri: lb://userservice # 路由的目标地址，lb就是指负载均衡，后面跟服务名称
             predicates: # 路由断言，也就是判断请求是否符合路由规则的条件
               - Path=/user/** # 这个是按照路径匹配，只要以/user/开头就符合要求
   ```

   > **说明**：我们将符合`Path`规则的一切请求，都代理到`uri`参数指定的地址，在本例中，我们是将`/user/**`开头的请求，代理到`lb://userservice`，lb是指负载均衡，然后根据后面的服务名拉取服务列表，实现负载均衡。

4. 启动测试gateway服务和user-service服务进行测试：

   ![image-20211031145422931](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211031165402.png) 

   浏览器访问`http://localhost:10010/user/1`时，请求就会被转发到`http://userservice/user/1`这个url，因为符合`/user/**`规则，如下图所示：

   ![image-20211031163147942](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211031165410.png) 

**以上过程，整个访问的流程图如下所示：**

![image-20210714211742956](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211031165416.png) 

**总结：**

网关搭建步骤：

1. 创建项目，引入nacos服务发现和gateway依赖；

2. 配置application.yml，包括服务基本信息、nacos地址、路由。

路由配置包括：

1. `路由id`：路由的唯一标识；

2. `路由目标(uri)`：路由的目标地址，http代表固定地址，lb代表根据服务名负载均衡；

3. `路由断言(predicates)`：判断路由的规则；

4. `路由过滤器(filters)`：对请求或响应做处理。

> **说明**：路由过滤器上文没有介绍到，下面就重点来介绍下路由断言和路由过滤器的用法。

#### 7.3 断言工厂

我们在配置文件中写的断言规则只是字符串，这些字符串会被Predicate Factory读取并处理，然后转变为路由判断的条件，例如`Path=/user/**`是按照路径匹配的，这个规则是由以下这个类来处理的：

```java
org.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory
```

像上面这样的断言工厂在[SpringCloud Gateway](https://docs.spring.io/spring-cloud-gateway/docs/2.2.9.RELEASE/reference/html/#gateway-request-predicates-factories)中还有十几个:

![image-20211101212100655](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211101212220.png) 

#### 7.4 过滤器工厂

[GatewayFilter](https://docs.spring.io/spring-cloud-gateway/docs/2.2.9.RELEASE/reference/html/#gatewayfilter-factories)是网关中提供的一种过滤器，可以对进入网关的请求和微服务返回的响应做处理：

![image-20211101212350891](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211101212356.png) 

##### 7.4.1 路由过滤器的种类

Spring提供了31种不同的路由过滤器工厂，例如：

| 名称                 | 说明                         |
| :------------------- | :--------------------------- |
| AddRequestHeader     | 给当前请求添加一个请求头     |
| AddRequestParameter  | 给当前请求添加一个请求参数   |
| AddResponseHeader    | 给响应结果中添加一个响应头   |
| RemoveRequestHeader  | 移除当前请求的一个请求头     |
| RemoveResponseHeader | 从响应结果中移除有一个响应头 |
| RequestRateLimiter   | 限制请求的流量               |
| PrefixPath           | 给请求路径添加一个前缀       |
| SetStatus            | 设置响应的状态码             |

##### 7.4.2 案例演示

1. 修改gateway服务的application.yml文件，增加`filters`相关内容：

   ```yaml
   server:
     port: 10010
   spring:
     application:
       name: gateway
     cloud:
       nacos:
         server-addr: 192.168.68.11:8848
       gateway:
         routes:
           - id: user-service
             uri: lb://userservice
             predicates:
               - Path=/user/**
             filters:
               - AddRequestHeader=X-Request-red, blue
   ```

   ![image-20211102205906100](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211102205917.png) 

   > **说明**：当前过滤器是写在userservice路由下的，所以仅仅对访问userservice的请求有效。

2. 然后在user-service服务的`com.user.controller.UserController`文件中增加如下代码：

   ```java
   @GetMapping("/header")
       public String getHeader(@RequestHeader(value = "X-Request-red",
                                              required = false) String request) {
           return "X-Request-red："+request;
       }
   ```

   ![image-20211102180327030](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211102180715.png) 

3. 重启服务后，浏览器输入`http://localhost:10010/user/header`进行访问测试：

   ![image-20211102180429817](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211102180722.png) 

   > **说明**：通过浏览器返回的结果可知，请求头确实已经添加成功了，并且我们也可以成功地获取到其内容。

##### 7.4.3 默认过滤器

如果要对所有的路由都生效，则可以将过滤器工厂写到default下。格式如下：

```yaml
server:
  port: 10010
spring:
  application:
    name: gateway
  cloud:
    nacos:
      server-addr: 192.168.68.11:8848
    gateway:
      routes:
        - id: user-service
          uri: lb://userservice
          predicates:
            - Path=/user/**
      default-filters:
        - AddResponseHeader=X-Response-Default-Red, Default-Blue
```

![image-20211102205823811](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211102205937.png) 

#### 7.5 全局过滤器

上一节学习的过滤器，网关提供了31种，但每一种过滤器的作用都是固定的。如果我们希望拦截请求，做自己的业务逻辑则没办法实现。

##### 7.5.1 全局过滤器的作用

全局过滤器[GlobalFilter](https://docs.spring.io/spring-cloud-gateway/docs/2.2.9.RELEASE/reference/html/#global-filters)的作用也是处理一切进入网关的请求和微服务响应，与`GatewayFilter`的作用一样。区别在于GatewayFilter通过配置定义，处理逻辑是固定的，而`GlobalFilter`的逻辑需要自己写代码实现。

定义方式是实现GlobalFilter接口：

```java
public interface GlobalFilter {
    /**
     *  处理当前请求，有必要的话通过{@link GatewayFilterChain}将请求交给下一个过滤器处理
     *
     * @param exchange 请求上下文，里面可以获取Request、Response等信息
     * @param chain 用来把请求委托给下一个过滤器 
     * @return {@code Mono<Void>} 标示当前过滤器业务结束
     */
    Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain);
}
```

在GlobalFilter中编写自定义逻辑，可以实现下列功能：

- 登录状态判断；
- 权限校验；
- 请求限流等。

##### 7.5.2 自定义全局过滤器

下面通过自定义一个过滤器的方式来进行请求参数的校验，具体逻辑如下：

1. 在gateway工程中创建一个filters包，然后再在下面创建一个自定义的`MyDefaultFilter`类：

   ```java
   package com.gateway.filters;
   
   import org.springframework.cloud.gateway.filter.GatewayFilterChain;
   import org.springframework.cloud.gateway.filter.GlobalFilter;
   import org.springframework.core.annotation.Order;
   import org.springframework.http.HttpStatus;
   import org.springframework.stereotype.Component;
   import org.springframework.util.MultiValueMap;
   import org.springframework.web.server.ServerWebExchange;
   import reactor.core.publisher.Mono;
   
   /**
    * @Author: gongsl
    * @Date: 2021-11-02 21:21
    */
   @Order(-1)
   @Component
   public class MyDefaultFilter implements GlobalFilter {
   
       @Override
       public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
           //1.获取请求参数
           MultiValueMap<String, String> params = exchange.getRequest().getQueryParams();
           //2.获取指定参数的参数值
           String user = params.getFirst("user");
           //3.进行参数值的校验
           if ("admin".equals(user)) {
               //满足要求，放行
               return chain.filter(exchange);
           }
           //4.不满足要求的则要进行拦截
           //4.1 禁止访问，设置状态码
           exchange.getResponse().setStatusCode(HttpStatus.FORBIDDEN);
           //4.2 结束流程
           return exchange.getResponse().setComplete();
       }
   }
   
   ```

   > **说明**：如果想要自定义全局过滤器，只要继承`GlobalFilter`接口即可。另外，上面的`@Order`注解是用于设置过滤器的执行顺序的，其值越小，越先执行，我们也可以实现`org.springframework.core.Ordered`接口，通过里面的`getOrder`方法同样可以设置执行顺序，和使用注解的方式效果是一样的。

2. 然后直接重启项目后浏览器进行访问测试即可：

   ![image-20211102214232750](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211102220941.png) 

   ![image-20211102214257251](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211102220947.png) 

   > **说明**：由以上访问结果可知，只有当请求参数中包含`user=admin`这样的参数及其参数值时，才能访问成功，否则就会被拒绝访问，并且响应码也是我们设置的403。

##### 7.5.3 过滤器的执行顺序

当请求进入到网关后，可能会碰到三类过滤器：当前路由的过滤器、DefaultFilter以及GlobalFilter。

请求路由后，会将当前路由的过滤器、DefaultFilter以及GlobalFilter合并到一个过滤器链(集合)中，排序后依次执行每个过滤器：

![image-20211102215930728](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211102220954.png) 

具体的排序规则：

- 每一个过滤器都必须指定一个int类型的order值，**order值越小，优先级越高，执行顺序越靠前**；
- GlobalFilter通过实现Ordered接口，或者添加@Order注解来指定order值，是由我们自己指定的；
- 而当前路由的过滤器和DefaultFilter的order则是由Spring指定的，默认是按照声明顺序从1递增；
- 当过滤器的order值一样时，会按照`DefaultFilter > 当前路由的过滤器 > GlobalFilter`的顺序执行。

详细内容，可以查看源码：

源码中的`org.springframework.cloud.gateway.route.RouteDefinitionRouteLocator#getFilters()`方法是先加载defaultFilters，然后再加载某个route的filters，最后再合并。

而`org.springframework.cloud.gateway.handler.FilteringWebHandler#handle()`方法会加载全局过滤器，与前面的过滤器合并后根据order排序，然后组织过滤器链。

#### 7.6 跨域问题

##### 7.6.1 什么是跨域问题

跨域主要包括：

- ==域名不同==：比如`www.taobao.com` 和 `www.taobao.org`；
- ==域名相同，端口不同==：比如`localhost:8080`和`localhost:8081`。

跨域问题：浏览器禁止请求的发起者与服务端发生跨域ajax请求，请求被浏览器拦截的问题。

解决方案：**CORS**，不了解的也可以参考[相关博客](https://www.ruanyifeng.com/blog/2016/04/cors.html)。

##### 7.6.2 gateway的跨域解决方案

网关处理跨域同样采用的是CORS方案，我们只需在gateway服务的application.yml文件中，添加下面的配置即可：

```yaml
spring:
  cloud:
    gateway:
      # ...
      globalcors: # 全局的跨域处理
        add-to-simple-url-handler-mapping: true # 解决options请求被拦截问题
        corsConfigurations:
          '[/**]': # 对哪些请求进行跨域处理，这里是指一切请求
            allowedOrigins: # 允许哪些网站的跨域请求 
              - "http://localhost:8090"
            allowedMethods: # 允许的跨域ajax的请求方式
              - "GET"
              - "POST"
              - "DELETE"
              - "PUT"
              - "OPTIONS"
            allowedHeaders: "*" # 允许在请求中携带的头信息
            allowCredentials: true # 是否允许携带cookie
            maxAge: 360000 # 指这次跨域检测的有效期，有效期范围内符合要求的，浏览器将不再询问，直接放行
```

> **说明**：截止到目前为止的代码可以点击[cloud-parent](https://gitee.com/gongcqq/others/attach_files/869761/download/cloud-parent.zip)进行下载。

### 8.Sentinel的使用

#### 8.1 雪崩问题及解决方案

在微服务架构中，会频繁进行服务之间的调用。一旦某个被调用方服务出现了故障，高并发情况下，由于资源无法及时得到释放，将会很快耗尽上游调用方的tomcat资源，从而拖垮上游服务，依次类推，整个微服务链路都将受到影响，从而出现雪崩问题，如下图所示：

![image-20211103180337537](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180350.png)  

![image-20211103180321223](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180357.png)   

> 微服务调用链路中的某个服务故障，引起整个链路中的所有微服务都不可用，这就是`雪崩`。

解决雪崩问题的常见方式有四种：

- `超时处理`：设定超时时间，请求超过一定时间没有响应就返回错误信息，不会无休止地等待；

- `舱壁模式`：限定每个业务能使用的线程数，避免耗尽整个tomcat的资源，因此也叫线程隔离；
- `熔断降级`：由**断路器**统计业务执行的异常比例，如果超出阈值则会**熔断**该业务，拦截访问该业务的一切请求；
- `流量控制`：限制业务访问的QPS，避免服务因流量的突增而故障。

**超时处理**可以在一定程度上有效缓解服务压力，但并不能完全解决雪崩问题。比如下图，我们可以在进行服务调用时设置一个1秒的等待时间，超过这个时间没返回就结束调用，以便尽快释放线程资源，不至于因为服务C的故障拖垮服务A，但是如果1秒内并发量很大的话，之前的线程资源还没来得及释放，就又会占用大量新的资源，这样还是可能导致tomcat的资源被耗尽，从而出现雪崩问题。

![image-20211103130502401](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180406.png) 

**舱壁模式**使用的是线程隔离的方法，就是给每个业务限制线程数，这样一来，即便某个业务对应的下游服务出现了故障，也不会将自己拖垮，这种方式可以避免雪崩问题的发生。但是也可能会造成资源的浪费，毕竟即便某个业务对应的下游服务已经故障了，业务也不再去调用了，但是该业务还是占用着自身服务的线程数，这些空闲的线程也无法被别的业务所使用。

![image-20211103150151889](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180411.png) 

**熔断降级**的方式要比舱壁模式好一些，这种方式是统计业务的异常比例，超出指定阈值就拦截该业务的一切请求，也不会造成线程数的浪费。

![image-20211103152829683](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180421.png) 

**流量控制**属于一种预防手段，通过限制业务量的方式，避免因为业务量过高而使服务发生故障。

![image-20211103153404212](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180434.png) 

目前业界知名的`Sentinel`和`Hystrix`都包含了以上提到的解决方案，但是它们俩也是有区别的，具体区别如下：

![image-20211103153949743](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180443.png) 

> 通过以上区别对比可知，`Sentinel`相对优秀一些，所以下文将会对`Sentinel`进行详细讲解。

#### 8.2 Sentinel的简介

##### 8.2.1 认识Sentinel

随着微服务的流行，服务和服务之间的稳定性变得越来越重要。[Sentinel](https://sentinelguard.io/zh-cn/index.html)是面向分布式服务架构的流量控制组件，主要以流量为切入点，从流量控制、熔断降级、系统自适应保护等多个维度来帮助我们保障微服务的稳定性，它具有以下特征：

- `丰富的应用场景`：Sentinel承接了阿里巴巴近10年的双十一大促流量的核心场景，例如秒杀(即突发流量控制在系统容量可以承受的范围)、消息削峰填谷、集群流量控制、实时熔断下游不可用应用等；
- `完备的实时监控`：Sentinel同时提供实时的监控功能，我们可以在控制台中看到接入应用的单台机器秒级数据，甚至500台以下规模的集群的汇总运行情况；
- `广泛的开源生态`：Sentinel提供开箱即用的与其它开源框架/库的整合模块，例如与Spring Cloud、Dubbo、gRPC的整合，我们只需要引入相应的依赖并进行简单的配置即可快速地接入Sentinel；
- `完善的SPI扩展点`：Sentinel同时还提供了简单易用且完善的SPI扩展接口，我们可以通过实现扩展接口来快速地定制逻辑，例如定制规则管理、适配动态数据源等。

##### 8.2.2 Sentinel的功能和设计理念

###### 8.2.2.1 流量控制

流量控制在网络传输中是一个常用的概念，它用于调整网络包的发送数据。然而，从系统稳定性角度考虑，在处理请求的速度上，也有非常多的讲究。任意时间到来的请求往往是随机不可控的，而系统的处理能力是有限的。我们需要根据系统的处理能力对流量进行控制。Sentinel作为一个调配器，可以根据需要把随机的请求调整成合适的形状，如下图所示：

![image-20211103161757517](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180451.png) 

流量控制有以下几个角度:

- 资源的调用关系，例如资源的调用链路，资源和资源之间的关系；
- 运行指标，例如 QPS、线程池、系统负载等；
- 控制的效果，例如直接限流、冷启动、排队等。

Sentinel的设计理念是让我们自由选择控制的角度，并进行灵活组合，从而达到想要的效果。

###### 8.2.2.2 熔断降级

**什么是熔断降级**

除了流量控制以外，降低调用链路中的不稳定资源也是Sentinel的使命之一。由于调用关系的复杂性，如果调用链路中的某个资源出现了不稳定，最终会导致请求发生堆积。

![image-20211103162319901](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180500.png) 

Sentinel和Hystrix的原则是一致的：当调用链路中某个资源出现不稳定，例如，表现为timeout，异常比例升高的时候，则对这个资源的调用进行限制，并让请求快速失败，避免因影响到其它的资源，最终产生雪崩的效果。

**熔断降级的设计理念**

在限制的手段上，Sentinel采取了和Hystrix完全不一样的方法。

Hystrix通过[线程池](https://github.com/Netflix/Hystrix/wiki/How-it-Works#benefits-of-thread-pools)的方式，来对资源进行了隔离。这样做的好处是资源和资源之间做到了最彻底的隔离。缺点是除了增加了线程切换的成本，还需要预先给各个资源做线程池大小的分配。

Sentinel对这个问题采取了两种手段：

- 通过并发线程数进行限制：

  和资源池隔离的方法不同，Sentinel通过限制资源并发线程的数量，来减少不稳定资源对其它资源的影响。这样不但没有线程切换的损耗，也不需要预先分配线程池的大小。当某个资源出现不稳定的情况下，例如响应时间变长，对资源的直接影响就是会造成线程数的逐步堆积。当线程数在特定资源上堆积到一定的数量之后，对该资源的新请求就会被拒绝，堆积的线程完成任务后才开始继续接收请求。

- 通过响应时间对资源进行降级：

  除了对并发线程数进行控制以外，Sentinel还可以通过响应时间来快速降级不稳定的资源。当依赖的资源出现响应时间过长后，所有对该资源的访问都会被直接拒绝，直到过了指定的时间窗口之后才重新恢复。

###### 8.2.2.3 系统负载保护

Sentinel同时提供[系统维度的自适应保护能力](https://sentinelguard.io/zh-cn/docs/system-adaptive-protection.html)。防止雪崩，是系统防护中重要的一环。当系统负载较高的时候，如果还持续让请求进入，可能会导致系统崩溃，无法响应。在集群环境下，网络负载均衡会把本应这台机器承载的流量转发到其它的机器上去。如果这个时候其它的机器也处在一个边缘状态，那么这个增加的流量就会导致这台机器也崩溃，最后导致整个集群不可用。

针对这种情况，Sentinel提供了对应的保护机制，让系统的入口流量和系统的负载达到一个平衡，保证系统在能力范围之内处理最多的请求。

#### 8.3 安装和使用Sentinel

为了方便我们操作，Sentinel官方提供了UI控制台，我们可以直接到[GitHub](https://github.com/alibaba/Sentinel/releases)上下载相关jar包，如果网速不行，也可以使用[备用地址](https://gongsl.lanzoui.com/izSNMw3kq0b)进行下载。然后直接使用`java -jar sentinel-dashboard-1.8.1.jar`命令进行启动即可，默认使用的端口是8080，控制台默认用户名和密码都是==sentinel==。

![image-20211103172557041](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180510.png) 

![image-20211103172746259](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180514.png) 

![image-20211103172834899](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180521.png) 

启动控制台后，下面在我们的微服务中引入Sentinel的相关依赖。在order-service工程的pom文件中加入以下内容：

```xml
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>
</dependency>
```

然后我们再在order-service工程的application.yml文件中配置Sentinel的控制台地址：

```yaml
spring:
  cloud:
    sentinel:
      transport:
        dashboard: 192.168.68.11:8080
```

以上配置完成之后，启动order-service工程和user-service工程，然后调用order-service的任意接口，以便触发Sentinel监控，比如多次调用`http://localhost:8080/order/101`接口，然后到Sentinel控制台查看监控情况：

![image-20211103175520668](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180533.png) 

#### 8.4 限流规则

##### 8.4.1 快速入门

在sentinel的控制台界面上，有一个`簇点链路`的菜单，这个链路其实就是项目内的调用链路，链路中**被监视**的每个接口就是一个资源。默认情况下sentinel会监控SpringMVC的每一个端点(Endpoint)，其实就是我们代码中controller层里面每个使用了`@RequestMapping`注解并设置了调用地址的方法。SpringMVC的每一个端点都对应着调用链路中的一个资源，流控、熔断等都是**针对簇点链路中的资源**来设置的，因此我们可以点击对应资源后面的按钮来设置规则：

![image-20211104150121681](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104150627.png) 

我们以名为`/order/{orderId}`的资源为例，点击操作中的**流控**按钮就会弹出一个表单，我们可以在表单中对该资源添加流控规则，如下图所示：

![image-20211104152228956](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165609.png) 

![image-20211104152330201](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165616.png)  

> **说明**：以上规则的含义是，限制`/order/{orderId}`这个资源的单机QPS为5，即每秒只允许5次请求，超出的请求会被拦截并报错。

然后我们利用jmeter压测工具访问`http://localhost:8080/order/101`这个地址进行测试，线程属性配置如下，两秒内访问20次，即每秒访问10次。

![image-20211104153007486](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165627.png) 

测试结果如下所示：

![image-20211104153702299](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165634.png) 

> 可以发现，每秒只有前五次是访问成功的，后五次都是失败的，由失败原因可知，是被sentinel限流了。

##### 8.4.2 流控模式

在添加限流规则时，点击高级选项，可以选择三种流控模式：

- `直接`：统计当前资源的请求，触发阈值时对当前资源直接限流，也是默认的模式；
- `关联`：统计与当前资源相关的另一个资源，触发阈值时，对当前资源限流；
- `链路`：统计从指定链路访问到本资源的请求，触发阈值时，对指定链路限流。

![image-20211104154520470](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165642.png) 

###### 流控模式-直接

这是一种默认的流控模式，该模式的效果在快速入门的时候已经演示过了，所以这里就不再赘述了。下面主要演示`关联`模式和`链路`模式的效果。

###### 流控模式-关联

**使用场景**：比如用户支付时需要修改订单状态，同时用户要查询订单。查询和修改操作会争抢数据库锁，产生竞争。业务需求是修改业务为主，因此当修改订单业务触发阈值时，需要对查询订单业务限流。

为了方便演示，下面在`OrderController`类中新增如下内容：

```java
//查询操作
@GetMapping("/select")
public String select() {
    return "select method!";
}

//修改操作
@GetMapping("/update")
public String update() {
    return "update method!";
}
```

重启order-service服务后依次访问一下查询操作和修改操作的url，以便触发sentinel监控，然后在sentinel控制台配置限流规则。由于是要对查询业务限流，所以就对该资源进行限流规则的配置：

![image-20211104161455950](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165649.png) 

![image-20211104161605316](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165654.png) 

![image-20211104161720754](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165701.png) 

> **说明**：以上将`/order/select`资源的阈值设置为5的含义是，当对`/order/update`的访问触发阈值(即每秒有超过五个以上请求进行访问)时，就会对`/order/select`资源进行限流。

然后我们利用jmeter压测工具访问`http://localhost:8080/order/update`这个地址进行测试，线程属性配置如下，即每秒访问十次，这个QPS肯定已经超过了`/order/select`资源的阈值。

![image-20211104163614543](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165706.png) 

线程属性配置完成之后，启动访问，此时`/order/select`资源的访问都是没有问题的：

![image-20211104164011811](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165713.png) 

在持续访问`/order/update`资源的时候，我们再在浏览器访问`http://localhost:8080/order/select`，由于修改操作的QPS已经超过了对查询操作设置的阈值，所以当我们访问查询操作的时候，是会被限流的，如下所示：

![image-20211104164649737](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165718.png) 

> **说明**：如果存在两个有竞争关系的资源，并且一个优先级高，一个优先级低，那么在限流的时候就可以使用关联模式。

###### 流控模式-链路

**使用场景**：比如有查询订单和创建订单业务，两者都需要查询商品。实际场景下，查询订单的请求肯定远高于创建订单的请求，如果查询订单的并发过高，肯定会影响到创建订单的业务。所以我们可以针对从查询订单进入到查询商品的请求做一个统计，并进行限流，而从创建订单进入到查询商品的请求不做限制。

为了方便演示，我们在order-service工程中加入以下代码：

- 在`OrderService`接口中增加如下代码：

  ```java
  void queryGoods();
  ```

- 在`OrderServiceImpl`类中增加如下代码：

  ```java
  @Override
  @SentinelResource("goods")
  public void queryGoods() {
      System.err.println(">>>查询商品...");
  }
  ```

  > **说明**：sentinel默认只会监控controller层中的端点，如果想让它也监控Service层的端点的话，就需要用到上述`@SentinelResource`注解才行，注解中的值是资源名。

- 在`OrderController`类中增加如下代码：

  ```java
  //查询订单
  @GetMapping("/query")
  public String query() {
      orderService.queryGoods();
      return "query order!";
  }
  
  //创建订单
  @GetMapping("/save")
  public String save() {
      orderService.queryGoods();
      return "save order!";
  }
  ```

- 由于sentinel默认会将Controller方法做context整合，导致链路模式的流控失效，所以我们需要在配置文件中关闭这个整合，以便上述在Service层添加的@SentinelResource生效。修改application.yml，添加如下配置：

  ```yaml
  spring:
    cloud:
      sentinel:
        web-context-unify: false # 关闭context整合
  ```

添加完以上代码配置后，重启order-service服务，然后依次访问一次查询订单和创建订单的url，以便触发sentinel监控，之后就可以对goods资源配置流控规则了，如下所示：

![image-20211104175603739](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104194250.png) 

![image-20211104180625860](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104194258.png) 

![image-20211104180727848](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104194306.png) 

然后我们利用jmeter压测工具同时访问以下两个url：

```http
http://localhost:8080/order/query
http://localhost:8080/order/save
```

然后线程属性配置都像下面这样，即每秒4个请求，都是超过了goods资源的阈值了的，同时进行访问测试，观察看看是否只会对入口为查询订单的请求做限流。

![image-20211104181111872](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104194316.png) 

测试结果如下，对于创建订单的请求，没有做任何限流操作，都是成功的。

![image-20211104182923477](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104194320.png) 

但是对于查询订单的请求，由于goods资源的阈值配置的是2，而查询订单请求的QPS为4，所以总是有一半的请求被限流。

![image-20211104182814367](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104194326.png) 

##### 8.4.3 流控效果

**流控效果**是指请求达到流控阈值时应该采取的措施，包括以下三种：

- `快速失败`：达到阈值后，新的请求会被立即拒绝并抛出FlowException异常，这个是默认的处理方式；
- `Warm Up`：预热模式，对超出阈值的请求同样是拒绝并抛出异常。但这种模式阈值会动态变化，从一个较小值逐渐增加到最大阈值；
- `排队等待`：让所有的请求按照先后次序排队执行，而且两个请求的间隔不能小于指定时长。

![image-20211104195251671](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104195303.png) 

###### 流控效果-快速失败

这是一种默认的流控效果，前面在进行限流规则配置时，流控效果都是使用的这种效果，所以这里就不再赘述了，下面主要演示`Warm Up`和`排队等待`这两种流控效果。

###### 流控效果-Warm Up

Warm Up也叫预热模式，是应对服务冷启动的一种方案。该效果请求阈值的初始值是==threshold / coldFactor==，持续到指定时长后，逐渐提高到threshold值，而coldFactor的默认值是3。

比如，我们设置QPS的threshold值为10，预热时间为5秒，那么初始阈值就是10/3，然后在5秒后逐渐增长到10。

![image-20211104200535498](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104203927.png) 

下面就给`/order/{orderId}`这个资源设置限流，阈值设置为10，然后利用warm up效果，预热时长为5秒，配置如下：

![image-20211104202443277](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104203935.png) 

![image-20211104202517902](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104203945.png) 

然后在jmeter中访问`http://localhost:8080/order/101`这个地址，线程属性配置如下：

![image-20211104202710860](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104203951.png) 

测试结果是，前五秒差不多只有三个是成功后，但是五秒后就都是成功的啦，说明达到预热效果了。

![image-20211104203114142](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104203957.png) 

![image-20211104203210151](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104204003.png) 

我们也可以在sentinel的控制台看到明显的效果，前几秒通过的QPS是在逐步增加的，而拒绝的QPS是逐步减少的，到最后，就全是通过的QPS了。

![image-20211104203638346](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104204009.png) 

###### 流控效果-排队等待

当请求超过设置的阈值时，`快速失败`和`Warm Up`都会拒绝新的请求并抛出异常。而`排队等待`则是让所有请求进入一个队列中，然后按照阈值允许的时间间隔依次执行。后来的请求必须等待前面执行完成，如果请求预期的等待时间超出最大时长，则会被拒绝。

比如QPS为5，即每秒最多处理5个请求，如果使用排队等待这种效果的话，也就是每200ms处理一个队列中的请求。假设我们将超时时间设置为2秒，意味着预期等待超过2000ms的请求会被拒绝并抛出异常。其实这种方式，也可以达到削峰的效果，对于突然到达的大量请求，并不是将阈值外的请求全部抛弃，而是放在队列中依次执行，如下图：

![image-20211104205316788](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104211550.png) 

下面就给`/order/{orderId}`这个资源设置限流，阈值设置为10，然后使用排队等待的效果，超时时间设置为5秒，配置如下所示：

![image-20211104210349686](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104211557.png) 

![image-20211104210440451](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104211605.png) 

然后在jmeter中访问`http://localhost:8080/order/101`这个地址，线程属性配置如下：

![image-20211104210530279](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104211613.png) 

启动测试后，虽然请求的QPS是15，已经超过了阈值，但是并没有出现报错的请求，因为这时候多出来的请求都被放到了队列中在等待执行。慢慢地，队列中有一些请求的等待时间超过了超时时间，所以就像下面这样，开始出现被拒绝的QPS。多出来的请求被限流后，请求慢慢趋于稳定，所以请求又能在队列中依次执行了，所以最后慢慢又没有被拒绝的QPS了。

![image-20211104210806626](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104211617.png) 

##### 8.4.4 热点参数限流

之前的限流是统计访问某个资源的所有请求，判断是否超过设定的QPS阈值。而热点参数限流则是对相同请求的不同参数值进行统计，判断是否超过设定的QPS阈值。如下图：

![image-20211105152010874](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211105161700.png)

下面进行一下配置演示。由于**热点参数限流对默认的SpringMVC资源无效**，而我们的controller层又都是SpringMVC资源，所以这里需要借助`@SentinelResource`注解。我们在order-service工程的OrderController类中对我们将要访问的资源增加@SentinelResource注解，注解中的值随便填写一个不重复的即可，如下所示：

```java
@SentinelResource("hot")
@GetMapping("/{orderId}")
public Order queryOrderByUserId(@PathVariable Long orderId) {
    log.info(">>>根据id查询商品信息，id：{}",orderId);
    return orderService.queryOrderById(orderId);
}
```

重启工程后访问一次` /order/{orderId}`地址，触发sentinel监控，然后在sentinel控制台配置热点规则，如下所示：

![image-20211105154854165](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211105161705.png) 

然后在jmeter中进行压力测试，首先配置好要访问的三个地址：

```http
http://localhost:8080/order/101
http://localhost:8080/order/102
http://localhost:8080/order/103
```

然后线程属性配置都是下面这样，即每秒5个请求：

![image-20211105155219617](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211105161719.png) 
启动测试后，观察测试结果，可以发现，对于`http://localhost:8080/order/101`地址的访问情况是这样的：

![image-20211105160806227](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211105161723.png) 

> 即每秒的五个请求中，总是只有两个是成功的，三个是失败的。

而对于`http://localhost:8080/order/102`地址的访问情况是这样的：

![image-20211105161030052](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211105161731.png) 

> 即每秒的五个请求中，总是有四个是成功的，一个是失败的。前面对参数值为102的请求进行过热点参数限流，设置其阈值为4，而不是2，通过测试结果可知，设置是生效的。

而对于`http://localhost:8080/order/103`地址的访问情况是这样的：

![image-20211105161432453](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211105161737.png) 

> 即每个请求都是成功的。由于之前对于参数值为103的请求设置的阈值为10，而压测的QPS为5，所以总是达不到阈值的，同时说明对于参数值为103的热点参数限流也是生效的。

<font color="red">**注意：**</font>以上限流规则涉及到的jmeter的配置已经导出成[sentinel.jmx](https://gitee.com/gongcqq/others/raw/master/sentinel.jmx)文件了，可以点击进行下载。

#### 8.5 隔离和降级

虽然限流可以尽量避免因高并发而引起的服务故障，但服务还会因为其它原因而故障。而要将这些故障控制在一定范围，避免雪崩，就要靠`线程隔离`(舱壁模式)和`熔断降级`手段了。线程隔离或者熔断降级，都是对客户端(调用方)的保护。

![image-20211109165957525](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211109170126.png) 

##### 8.5.1 openFeign整合Sentinel

在SpringCloud中，微服务调用都是通过openFeign实现的，因此保护客户端必须先整合openFeign和Sentinel，具体步骤如下所示：

1. 首先修改OrderService的application.yml文件，开启Feign的Sentinel功能：

   ```yaml
   feign:
     sentinel:
       enabled: true # 开启openFeign的Sentinel功能
   ```

2. 给FeignClient编写失败后的降级逻辑，一般可以使用`FallbackClass`和`FallbackFactory`，后者可以对远程调用的异常做处理，所以我们选择FallbackFactory。

   - 首先我们定义一个类，实现FallbackFactory接口，如下所示：

     ```java
     package com.order.config;
     
     import com.order.client.UserClient;
     import com.order.pojo.User;
     import feign.hystrix.FallbackFactory;
     import lombok.extern.slf4j.Slf4j;
     import org.springframework.stereotype.Component;
     
     @Slf4j
     @Component
     public class UserClientFallbackFactory implements FallbackFactory<UserClient> {
         @Override
         public UserClient create(Throwable throwable) {
             return new UserClient() {
                 @Override
                 public User findByUserId(Long id) {
                     log.error("根据用户id[{}]查询用户失败", id, throwable);
                     return new User();
                 }
             };
         }
     }
     ```

     > **说明**：这里是对远程调用失败的一个异常处理，findByUserId方法的返回值是User，这里的处理方式就是在调用失败后前台不报错，而是返回一个空的User对象。需要注意的是，这里要使用`@Component`注解将该类放到Spring容器中。

   - 接下来我们在之前使用了`@FeignClient`注解的UserClient接口中，在`@FeignClient`注解里面加上前一步创建的UserClientFallbackFactory，如下所示：

     ```java
     package com.order.client;
     
     import com.order.config.UserClientFallbackFactory;
     import com.order.pojo.User;
     import org.springframework.cloud.openfeign.FeignClient;
     import org.springframework.web.bind.annotation.GetMapping;
     import org.springframework.web.bind.annotation.PathVariable;
     
     @FeignClient(value = "userservice",fallbackFactory = UserClientFallbackFactory.class)
     public interface UserClient {
     
         @GetMapping("/user/{id}")
         User findByUserId(@PathVariable("id") Long id);
     }
     ```

3. 然后我们只启动order-service服务，不启动user-service服务，这时候浏览器访问如下地址进行远程调用：

   ```http
   http://localhost:8080/order/101
   ```

4. 可以发现，我们依然可以调用成功并有返回结果，只不过用户信息都为空，这是因为我们之前在代码中针对远程调用的异常处理方式是返回一个空的User对象。

   ![image-20211109203737302](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211109204506.png) 

   然后以下是我们在代码中打的异常日志信息：

   ![image-20211110101149121](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211110101215.png) 

##### 8.5.2 线程隔离

线程隔离主要有两种实现方式，即`线程池隔离`和`信号量隔离`，而Sentinel默认采用的是信号量隔离。

两种隔离方式的优缺点和使用场景如下：

|                | 优点                   | 缺点                     | 使用场景               |
| :------------- | :--------------------- | ------------------------ | ---------------------- |
| **线程池隔离** | 支持主动超时和异步调用 | 线程的额外开销比较大     | 低扇出的场景           |
| **信号量隔离** | 轻量级，无额外开销     | 不支持主动超时和异步调用 | 高频调用，高扇出的场景 |

下面进行线程隔离(舱壁模式)的演示：

由于前面在整合openFeign的时候，我们在UserClient接口中的`@FeignClient`注解上使用了fallbackFactory，即对远程调用异常进行了处理，所以下面演示的时候可能看不到明显效果，我们可以将`@FeignClient`注解中的以下内容去掉后再重启项目进行测试：

```java
fallbackFactory = UserClientFallbackFactory.class
```

1. 首先我们到sentinel控制台上进行线程隔离的相关配置：

   ![image-20211110203118098](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211110204956.png) 

   ![image-20211110203240805](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211110205001.png) 

   > **说明**：之前我们在"阈值类型"这里都是使用默认的QPS选项，这次我们选**线程数**，然后单机阈值配置成2。选择线程数的意思就是，该资源能够使用的tomcat线程数的最大值，也就是通过限制线程数量去实现舱壁模式。

2. 然后在jmeter中访问`http://localhost:8080/order/101`进行测试，jmeter的配置如下，意思是在启动测试的一瞬间就进来十个请求，而我们对该接口的最大线程数配置的是2，所以结果应该是只有两个请求是成功的才对。

   ![image-20211110203802214](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211110205015.png) 

   测试结果如下，结果也是符合预期的，所以线程隔离的配置是生效了的。

   ![image-20211110204640892](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211110205018.png) 

##### 8.5.3 熔断降级

`熔断降级`是解决雪崩问题的重要手段。其思路是由**断路器**统计服务调用的异常比例、慢请求比例，如果超出阈值则会**熔断**该服务，即拦截访问该服务的一切请求。而当服务恢复时，断路器会放行访问该服务的请求。下面是断路器的原理图：

<img src="https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211111123253.png" alt="image-20211111123217174" style="zoom: 65%;" /> 

**说明**：断路器默认是处于关闭状态的，这时所有请求都是可以通行的，但是当请求的失败量达到失败阈值后，就会开启断路器，这时候就会进入熔断阶段并拦截所有请求。当达到熔断时间后，断路器会关闭一部分来尝试对请求进行放行，如果放行的请求还是访问异常，就会再次打开断路器；如果请求访问是成功的，就会关闭断路器，不再对请求进行拦截。

断路器熔断一共有三种策略：==慢调用==、==异常比例==、==异常数==，下面就针对这三种策略进行演示。

###### 熔断策略-慢调用

业务的响应时长(RT)大于`指定时长`的请求就会被认定为是慢调用请求。在`指定时间`内，如果请求数量超过设定的`最小数量`并且慢调用比例大于设定的`比例阈值`，就会触发熔断。

1. 首先我们修改user-service服务中UserController类的queryById方法，让该方法休眠120毫秒，如下所示：

   ![image-20211111130358595](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211111134312.png) 

2. 重启服务后，在控制台配置熔断降级规则：

   ![image-20211111130814938](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211111134318.png) 

   ![image-20211111163049890](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211111163055.png)  

   ![image-20211111131444487](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211111134344.png) 

   > **说明**：以上配置的含义是，业务的响应时长超过100毫秒就会被认定为是慢调用。系统会统计每1000ms内的请求，如果请求超过5次，并且慢调用比例不低于0.3(比如10个请求中有3个或3个以上是慢调用)，那么就会触发熔断，熔断时间会持续5秒。

3. 浏览器连续访问`http://localhost:8080/order/101`，当一秒内访问5次以上后，就会触发熔断并返回以下结果：

   ![image-20211111132730446](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211111134358.png) 

   然后5秒内访问`http://localhost:8080/order/102`，发现也会返回同样的结果，因为此时已经触发熔断了，在熔断时间内(配置的是5秒)该接口的所有请求都会被拦截。

   ![image-20211111132841632](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211111134403.png) 

   > 当过了熔断时间后，如果我们首先访问的是`/order/102`就没事，如果访问的是`/order/101`，由于刚过熔断时间后是处于half-open状态的，而该请求又是慢调用，所以该次请求过后就会又进入熔断，这期间就算我们访问的是`/order/102`，请求也会被拦截，只能等熔断时间过后再访问才能访问成功。

###### 熔断策略-异常比例

对于`指定时间`内的调用，如果调用次数超过指定的`最小请求数`，并且出现异常的比例达到设定的`比例阈值`，则触发熔断。

1. 首先我们修改user-service服务中UserController类的queryById方法，故意抛出异常，如下所示：

   ![image-20211111162728087](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211111173753.png) 

2. 重启服务后，在控制台配置熔断降级规则：

   ![image-20211111130814938](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211111173800.png) 

   ![image-20211111163325380](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211111173807.png) 

   ![image-20211111163410989](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211111173813.png) 

   > **说明**：以上配置的含义是，系统会统计每5000ms内的请求，如果请求超过5次，并且异常请求比例超过一半，那么就会触发熔断，熔断时间会持续10秒。

3. 浏览器多次访问`http://localhost:8080/order/102`，都是可以正常返回的，如下所示：

   ![image-20211111170902478](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211111173819.png) 

   然后五秒内再多次访问`http://localhost:8080/order/101`，由于我们手动抛了异常，所以这个接口肯定是访问异常的，只要这个接口五秒内的访问次数超过了总请求数的一半，就会触发熔断，那么我们再访问`/order/102`，请求也会被拦截，如下所示，除非是过了熔断时长后再访问才能成功。

   ![image-20211111171802789](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211111173825.png) 

###### 熔断策略-异常数

对于`指定时间`内的调用，如果调用次数超过指定的`最小请求数`，并且出现的异常数超过设定的`异常数`，则触发熔断。

异常数的配置和上面异常比例的差不太多，代码还是用上面的代码，控制台的配置，只需在降级规则的熔断策略中将"异常比例"换成"异常数"就可以了。

![image-20211111172251744](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211111173841.png) 

> **说明**：以上配置的含义是，系统会统计每5000ms内的请求，如果请求超过5次，并且异常请求数超过2个，那么就会触发熔断，熔断时间会持续10秒。

这种熔断策略就很好测试了，连续访问两次以上`/order/101`接口，这时就会触发熔断，熔断时间是10秒，所以在10秒内访问`/order/102`这个无异常的接口也会访问不通，因为熔断时间内请求都被拦截了，只有过了熔断时间后才能访问成功。

##### 8.5.4 授权规则

授权规则可以对调用方的来源做控制，有白名单和黑名单两种方式：

- `白名单`：来源(origin)在白名单内的调用者允许访问；
- `黑名单`：来源(origin)在黑名单内的调用者不允许访问。

Sentinel是通过RequestOriginParser这个接口的parseOrigin方法来获取请求来源的，我们可以在代码中新增一个类实现这个接口来设置规则，我们在order-service服务中新增一个LocalOriginParser类，内容如下：

```java
package com.order.config;

import com.alibaba.csp.sentinel.adapter.spring.webmvc.callback.RequestOriginParser;
import org.springframework.stereotype.Component;
import javax.servlet.http.HttpServletRequest;

@Component
public class LocalOriginParser implements RequestOriginParser {

    @Override
    public String parseOrigin(HttpServletRequest request) {
        String uri = request.getRequestURI();
        //到时候会针对"/order/103"请求设置白名单或者黑名单，所以这里进行单独处理
        if(uri.endsWith("103")){
            return "ok";
        }
        return "error";
    }
}
```

然后重启order-service服务，在sentinel控制台进行规则配置：

![image-20211112161001149](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112162637.png) 

![image-20211112161325863](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112162643.png) 

配置好以上规则后，由于在代码的LocalOriginParser类的parseOrigin方法中，只有当请求地址后缀以"103"结尾的时候才会返回"ok"，而授权规则中针对"ok"配置的白名单。也就是说，针对"/order/{orderId}"请求，只有orderId的值为103时请求才能访问成功，别的值都会访问失败，如下所示：

![image-20211112161907944](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112162650.png)  

![image-20211112161948959](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112162655.png) 

> **说明**：如果我们将"ok"配置的不是白名单而是黑名单的话，那就反过来了，就只有"/order/103"是访问不通的，其他请求都是可以通的。或者将流控应用的值配置为parseOrigin方法的另一个返回值"error"，并且授权类型配置的是白名单，那么也是只有"/order/103"访问不通。

#### 8.6 自定义异常结果

默认情况下，发生限流、降级、授权拦截时，都会抛出异常到调用方，并且都是`BlockException`异常。如果要自定义异常时的返回结果，可以实现`BlockExceptionHandler`接口。

另外，BlockException是包含很多个子类的，并且分别对应不同的场景，如下所示：

| 异常                 | 说明               |
| :------------------- | :----------------- |
| FlowException        | 限流异常           |
| ParamFlowException   | 热点参数限流的异常 |
| DegradeException     | 降级异常           |
| AuthorityException   | 授权规则异常       |
| SystemBlockException | 系统规则异常       |

在order-service服务中新增一个实现`BlockExceptionHandler`接口的类，内容如下：

```java
package com.order.config;

import com.alibaba.csp.sentinel.adapter.spring.webmvc.callback.BlockExceptionHandler;
import com.alibaba.csp.sentinel.slots.block.BlockException;
import com.alibaba.csp.sentinel.slots.block.authority.AuthorityException;
import com.alibaba.csp.sentinel.slots.block.degrade.DegradeException;
import com.alibaba.csp.sentinel.slots.block.flow.FlowException;
import com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowException;
import org.springframework.http.HttpStatus;
import org.springframework.stereotype.Component;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

@Component
public class SentinelBlockHandler implements BlockExceptionHandler {

    @Override
    public void handle(HttpServletRequest request,
                       HttpServletResponse response, BlockException e) throws Exception {
        String msg = "未知异常";
        //429, "Too Many Requests"
        int status = HttpStatus.TOO_MANY_REQUESTS.value();
        if (e instanceof FlowException) {
            msg = "请求被限流了!";
        } else if (e instanceof DegradeException) {
            msg = "请求被降级了!";
        } else if (e instanceof ParamFlowException) {
            msg = "热点参数限流!";
        } else if (e instanceof AuthorityException) {
            msg = "请求没有权限!";
            //401, "Unauthorized"
            status = HttpStatus.UNAUTHORIZED.value();
        }
        response.setContentType("application/json;charset=utf-8");
        response.setStatus(status);
        response.getWriter().println("{\"msg\":\""+ msg + "\",\"status\":" + status + "}");
    }
}
```

然后重启服务，并在控制台分别配置限流、降级、授权拦截规则进行测试，可以发现，异常返回已经是我们自己设置的了。 

![image-20211112170944489](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112170959.png)  

![image-20211112170631839](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112170801.png) 

![image-20211112170331144](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112170809.png) 

> **说明**：截止到目前为止的代码可以点击[cloud-parent](https://gitee.com/gongcqq/others/attach_files/879729/download/cloud-parent.zip)进行下载。

#### 8.7 规则持久化

默认情况下，对于我们在控制台配置的限流或者熔断降级等规则，Sentinel是保存在内存中的，所以一旦我们的服务进行重启，这些规则就会消失。生产环境上我们肯定是不能容忍这种情况发生的，所以就需要将规则进行持久化保存。

##### 8.7.1 规则的模式

Sentinel的控制台规则管理有三种模式：

![image-20211112175305460](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112192730.png)  

`原始模式`：控制台配置的规则直接推送到Sentinel客户端，也就是我们的应用，然后保存在内存中，服务重启则丢失。

![image-20211112174743861](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112192739.png) 

`Pull模式`：控制台将配置的规则推送到Sentinel客户端，而客户端会将配置规则保存在本地文件或数据库中，以后会定时去本地文件或数据库中查询，更新本地规则。

![image-20211112174839087](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112192745.png) 

`Push模式`：控制台将配置规则推送到远程配置中心，例如Nacos，然后Sentinel客户端监听Nacos，获取配置变更的推送消息，完成本地配置更新。

![image-20211112174925429](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112192758.png) 

> **说明**：生产上我们一般都使用`Push模式`，所以下面也只会演示这种模式的用法，如果想要了解规则持久化的详情，可以参考GitHub官方上的[在生产环境中使用Sentinel](https://github.com/alibaba/Sentinel/wiki/%E5%9C%A8%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E4%B8%AD%E4%BD%BF%E7%94%A8-Sentinel)。

##### 8.7.2 实现Push模式

Push模式实现还是比较复杂的，依赖于Nacos，并且是需要修改Sentinel源码的，下面进行相关操作。

1. 在order-service的pom文件中引入sentinel监听nacos的依赖：

   ```xml
   <dependency>
       <groupId>com.alibaba.csp</groupId>
       <artifactId>sentinel-datasource-nacos</artifactId>
   </dependency>
   ```

2. 在order-service工程的application.yml文件中配置nacos地址及监听的配置信息：

   ```yaml
   spring:
     cloud:
       sentinel:
         datasource:
           flow: # 限流相关的配置，还可以是：degrade、authority、param-flow
             nacos:
               server-addr: 192.168.68.11:8848 # nacos地址
               dataId: orderservice-flow-rules
               groupId: SENTINEL_GROUP
               rule-type: flow # 还可以是：degrade、authority、param-flow
   ```

3. 下载[Sentinel源码](https://github.com/alibaba/Sentinel/archive/refs/tags/1.8.1.zip)并使用IDEA打开源码中的sentinel-dashboard工程：

   ![image-20211112212030686](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112234123.png) 

4. 修改sentinel-dashboard工程的pom文件，将其中sentinel-datasource-nacos依赖的scope去掉：

   **修改前是下面这样：**

   ```xml
   <dependency>
       <groupId>com.alibaba.csp</groupId>
       <artifactId>sentinel-datasource-nacos</artifactId>
       <scope>test</scope>
   </dependency>
   ```

   **修改后是下面这样：**

   ```xml
   <dependency>
       <groupId>com.alibaba.csp</groupId>
       <artifactId>sentinel-datasource-nacos</artifactId>
   </dependency>
   ```

5. 修改NacosConfig类的内容，如下所示：

   ![image-20211112214232303](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112234134.png) 

   **修改前的内容是这样：**

   ![image-20211112213922734](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112234144.png) 

   **修改后的内容是这样：**

   ```java
   @Configuration
   @ConfigurationProperties(prefix = "nacos")
   public class NacosConfig {
   
       //nacos地址
       private String addr;
   
       public String getAddr() {
           return addr;
       }
   
       public void setAddr(String addr) {
           this.addr = addr;
       }
   
       @Bean
       public Converter<List<FlowRuleEntity>, String> flowRuleEntityEncoder() {
           return JSON::toJSONString;
       }
   
       @Bean
       public Converter<String, List<FlowRuleEntity>> flowRuleEntityDecoder() {
           return s -> JSON.parseArray(s, FlowRuleEntity.class);
       }
   
       @Bean
       public ConfigService nacosConfigService() throws Exception {
           return ConfigFactory.createConfigService(addr);
       }
   }
   ```

6. 将test路径下的整个nacos包拷贝到main路径下的com.alibaba.csp.sentinel.dashboard.rule包下：

   ![image-20211112215318819](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112234200.png) 

7. 在sentinel-dashboard工程的application.properties文件中添加nacos地址的配置：

   ```properties
   nacos.addr=localhost:8848
   ```

8. 修改`com.alibaba.csp.sentinel.dashboard.controller.v2.FlowControllerV2`类，配置nacos的数据源：

   ![image-20211112220227409](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112234227.png) 

   **修改后是下面这样：**

   ```java
   @RestController
   @RequestMapping(value = "/v2/flow")
   public class FlowControllerV2 {
   
       private final Logger logger = LoggerFactory.getLogger(FlowControllerV2.class);
   
       @Autowired
       private InMemoryRuleRepositoryAdapter<FlowRuleEntity> repository;
   
       @Autowired
       @Qualifier("flowRuleNacosProvider")
       private DynamicRuleProvider<List<FlowRuleEntity>> ruleProvider;
       @Autowired
       @Qualifier("flowRuleNacosPublisher")
       private DynamicRulePublisher<List<FlowRuleEntity>> rulePublisher;
   ```

9. 修改`src\main\webapp\resources\app\scripts\directives\sidebar\sidebar.html`文件，添加支持nacos的菜单，将下面这部分的注释打开：

   ![image-20211112222057505](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112234250.png) 

   然后修改一下菜单的中文名称，修改后内容如下：

   ```html
   <li ui-sref-active="active" ng-if="entry.appType==0">
     <a ui-sref="dashboard.flow({app: entry.app})">
       <i class="glyphicon glyphicon-filter"></i>&nbsp;&nbsp;流控规则-NACOS</a>
   </li>
   ```

10. 重新编译打包修改好的sentinel-dashboard工程：

    ![image-20211112222709133](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112234255.png) 

    > **说明**：可以直接点击[sentinel-dashboard-nacos](https://gitee.com/gongcqq/others/attach_files/879793/download/sentinel-dashboard-nacos.jar)来下载我上面打包好的sentinel-dashboard工程。

11. 启动打包好的jar包。之前把sentinel源码中nacos的地址设置成了"localhost:8848"，假设我想指定nacos的地址来启动sentinel，那么可以使用类似如下的启动命令：

    ```bash
    java -jar -Dnacos.addr=192.168.68.11:8848 sentinel-dashboard-nacos.jar
    ```

12. 接下来启动我们的order-service服务以及user-service服务，并通过`http://192.168.68.11:8080/`地址登录sentinel控制台，随便访问一个请求，触发sentinel监控，然后我们可以发现控制台多了一个菜单：

    ![image-20211112231923476](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112234301.png) 

    > **说明**：如果没有正常显示我们添加的菜单，可以清理缓存后刷新下试试，或者直接使用无痕模式进行访问。

13. 然后我们可以配置一个流控规则测试该规则是否进行了持久化，在"流控规则-NACOS"菜单进行新增：

    ![image-20211112232512924](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112234309.png) 

    > **说明**：我们不能在`簇点链路`或者`流控规则`这两处的菜单内新增流控规则，因为这两个地方新增的规则是不会进行持久化的，只能在`流控规则-NACOS`菜单进行新增。如果想让降级、授权等规则也进行持久化，就要使用同样的方法对sentinel源码进行修改才行。

14. 以上限流规则新增成功后，我们可以访问`http://192.168.68.11:8848/`地址登录nacos，然后在**配置管理**里面可以看到我们新增的流控规则，而nacos的配置是保存到数据库中的，这也说明sentinel的限流规则进行了持久化。

    ![image-20211112233120705](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112234319.png) 

15. 此时我们访问order-service服务，发现确实被限流了，而且即便我们重启order-service服务，限流规则的配置也依然会存在，而限流规则也依然会生效。

    ![image-20211112233254820](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211112234326.png) 

    > **说明**：截止到目前为止的代码可以点击[cloud-parent](https://gitee.com/gongcqq/others/attach_files/879805/download/cloud-parent.zip)进行下载。

### 9.Seata的使用

#### 9.1 分布式事务入门

事务必须具有的四个特性分别是：`原子性`(atomicity)、`一致性`(consistency)、`隔离性`(isolation，又称独立性)以及`持久性`(durability)。这就是事务的ACID原则。

<img src="https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115124750.png" alt="image-20211115101048405" style="zoom:80%;" /> 

下面进行分布式服务的案例演示，看看没有分布式事务时可能会引发的问题。项目工程是[seata-demo](https://gitee.com/gongcqq/others/attach_files/880659/download/seata-demo.zip)，主要包含以下三个服务，分别是order-service(订单服务)、account-service(账户服务)以及storage-service(库存服务)：

![image-20211115104659133](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115124842.png) 

项目的初始化数据库脚本如下所示，或者也可以直接下载[seata-init.sql](https://gitee.com/gongcqq/others/raw/master/seata/seata-init.sql)文件，内容是一样的。

```sql
SET NAMES utf8mb4;
SET FOREIGN_KEY_CHECKS = 0;

-- account_tbl表
DROP TABLE IF EXISTS `account_tbl`;
CREATE TABLE `account_tbl`  (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `user_id` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `money` int(11) UNSIGNED NULL DEFAULT 0,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 2 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = COMPACT;

INSERT INTO `account_tbl` VALUES (1, 'user202109132032012', 1000);

-- order_tbl表
DROP TABLE IF EXISTS `order_tbl`;
CREATE TABLE `order_tbl`  (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `user_id` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `commodity_code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `count` int(11) NULL DEFAULT 0,
  `money` int(11) NULL DEFAULT 0,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = COMPACT;

-- storage_tbl表
DROP TABLE IF EXISTS `storage_tbl`;
CREATE TABLE `storage_tbl`  (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `commodity_code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `count` int(11) UNSIGNED NULL DEFAULT 0,
  PRIMARY KEY (`id`) USING BTREE,
  UNIQUE INDEX `commodity_code`(`commodity_code`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 2 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = COMPACT;

INSERT INTO `storage_tbl` VALUES (1, '100202109032041', 10);

SET FOREIGN_KEY_CHECKS = 1;
```

本地使用的数据库地址是`jdbc:mysql://localhost:3306/seata_demo`。项目的主要业务逻辑是，用户通过订单服务创建订单，然后订单服务会调用账户服务进行用户余额扣减，还会调用库存服务进行商品库存扣减。

<img src="https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115125038.png" alt="image-20211115105626824" style="zoom: 67%;" /> 

执行完初始化数据库脚本后，三个表的数据情况如下所示：

![image-20211115110549664](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115125045.png) 

然后依次启动三个微服务项目：

<img src="https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115125058.png" alt="image-20211115110722832" style="zoom: 78%;" /> 

启动成功后，使用工具访问`http://localhost:8082/order/create`，请求方式为POST，请求体内容如下：

```json
{"userId":"user202109132032012","commodityCode":"100202109032041","count":2,"money":200}
```

> **说明**：`userId`是用户id，`commodityCode`是商品编码，`count`是商品数量，`money`是商品金额。

出现如下结果，表示订单创建成功：

![image-20211115111142372](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115125228.png) 

成功创建订单后数据库中各个表的数据情况如下：

![image-20211115112230526](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115125239.png) 

以上演示的是正常情况，下面再来演示下异常情况。在上面提供的初始化数据库脚本中，storage_tbl表的count字段使用了`UNSIGNED`属性，该属性意思是count字段的值不能为负数，那么我们只要在创建订单时使商品数量大于当前数据库的库存数量即可抛出异常。

再次调用创建订单的`http://localhost:8082/order/create`接口，将商品数量设置为20，详细请求体如下所示：

```json
{"userId":"user202109132032012","commodityCode":"100202109032041","count":20,"money":200}
```

返回的应答如下所示：

![image-20211115123629621](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115125508.png) 

由返回的应答可知，创建订单接口显然已经调用异常了，这时候我们再看看数据库中各个表的数据情况：

![image-20211115124311003](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115125513.png) 

由数据库中各个表的数据可知，在微服务项目中，其中一个服务出现了问题，并不会让所以服务都进行回滚，所以就出现了订单创建失败，但是用户余额依然进行扣减了的情况。而分布式事务就是用来解决以上问题的。

在分布式系统下，一个业务跨越多个服务或数据源，每个服务都是一个分支事务，要保证所有分支事务最终状态一致，这样的事务就是`分布式事务`。下面将要详细介绍的seata框架就是分布式事务框架。

#### 9.2 理论基础

##### 9.2.1 CAP定理

1998年，加州大学的计算机科学家Eric Brewer提出，分布式系统有三个指标：

- `Consistency(一致性)`：用户访问分布式系统中的任意节点，得到的数据必须一致；
- `Availability(可用性)`：用户访问集群中的任意健康节点，必须能得到响应，而不是超时或拒绝；
- `Partition tolerance(分区容错性)`：如果因为网络故障或其它原因导致分布式系统中的部分节点与其它节点失去连接，就会形成独立分区，在集群出现分区时，整个系统也要持续对外提供服务。

Eric Brewer说，分布式系统无法同时满足这三个指标，这个结论就叫做==CAP定理==。

![image-20211115141413894](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115141419.png) 

> **说明**：分布式系统节点通过网络连接，一定会出现分区问题(P)，当分区出现时，系统的一致性(C)和可用性(A)就无法同时满足。

##### 9.2.2 BASE理论

BASE理论是对CAP的一种解决思路，包含三个思想：

- `Basically Available(基本可用)`：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用；
- `Soft State(软状态)`：在一定时间内，允许出现中间状态，比如临时的不一致状态；
- `Eventually Consistent(最终一致性)`：虽然无法保证强一致性，但是在软状态结束后，最终达到数据一致。

而分布式事务最大的问题是各个子事务的一致性问题，因此可以借鉴CAP定理和BASE理论：

- `AP模式`：各子事务分别执行和提交，允许出现结果不一致，然后采用弥补措施恢复数据即可，实现最终一致。
- `CP模式`：各个子事务执行后互相等待，同时提交，同时回滚，达成强一致。但事务等待过程中，处于弱可用状态。

##### 9.2.3 分布式事务模型

解决分布式事务，各个子系统之间必须能感知到彼此的事务状态，才能保证状态一致，因此需要一个==事务协调者==来协调每一个事务的参与者(子系统事务)。

这里的子系统事务，称为`分支事务`，有关联的各个分支事务在一起称为`全局事务`。

<img src="https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115221326.png" alt="image-20211115205059742" style="zoom:65%;" />  

#### 9.3 Seata的入门

##### 9.3.1 初识Seata

[Seata](http://seata.io/zh-cn/)是2019年1月份蚂蚁金服和阿里巴巴共同开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务，为用户打造一站式的分布式解决方案。

Seata事务管理中有三个重要的角色：

- `TC(Transaction Coordinator) - 事务协调者`：维护全局和分支事务的状态，协调全局事务提交或回滚；
- `TM(Transaction Manager) - 事务管理器`：定义全局事务的范围、开始全局事务、提交或回滚全局事务；
- `RM(Resource Manager) - 资源管理器`：管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。

![image-20211115210638103](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115221349.png) 

Seata提供了四种不同的分布式事务解决方案：

- `XA模式`：强一致性分阶段事务模式，牺牲了一定的可用性，无业务侵入；
- `TCC模式`：最终一致的分阶段事务模式，有业务侵入；
- `AT模式`：最终一致的分阶段事务模式，无业务侵入，也是Seata的默认模式；
- `SAGA模式`：长事务模式，有业务侵入。

> **说明**：一般最常用的是AT模式，如果某些接口对于性能的要求非常高的话，这些接口可以使用TCC模式，其他接口还是使用AT模式，多种分布式事务模式是可以共存的。

##### 9.3.2 部署Seata

这里部署的Seata其实就是上文中提到的事务协调者(TC)，我们可以直接到GitHub上进行[下载](https://github.com/seata/seata/releases/download/v1.4.2/seata-server-1.4.2.tar.gz)，如果网速不好，也可以使用[备用地址](https://gongsl.lanzoui.com/i5hNGwjbu9g)进行下载。下载完成之后，使用`tar -zxvf seata-server-1.4.2.tar.gz `命令进行解压即可。

解压后，我们需要进行到`seata/seata-server-1.4.2/conf/`目录下，修改registry.conf文件的内容。该文件中主要是配置seata注册中心以及读取seata配置文件的方式，我们这里都使用nacos，修改后的内容如下：

```properties
registry {
  # seata tc服务的注册中心类，这里选择nacos，也可以是eureka、zookeeper等
  type = "nacos"

  nacos {
    # seata tc服务注册到nacos的服务名称，可以自定义
    application = "seata-tc-server"
    # nacos的地址
    serverAddr = "192.168.68.11:8848"
    # seata服务所在分组
    group = "DEFAULT_GROUP"
    # seata服务所在的名称空间，这里不填就是使用默认的"public"
    namespace = ""
    # 这个是seata在nacos中的集群配置，默认是"default"
    cluster = "SH"
    # 这个是nacos的用户名
    username = "nacos"
    # 这个是nacos的密码
    password = "nacos"
  }
}

config {
  # 读取tc服务端的配置文件的方式，这里是从nacos配置中心读取，这样如果tc是集群，可以共享配置
  type = "nacos"
  # 配置nacos地址等信息
  nacos {
    serverAddr = "192.168.68.11:8848"
    namespace = ""
    group = "SEATA_GROUP"
    username = "nacos"
    password = "nacos"
    dataId = "seataServer.properties"
  }
}
```

然后我们需要在nacos的配置管理里面添加上面设置的`seataServer.properties`文件：

![image-20211115214148231](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115221357.png) 

seataServer.properties文件的内容如下所示：

```properties
# 数据存储方式，db代表数据库
store.mode=db
store.db.datasource=druid
store.db.dbType=mysql
store.db.driverClassName=com.mysql.jdbc.Driver
store.db.url=jdbc:mysql://192.168.68.11:3306/seata?useUnicode=true
store.db.user=root
store.db.password=root
store.db.minConn=5
store.db.maxConn=30
store.db.globalTable=global_table
store.db.branchTable=branch_table
store.db.queryLimit=100
store.db.lockTable=lock_table
store.db.maxWait=5000
# 事务、日志等配置
server.recovery.committingRetryPeriod=1000
server.recovery.asynCommittingRetryPeriod=1000
server.recovery.rollbackingRetryPeriod=1000
server.recovery.timeoutRetryPeriod=1000
server.maxCommitRetryTimeout=-1
server.maxRollbackRetryTimeout=-1
server.rollbackRetryTimeoutUnlockEnable=false
server.undo.logSaveDays=7
server.undo.logDeletePeriod=86400000

# 客户端与服务端传输方式
transport.serialization=seata
transport.compressor=none
# 关闭metrics功能，提高性能
metrics.enabled=false
metrics.registryType=compact
metrics.exporterList=prometheus
metrics.exporterPrometheusPort=9898
```

> **说明**：其实seataServer.properties文件的内容我们只用关注`store`开头的配置即可，其余配置使用的都是默认的，所以不写也没事，这里贴出来是为方便以后万一想修改，知道需要修改的是哪些配置。

由于Seata在管理分布式事务时，需要记录事务相关数据到数据库中，因此我们需要提前创建好这些表，上面使用nacos的配置中心管理的seataServer.properties文件中，使用的是名为seata的库，所以首先要创建这个库，然后再在这个库中创建对应的表，建表语句如下所示，或者也可以直接下载[seata-tc-server.sql](https://gitee.com/gongcqq/others/raw/master/seata/seata-tc-server.sql)文件，内容是一样的。

```sql
-- 全局事务表
CREATE TABLE IF NOT EXISTS `global_table`
(
    `xid`                       VARCHAR(128) NOT NULL,
    `transaction_id`            BIGINT,
    `status`                    TINYINT      NOT NULL,
    `application_id`            VARCHAR(32),
    `transaction_service_group` VARCHAR(32),
    `transaction_name`          VARCHAR(128),
    `timeout`                   INT,
    `begin_time`                BIGINT,
    `application_data`          VARCHAR(2000),
    `gmt_create`                DATETIME,
    `gmt_modified`              DATETIME,
    PRIMARY KEY (`xid`),
    KEY `idx_gmt_modified_status` (`gmt_modified`, `status`),
    KEY `idx_transaction_id` (`transaction_id`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8;

-- 分支事务表
CREATE TABLE IF NOT EXISTS `branch_table`
(
    `branch_id`         BIGINT       NOT NULL,
    `xid`               VARCHAR(128) NOT NULL,
    `transaction_id`    BIGINT,
    `resource_group_id` VARCHAR(32),
    `resource_id`       VARCHAR(256),
    `branch_type`       VARCHAR(8),
    `status`            TINYINT,
    `client_id`         VARCHAR(64),
    `application_data`  VARCHAR(2000),
    `gmt_create`        DATETIME(6),
    `gmt_modified`      DATETIME(6),
    PRIMARY KEY (`branch_id`),
    KEY `idx_xid` (`xid`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8;

-- 全局锁表
CREATE TABLE IF NOT EXISTS `lock_table`
(
    `row_key`        VARCHAR(128) NOT NULL,
    `xid`            VARCHAR(128),
    `transaction_id` BIGINT,
    `branch_id`      BIGINT       NOT NULL,
    `resource_id`    VARCHAR(256),
    `table_name`     VARCHAR(32),
    `pk`             VARCHAR(36),
    `gmt_create`     DATETIME,
    `gmt_modified`   DATETIME,
    PRIMARY KEY (`row_key`),
    KEY `idx_branch_id` (`branch_id`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8;

CREATE TABLE IF NOT EXISTS `distributed_lock`
(
    `lock_key`       CHAR(20) NOT NULL,
    `lock_value`     VARCHAR(20) NOT NULL,
    `expire`         BIGINT,
    primary key (`lock_key`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8mb4;

INSERT INTO `distributed_lock` (lock_key, lock_value, expire) VALUES ('AsyncCommitting', ' ', 0);
INSERT INTO `distributed_lock` (lock_key, lock_value, expire) VALUES ('RetryCommitting', ' ', 0);
INSERT INTO `distributed_lock` (lock_key, lock_value, expire) VALUES ('RetryRollbacking', ' ', 0);
INSERT INTO `distributed_lock` (lock_key, lock_value, expire) VALUES ('TxTimeoutCheck', ' ', 0);
```

完成以上操作后，就可以启动seata服务了，进入到`seata/seata-server-1.4.2/bin`目录中，直接执行如下命令即可：

```bash
sh seata-server.sh
```

启动成功后，我们可以在nacos的服务列表中看到一个名为seata-tc-server的服务，查看详情时可以发现，seata服务所在集群是SH，然后默认是以8091端口运行的。

<img src="https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115221409.png" alt="image-20211115221050793" style="zoom:82.5%;" /> 

![image-20211115221155614](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211115221413.png)  

##### 9.3.3 集成Seata

我们需要将本地项目的account-service服务、order-service服务以及storage-service服务都按照以下步骤进行依赖的引入以及配置文件的修改。

1. 首先我们需要在各个服务中引入seata依赖：

   ```xml
   <dependency>
       <groupId>com.alibaba.cloud</groupId>
       <artifactId>spring-cloud-starter-alibaba-seata</artifactId>
       <exclusions>
           <exclusion>
               <artifactId>seata-spring-boot-starter</artifactId>
               <groupId>io.seata</groupId>
           </exclusion>
       </exclusions>
   </dependency>
   <dependency>
       <groupId>io.seata</groupId>
       <artifactId>seata-spring-boot-starter</artifactId>
       <version>${seata.version}</version>
   </dependency>
   ```

   > **说明**：在`spring-cloud-starter-alibaba-seata`里面引入的seata的版本较低，因此上面排除掉了，重新引入了1.4.2版本的seata。

2. 然后分别修改各个服务的application.yml文件，添加以下配置：

   ```yaml
   seata:
     registry: # seata tc服务注册中心的配置，微服务根据这些信息去注册中心获取tc服务地址
       # 下面参考seata服务的registry.conf中的配置
       type: nacos
       nacos:
         server-addr: 192.168.68.11:8848
         namespace: ""
         group: DEFAULT_GROUP
         application: seata-tc-server # tc服务在nacos中的服务名称
         cluster: SH
     tx-service-group: seata-demo # 事务组，根据这个获取tc服务的cluster名称
     service:
       vgroup-mapping: # 事务组与tc服务cluster的映射关系
         seata-demo: SH
   ```

#### 9.4 Seata的实践

Seata通过`XA`、`AT`、`TCC`和`SAGA`这四种事务模式，为我们打造了一站式的分布式解决方案，下面会分别演示这四种事务模式是如何解决分布式事务问题的。

##### 9.4.1 XA模式

###### 9.4.1.1 XA模式入门

XA规范是X/Open组织定义的分布式事务处理(DTP，Distributed Transaction Processing)标准，XA规范描述了全局的TM与局部的RM之间的接口，几乎所有主流的数据库都对XA规范提供了支持，它一般包含两个阶段。

<img src="https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211116205022.png" alt="image-20211116194206492" style="zoom: 67%;" /> 

第一阶段只是做一个准备就绪的工作，并不会提交事务，第一阶段如果都成功了，才会在第二阶段进行事务的提交。但是如果第一阶段有任何一个服务是失败的，那么在第二阶段，其余服务也会进行回滚。

以上规范只是一个标准，在具体实现的时候，可能会有差别。Seata的XA模式在实现上与上面大体相似，但是也做了一些调整，主要是增加了一个事务管理器(TM)：

<img src="https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211116205036.png" alt="image-20211116200446770" style="zoom: 80%;" /> 

###### 9.4.1.2 XA模式的优缺点

![image-20211116201552666](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211116205053.png) 

###### 9.4.1.3 实现XA模式

Seata的starter已经完成了XA模式的自动装配，所以实现非常简单，步骤如下：

1. 修改所有参与事务的微服务的application.yml文件，开启XA模式：

   ```yaml
   seata:
     data-source-proxy-mode: XA # 开启数据源代理的XA模式
   ```

2. 给发起全局事务的入口类或方法添加`@GlobalTransactional`注解，我这边就直接将order-service服务中的全局事务入口类OrderServiceImpl之前的`@Transactional`注解换成了`@GlobalTransactional`注解：

   ![image-20211116203854734](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211116205106.png) 

3. 重启服务并测试：

   测试前数据库表数据情况如下所示：

   ![image-20211116204106608](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211116205111.png) 

   然后以POST方式访问`http://localhost:8082/order/create`接口，请求体内容如下：

   ```json
   {"userId":"user202109132032012","commodityCode":"100202109032041","count":20,"money":200}
   ```

   由于我们将商品数量设置成了20，已经超过了库存数量，所以请求一定会报错，接口调用完成之后我们再次查看数据库中各个表的数据时发现，三个表的数据都没有变化，说明分布式事务已经起作用了，并完成了回滚。我们从控制台中account-service服务打印的日志也可以看到回滚的日志：

   ![image-20211116204901596](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211116205118.png)  

> **说明**：至此，Seata的XA模式就演示完成了。XA模式默认是等待和其他微服务一起提交事务，所以比较消耗性能，我们一般不使用这种模式，除非是对强一致性要求比较高的服务。

##### 9.4.2 AT模式

###### 9.4.2.1 AT模式原理

AT模式同样是分阶段提交的事务模型，不过却弥补了XA模型中资源锁定周期过长的缺陷。详细步骤如下所示：

![image-20211116210412067](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211116214525.png)  

阶段一RM的工作：

- 注册分支事务；
- 记录undo-log(数据快照)；
- 执行业务sql并**提交**；
- 报告事务状态。

阶段二提交时RM的工作：

- 删除undo-log。

阶段二回滚时RM的工作：

- 根据undo-log恢复数据到更新前。

举个例子，比如一个分支业务的SQL是这样的：`update tb_account set money = money - 10 where id = 1`，那么两个阶段的执行逻辑就如下所示：

![image-20211116214516462](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211116214550.png) 

> **说明**：在AT模式中，记录、删除或者恢复快照等操作，都是由框架自动完成的，是不用我们手动操作的。

**AT模式和XA模式的区别：**

- XA模式一阶段不提交事务，锁定资源；AT模式一阶段直接提交，不锁定资源。
- XA模式依赖数据库机制实现回滚；AT模式利用数据快照实现数据回滚。
- XA模式是强一致；AT模式是最终一致。

###### 9.4.2.2 脏写问题及解决方案

由于AT模式的事务都是各自提交各自的，并不是像XA模式那样等待着一起提交，所以在并发场景下可能会存在一些问题。还是以业务执行`update account set money = money - 10 where id = 1`这样一条SQL为例：

![image-20211117143309275](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211117155139.png) 

**解释说明**：如上图所示，线程一在第一阶段首先会记录快照，即money等于100，执行完业务sql并提交后，money的值就变成了90。提交后就会释放数据库锁，假设此时线程二过来抢到了锁，由于此时money的值是90，所以就将此时的值作为线程二的快照进行了保存，然后就开始执行业务sql并提交，执行完之后money的值就变成了80。线程二提交后会释放数据库锁，假设线程一又抢到了锁，然后就开始进行第二阶段的操作，可是如果第二阶段出错了，就会根据之前记录的快照进行回滚，然后money的值就会变成100了。但是数据库中money的值已经被线程二改过一次了，线程一失败后还是将money的值回滚成100显然是有问题的。

为了解决以上提及的问题，Seata引入了==全局锁==的概念。持有全局锁的事务会被记录到一张表里，这个表里面会有事务的名称、事务正在操作的表的表名以及该表的主键等信息，具体的操作流程如下所示：

![image-20211117170940040](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211117180259.png)  

**解释说明**：同样是对account表的money字段进行操作，线程一在执行完业务sql后会先获取全局锁，然后才会提交事务并释放数据库锁。假设这时线程二获取到了数据库锁，然后也执行了业务sql，执行完后就会去尝试获取全局锁，但是由于此时线程一整个阶段还没执行完，全局锁还没有被释放，所以线程二无法获取到全局锁。这时就会出现线程一等待获取数据库锁，线程二等待获取全局锁的局面。不过最多300毫秒后，线程二还没获取到全局锁的话，就会超时回滚，然后释放数据库锁。这时线程一获取到数据库锁后会继续执行第二阶段的流程，一旦执行失败，就会根据之前的快照进行回滚，然后释放全局锁。由于线程一全程持有全局锁，所以不会出现脏写的情况。

> **注意**：AT模式和XA模式一样，都使用锁来保证数据的一致性，但是AT模式的性能却比XA模式要高，是因为它们的锁是有本质区别的。XA模式使用的是数据库锁，一旦锁定，所有对数据库中相关数据的操作都要等待，所以性能较差。但是AT模式的全局锁不同，它是Seata框架提供的锁，所以不使用该框架对数据库的操作是不受全局锁的影响的，而且如果操作的是同一行数据，只要和获取到全局锁的事务操作的不是同一个字段，也是不受影响的。

正是由于不通过Seata框架直接对数据库的操作不受全局锁影响，所以虽然概率很低，但是也还是可能出现某个事务和使用了Seata框架中的事务操作同一个表的同一行的相同字段的情况。如果真出现了这种情况，Seata框架的解决办法就是触发警告，然后通过人工接入的方式来解决，如下图所示：

![image-20211117180250815](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211117180712.png) 

**解释说明**：其实Seata在保存快照的时候，是会保存两份的，即不仅会把更新前的数据保存为快照，还会把更新后的数据保存为快照。所以在事务一获取到全局锁后进行数据库操作时，如果还有一个非Seata管理的事务也对同样的数据进行了操作的话，一旦事务一操作有问题要回滚，Seata就会比对之前执行业务sql后的快照，看看数据库中的数据是不是和快照的一致，一致就回滚，不一致就说明自己在操作期间还有别的事务对数据库中相同的数据进行了操作，那就发出警告通知人工介入来进行解决。

###### 9.4.2.3 AT模式的优缺点

**AT模式的优点：**

- 一阶段完成直接提交事务，释放数据库资源，性能比较好；
- 利用全局锁实现读写隔离；
- 没有代码侵入，框架自动完成回滚和提交。

**AT模式的缺点：**

- 两阶段之间属于软状态，属于最终一致；
- 框架的快照功能会影响性能，但也比XA模式要好很多。

###### 9.4.2.4 实现AT模式

AT模式中的快照生成、回滚等动作都是由框架自动完成，没有任何代码侵入，因此实现非常简单。

1. 在我们自己微服务关联的数据库(库名为seata_demo)中执行以下sql：

   ```sql
   CREATE TABLE IF NOT EXISTS `undo_log` (
     `id` bigint(20) NOT NULL AUTO_INCREMENT,
     `branch_id` bigint(20) NOT NULL COMMENT 'branch transaction id',
     `xid` varchar(100) NOT NULL COMMENT 'global transaction id',
     `context` varchar(128) NOT NULL COMMENT 'undo_log context,such as serialization',
     `rollback_info` longblob NOT NULL COMMENT 'rollback info',
     `log_status` int(11) NOT NULL COMMENT '0:normal status,1:defense status',
     `log_created` datetime NOT NULL COMMENT 'create datetime',
     `log_modified` datetime NOT NULL COMMENT 'modify datetime',
     PRIMARY KEY (`id`),
     UNIQUE KEY `ux_undo_log` (`xid`,`branch_id`)
   ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;
   ```

2. 修改所有参与分布式事务的服务的application.yml文件，将事务模式修改为AT模式：

   ```yaml
   seata:
     data-source-proxy-mode: AT # 开启数据源代理的AT模式
   ```

   > **说明**：这里不设置`seata.data-source-proxy-mode`属性也可以，因为Seata默认使用的就是AT模式。

3. 给发起全局事务的入口类或方法添加`@GlobalTransactional`注解：

   ![image-20211117213401894](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211117220117.png) 

4. 重启服务进行测试：

   可以发现，当我们多个服务中有一个因为报错而回滚时，其他服务也会回滚。从account-service服务的控制台日志也可以看出确实已经进行了回滚操作：

   ![image-20211117214532409](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211117220123.png) 

   然后undo_log表里面也会增加快照的信息，不过不管事务最终是成功提交还是回滚，该表中的数据都会被立即删除掉，所以我们在表里面最终是看不到内容的。通过某种方式，我在表数据被删除前查询出来了数据，如下所示：

   ![image-20211117214852250](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211117220135.png) 

   > **说明**：以上undo_log表中rollback_info字段的内容我给放到[rollback_info.json](https://gitee.com/gongcqq/others/raw/master/seata/rollback_info.json)文件中了，该文件中的`beforeImage`属性和`afterImage`属性的内容对应的就是执行业务sql前和执行业务sql后的快照信息。

##### 9.4.3 TCC模式

###### 9.4.3.1 TCC模式原理

TCC(Try-Confirm-Cancel)模式与AT(Automatic Transaction)模式非常相似，每阶段都是独立事务，它们不同的是TCC通过人工编码来实现数据恢复，不用像AT模式那样先生成快照，然后在提交或回滚的时候再删除快照，减少了性能的损耗。TCC模式使用起来需要实现三个方法：

- `Try`：资源的检测和预留； 
- `Confirm`：完成资源操作业务，要求Try成功Confirm一定要能成功；
- `Cancel`：预留资源释放，可以理解为Try的反向操作。

比如说，有一个扣减用户余额的业务。假设账户A原来余额是100元，需要余额扣减30元。

![image-20211117230603105](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211117231405.png) 

阶段一(**Try**)：检查余额是否充足，如果充足则冻结金额增加30元，可用余额扣除30元。 

![image-20211117230642099](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211117231410.png) 

阶段二：假如要提交(**Confirm**)，则扣减掉冻结金额的30元，可用余额变成70元。

![image-20211117230803210](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211117231416.png) 

阶段二：如果要回滚(**Cancel**)，则扣减掉冻结金额的30元，增加到可用余额中，可用余额增加30元后再次变回100元。

![image-20211117230603106](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211117231419.png) 

TCC的工作模型图如下所示：

![image-20211117231622451](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211117231627.png) 

###### 9.4.3.2 TCC的优缺点

**TCC模式的优点：**

- 一阶段完成直接提交事务，释放数据库资源，性能好；
- 相比AT模式，无需生成快照，无需使用全局锁，性能最强；
- 不依赖数据库事务，而是依赖补偿操作，可以用于非事务型数据库。

**TCC模式的缺点：**

- 有代码侵入，需要人为编写try、Confirm和Cancel接口，太麻烦；
- 软状态，事务是最终一致；
- 需要考虑Confirm和Cancel的失败情况，做好幂等处理。

###### 9.4.3.3 空回滚和业务悬挂

当某分支事务的try阶段阻塞时，可能导致全局事务超时而触发二阶段的cancel操作。在未执行try操作的时候却先执行了cancel操作，这时cancel做回滚，就是`空回滚`。对于已经空回滚的业务，如果以后继续执行try操作，就永远不可能confirm或cancel，这就是`业务悬挂`。应当阻止执行空回滚后的try操作，避免悬挂。

###### 9.4.3.4 实现TCC模式

为了应对空回滚、防止业务悬挂以及实现幂等性要求，我们必须在数据库记录冻结金额的同时，记录当前事务id和执行状态，为此我们设计了一张account_freeze_tbl表，直接在我们的微服务所在的seata_demo库中执行即可：

```sql
CREATE TABLE `account_freeze_tbl` (
  `xid` varchar(128) NOT NULL,
  `user_id` varchar(255) DEFAULT NULL COMMENT '用户id',
  `freeze_money` int(11) unsigned DEFAULT '0' COMMENT '冻结金额',
  `state` int(1) DEFAULT NULL COMMENT '事务状态，0:try，1:confirm，2:cancel',
  PRIMARY KEY (`xid`) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT;
```

然后下面我们做一个业务分析，如下所示：

![image-20211117235213085](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211117235218.png) 

TCC的Try、Confirm、Cancel方法都需要在接口中基于注解来声明，语法如下：

![image-20211117235459149](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211117235523.png) 

下面编写详细的操作步骤：











##### 9.4.4 SAGA模式
























