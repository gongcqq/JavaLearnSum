### 1.认识微服务

随着互联网行业的发展，对服务的要求也越来越高，服务架构也从单体架构逐渐演变为现在流行的微服务架构。

#### 1.1 单体架构

`单体架构`：将业务的所有功能集中在一个项目中开发，打成一个包部署。

![image-20210713202807818](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903120529.png) 

单体架构的优缺点：

**优点：**

- 架构简单
- 部署成本低

**缺点：**

- 耦合度高(维护困难、升级困难)

#### 1.2 分布式架构

`分布式架构`：根据业务功能对系统做拆分，每个业务功能模块作为独立项目开发，称为一个服务。

![image-20210713203124797](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903120537.png) 

分布式架构的优缺点：

**优点：**

- 降低服务耦合
- 有利于服务升级和拓展

**缺点：**

- 服务调用关系错综复杂

分布式架构虽然降低了服务耦合，但是服务拆分时也有很多问题需要思考：

- 服务拆分的粒度如何界定？
- 服务之间如何调用？
- 服务的调用关系如何管理？

所以需要制定一套行之有效的标准来约束分布式架构。

#### 1.3 微服务架构

微服务的架构特征：

- `单一职责`：微服务拆分粒度更小，每一个服务都对应唯一的业务能力，做到单一职责；
- `自治`：团队独立、技术独立、数据独立，独立部署和交付；
- `面向服务`：服务提供统一标准的接口，与语言和技术无关；
- `隔离性强`：服务调用做好隔离、容错、降级，避免出现级联问题。

![image-20210713203753373](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903120550.png) 

微服务的上述特性其实是在给分布式架构制定一个标准，进一步降低服务之间的耦合度，提供服务的独立性和灵活性。做到高内聚，低耦合。

因此，可以认为**微服务**是一种经过良好架构设计的**分布式架构方案**。以下是微服务较为完整的架构图：

![微服务技术架构](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225113.jpg) 

但方案该怎么落地？微服务带来的各种问题怎么解决？选用什么样的技术栈？全球的互联网公司都在积极尝试自己的微服务落地方案，其中在Java领域最引人注目的就是SpringCloud提供的方案了，SpringCloud其实是一套解决微服务方案的技术整合。

#### 1.4 SpringCloud

[SpringCloud](https://spring.io/projects/spring-cloud)是目前国内使用最广泛的微服务框架，它集成了各种微服务功能组件，并基于SpringBoot实现了这些组件的自动装配，从而提供了良好的开箱即用体验。

其中常见的组件包括：

![image-20210713204155887](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903120601.png) 

SpringCloud底层是依赖于SpringBoot的，并且[官网](https://spring.io/projects/spring-cloud)也提供了版本的兼容关系，如下：

| Release Train                                                | Boot Version                          |
| :----------------------------------------------------------- | :------------------------------------ |
| [2020.0.x](https://github.com/spring-cloud/spring-cloud-release/wiki/Spring-Cloud-2020.0-Release-Notes) aka Ilford | 2.4.x, 2.5.x (Starting with 2020.0.3) |
| [Hoxton](https://github.com/spring-cloud/spring-cloud-release/wiki/Spring-Cloud-Hoxton-Release-Notes) | 2.2.x, 2.3.x (Starting with SR5)      |
| [Greenwich](https://github.com/spring-projects/spring-cloud/wiki/Spring-Cloud-Greenwich-Release-Notes) | 2.1.x                                 |
| [Finchley](https://github.com/spring-projects/spring-cloud/wiki/Spring-Cloud-Finchley-Release-Notes) | 2.0.x                                 |
| [Edgware](https://github.com/spring-projects/spring-cloud/wiki/Spring-Cloud-Edgware-Release-Notes) | 1.5.x                                 |
| [Dalston](https://github.com/spring-projects/spring-cloud/wiki/Spring-Cloud-Dalston-Release-Notes) | 1.5.x                                 |

#### 1.5 微服务技术对比

![image-20210903122239457](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903122252.png) 

#### 1.6 总结

- 单体架构：简单方便，高度耦合，扩展性差，适合小型项目。例如：学生管理系统
- 分布式架构：松耦合，扩展性好，但架构复杂，难度大。适合大型互联网项目，例如：京东、淘宝
- 微服务：一种良好的分布式架构方案
  - 优点：拆分粒度更小、服务更独立、耦合度更低
  - 缺点：架构非常复杂，运维、监控、部署难度提高
- SpringCloud是微服务架构的一站式解决方案，集成了各种优秀微服务功能组件

### 2.服务拆分和远程调用

任何分布式架构都离不开服务的拆分，微服务也是一样。

#### 2.1 服务拆分原则

微服务拆分时要遵循以下几个原则：

- 不同微服务，不要重复开发相同业务；
- 微服务数据独立，不要访问其它微服务的数据库；
- 微服务可以将自己的业务暴露为接口，供其它微服务调用。

![image-20210903162502053](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225130.png) 

#### 2.2 服务拆分示例

这里以微服务项目[cloud-parent](https://gitee.com/gongcqq/others/attach_files/821170/download/cloud-parent.zip)为例演示服务拆分，其结构如下：

![image-20210903214759456](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225153.png) 

`cloud-parent`：父工程，管理依赖。

- `order-service`：订单微服务，负责订单相关业务。
- `user-service`：用户微服务，负责用户相关业务。

要求：

- 订单微服务和用户微服务都必须有各自的数据库，相互独立；
- 订单服务和用户服务都对外暴露Restful的接口；
- 订单服务如果需要查询用户信息，只能调用用户服务的Restful接口，不能查询用户数据库。

##### 2.2.1 数据库设计

首先创建两个库，分别为**cloud_order**和**cloud_user**，然后不同库中执行以上`cloud-parent`项目中提供的不同sql脚本即可。

执行脚本后，cloud_user库中的t_user表的初始数据如下：

![image-20210903220430580](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225210.png) 

cloud_order库中的t_order表的初始数据如下：

![image-20210903220539317](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225215.png) 

> **说明**：t_order表中的user_id字段对应t_user表的id字段。

##### 2.2.2 导入微服务工程

首先将以上提供的代码导入到IDEA工程中，然后打开`Run Dashboard`面板，如下所示：

![image-20210903221334282](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225225.png) 

如果IDEA中选不到`Run Dashboard`面板(有的版本的IDEA是`Services`面板)，在项目的`.idea`目录的workspace.xml文件中添加以下内容后重启工程即可：

```xml
<option name="configurationTypes">
  <set>
    <option value="SpringBootApplicationConfigurationType" />
  </set>
</option>
```

![image-20210903222035404](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225235.png) 

生效后的`Run Dashboard`面板如下所示：

![image-20210903222418187](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225240.png) 

> **注意**：项目中用到的java的版本是jdk1.8。

#### 2.3 实现远程调用

订单模块的项目中是在启动类中使用`RestTemplate`类实现远程调用的，首先是使用如下代码将该类注入到Spring容器：

```java
@Bean
public RestTemplate restTemplate(){
    return new RestTemplate();
}
```

然后当订单模块的Service层获取订单信息的时候，使用如下方法通过远程调用将用户信息填充到Order对象中返回：

```java
@Override
public Order queryOrderById(Long orderId) {
    //1.查询订单
    Order order = orderMapper.findById(orderId);
    //2.利用RestTemplate发起http请求，查询用户
    //2.1 url路径
    String url = "http://localhost:8080/user/" + order.getUserId();
    //2.2 发送http请求，实现远程调用
    User user = restTemplate.getForObject(url, User.class);
    //3.封装user到Order中
    order.setUser(user);
    //4.返回
    return order;
}
```

在IDEA中分别启动订单模块和用户模块，然后浏览器中直接进行访问即可：

```http
http://localhost:8081/order/101
```

![image-20210903223928146](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225254.png) 

#### 2.4 提供者与消费者

在服务调用关系中，会有两个不同的角色：

`服务提供者`：一次业务中，被其它微服务调用的服务(提供接口给其它微服务)；

`服务消费者`：一次业务中，调用其它微服务的服务(调用其它微服务提供的接口)。

![image-20210903224606793](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210903225301.png) 

但是，服务提供者与服务消费者的角色并不是绝对的，而是相对于业务而言。

如果服务A调用了服务B，而服务B又调用了服务C，那么服务B的角色是什么？

- 对于A调用B的业务而言：A是服务消费者，B是服务提供者；
- 对于B调用C的业务而言：B是服务消费者，C是服务提供者。

因此，服务B既可以是服务提供者，也可以是服务消费者。

### 3.Eureka注册中心

#### 3.1 Eureka的结构

在上面远程调用的例子中，我们是把调用地址通过如下的一行代码写死在了代码中的：

```java
String url = "http://localhost:8080/user/" + order.getUserId();
```

直接写死就会有很多问题，比如，服务提供者如果是一个集群的话，那这个地址该写成什么合适？我们又如何实现远程调用呢？这时就需要用到注册中心了，我们可以先把各自的微服务注册到注册中心上，需要的时候通过注册中心完成远程调用即可，这一章节主要介绍的是Eureka注册中心的使用。

**Eureka注册中心的结构如下：**

![20210904200248](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904200602.jpg) 

**结构图解：**

1. user-service服务实例启动后，将自己的信息注册到eureka-server(Eureka服务端)，这个是`服务注册`；
2. eureka-server保存服务名称到服务实例地址列表的映射关系中，然后order-service根据服务名称，拉取实例地址列表，这个是`服务发现`或`服务拉取`；
3. 然后order-service从实例列表中利用`负载均衡`算法选中一个实例地址，并向该实例地址发起`远程调用`；
4. user-service会每隔一段时间(默认30秒)向eureka-server发起请求，报告自己的状态，称为**心跳**；当超过一定时间没有发送心跳时，eureka-server会认为对应的微服务实例出现了故障，便会将该实例从服务列表中剔除，这样一来，当order-service拉取服务时，就能将有故障的实例排除掉了。

> **说明**：一个微服务，既可以是服务提供者，又可以是服务消费者，因此eureka将服务注册、服务发现等功能统一封装到了eureka-client端。

#### 3.2 搭建Eureka服务端

Eureka服务端的搭建，需要创建一个独立的微服务，下面就在cloud-parent父工程下创建一个名为eureka-server的SpringBoot工程，目录结构如下所示：

![image-20210904204235259](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221633.png) 

然后我们需要在eureka-server工程对应的pom文件中引入以下依赖：

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>
</dependency>
```

之后我们需要在启动类中添加`@EnableEurekaServer`注解：

```java
package com.eureka;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;

@EnableEurekaServer
@SpringBootApplication
public class EurekaApplication {
    public static void main(String[] args) {
        SpringApplication.run(EurekaApplication.class, args);
    }
}
```

最后就是在application.yml文件中添加对应的配置，比如下面这样：

```yaml
server:
  port: 10086 # 服务端口
spring:
  application:
    name: eurekaserver # eureka的服务名称，自定义，不重复即可
eureka:
  client:
    service-url:  # eureka的地址信息
      defaultZone: http://localhost:10086/eureka
```

> **说明**：eureka自己也是一个微服务，所以eureka在启动的时候，会将自己也注册到eureka上，所以上面也定义了它自己的服务名称。

接下来就可以在eureka-server的启动类中启动eureka的服务端了，启动成功后直接在浏览器访问如下地址即可：

```http
http://localhost:10086/
```

![image-20210904210416023](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221642.png) 

#### 3.3 服务注册

接下来将我们的user-service服务和order-service服务都注册到Eureka上。

首先在各自工程的pom文件中都引入如下依赖：

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
</dependency>
```

然后在user-service工程的application.yml文件中添加如下内容：

```yaml
spring:
  application:
    name: userservice # user-service服务的服务名称
eureka:
  client:
    service-url:
      defaultZone: http://127.0.0.1:10086/eureka
```

而在order-service工程的application.yml文件中添加以下内容：

```yaml
spring:
  application:
    name: orderservice # order-service服务的服务名称
eureka:
  client:
    service-url:
      defaultZone: http://127.0.0.1:10086/eureka
```

然后就可以启动user-service服务以及order-service服务了，服务都启动成功后，我们再到eureka的服务端查看的时候就可以发现，已注册的服务名称中，除了eurekaserver(eureka自身服务的服务名)外，还包含了userservice和orderservice，说明我们自己的服务都已经成功注册到eureka上了。

![image-20210904214759080](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221649.png) 

我们也可以给自己的服务启多个实例，下面就以user-service服务为例，演示一下多实例的效果。本地同一个服务直接启动多个实例的话会端口冲突，所以我们需要变更一下端口，比如`-Dserver.port=8082`这样，步骤如下：

![image-20210904220144332](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221701.png) 

![image-20210904220643931](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221707.png) 

![image-20210904220851962](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221720.png) 

![image-20210904221046962](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221728.png) 

![image-20210904221431546](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904221735.png) 

#### 3.4 服务拉取和负载均衡

在前面的步骤中，已经把user-service服务和order-service服务都注册到eureka注册中心了，接下来就从eureka注册中心拉取user-service服务的实例列表并实现负载均衡。

首先在order-service的OrderApplication类中，给RestTemplate这个Bean添加一个`@LoadBalanced`注解：

```java
package com.order;

import org.mybatis.spring.annotation.MapperScan;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.client.loadbalancer.LoadBalanced;
import org.springframework.context.annotation.Bean;
import org.springframework.web.client.RestTemplate;

@MapperScan("com.order.mapper")
@SpringBootApplication
public class OrderApplication {

    public static void main(String[] args) {
        SpringApplication.run(OrderApplication.class, args);
    }

    @Bean
    @LoadBalanced
    public RestTemplate restTemplate(){
        return new RestTemplate();
    }
}
```

然后修改`com.order.service.impl.OrderServiceImpl#queryOrderById`方法中调用user-service服务的地址，把之前的地址和端口的部分改成user-service服务注册到eureka上的服务名，即`userservice`，如下所示：

```java
package com.order.service.impl;

import com.order.mapper.OrderMapper;
import com.order.pojo.Order;
import com.order.pojo.User;
import com.order.service.OrderService;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;
import org.springframework.web.client.RestTemplate;

/**
 * @Author: gongsl
 * @Date: 2021-09-02 23:35
 */
@Service
@Transactional
public class OrderServiceImpl implements OrderService {

    @Autowired
    private OrderMapper orderMapper;

    @Autowired
    private RestTemplate restTemplate;

    @Override
    public Order queryOrderById(Long orderId) {
        Order order = orderMapper.findById(orderId);
        //String url = "http://localhost:8080/user/" + order.getUserId();
        String url = "http://userservice/user/" + order.getUserId();
        User user = restTemplate.getForObject(url, User.class);
        order.setUser(user);
        return order;
    }
}
```

完成以上修改后，重启order-service服务，然后在浏览器访问以下地址：

```http
http://localhost:8080/order/102
```

![image-20210904223942005](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904225750.png) 

通过以上结果可知，已经通过eureka注册中心获取到了用户信息，我们可以访问多次，来测试负载均衡的效果，我访问了六次后，以下是控制台日志打印的结果：

![image-20210904224427752](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904225757.png) 

![image-20210904224815466](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904225806.png) 

![image-20210904225013553](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210904225816.png) 

> **说明**：启动两个user-service服务并且都注册到eureka上后，通过以上截图可知，当我们通过远程调用的方式去访问user-service服务的时候，每个user-service服务都有被访问到，说明确实实现了负载均衡。上面在进行访问测试的时候只访问了六次，我们也可以多访问几次进行测试，可以发现，默认使用的负载均衡策略是轮询策略。

### 4.Ribbon负载均衡

在上面3.4章节中，我们只是使用了一个`@LoadBalanced`注解，就实现了负载均衡的效果，这里面的原理是什么呢？我们如何改变负载均衡的策略呢？这一章节就是来回答这些问题的。

#### 4.1 负载均衡原理

SpringCloud的底层其实是利用了一个名为**Ribbon**的组件，来实现负载均衡功能的。

前面我们明明在代码中将调用用户模块的地址写成了类似`http://userservice/user/1`这种形式(其中`userservice`是user-service服务注册到eureka上的服务名称)，可是为什么还能轮询调到对应的user-service服务呢？下图就是负载均衡的原理图：

![image-20210713224517686](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135316.png) 

#### 4.2 源码分析

我们并没有通过具体的IP和端口，而是通过服务名就可以找到对应的user-service服务，主要依靠的是SpringCloud源码中的`org.springframework.cloud.client.loadbalancer.LoadBalancerInterceptor`类。

我们在代码中是使用如下`@LoadBalanced`注解的方式实现负载均衡的：

```java
@Bean
@LoadBalanced
public RestTemplate restTemplate(){
    return new RestTemplate();
}
```

当我们进行请求访问的时候，`LoadBalancerInterceptor`类就会对RestTemplate的请求进行拦截，然后从Eureka上根据服务名称获取服务列表，随后利用负载均衡算法得到真实的服务地址信息，替换服务名称。

现在我们在浏览器访问`http://localhost:8080/order/102`，然后通过打断点的方式进行源码跟踪：

##### 4.2.1 LoadBalancerInterceptor

首先在`LoadBalancerInterceptor`类的`intercept`方法上打断点，然后浏览器开始访问：

![image-20210905113104180](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135323.png) 

可以看到这里的intercept方法，已经拦截了用户的HttpRequest请求，并做了几件事：

- `request.getURI()`：获取请求的uri；
- `originalUri.getHost()`：获取url路径中的服务名称，比如userservice；
- `this.loadBalancer.execute()`：处理服务名称和用户请求。

这里的`this.loadBalancer`是`LoadBalancerClient`类型，我们继续跟入到`execute()`方法内部。

##### 4.2.2 RibbonLoadBalancerClient

接下来会进入到`org.springframework.cloud.netflix.ribbon.RibbonLoadBalancerClient`类中，该类是上面提到的`LoadBalancerClient`接口的实现类。

![image-20210905113828250](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135329.png) 

![image-20210905120003916](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135333.png) 

通过以上截图可知，这次访问获取到的是端口为8081的user-service服务，我们放行后再次访问，发现这次访问到的服务是端口为8082的了，说明确实实现了负载均衡。

![image-20210905120418054](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135338.png) 

##### 4.2.3 IRule

在上面截图的RibbonLoadBalancerClient类的execute方法中，可以发现，是使用如下两行代码来做负载均衡的：

```java
ILoadBalancer loadBalancer = getLoadBalancer(serviceId);
Server server = getServer(loadBalancer, hint);
```

那么我们就跟到`getServer()`方法中，看看到底使用的是哪种负载均衡策略：

![image-20210905121634843](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135345.png) 

![image-20210905121925079](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135352.png) 

然后我们会进入到`com.netflix.loadbalancer.BaseLoadBalancer`类的`chooseServer()`方法中，如下所示：

![image-20210905122603652](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135358.png) 

我们可以看下上图中成员变量rule的值，由下图可知，rule其实是`IRule`类型，默认值是`RoundRobinRule`对象。

![image-20210905123433641](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135405.png) 

如果我们打断点的话，可以发现，上面rule的值最终其实是`ZoneAvoidanceRule`对象，它和`RoundRobinRule`一样都是采用的轮询策略，具体它们有什么区别，下文会进行对比介绍。

##### 4.2.5 总结

SpringCloudRibbon的底层采用了一个拦截器，拦截了RestTemplate发出的请求，对地址做了修改。用一幅图来总结的话，就是下面这样：

![image-20210713224724673](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905135435.png) 

#### 4.3 负载均衡策略

##### 4.3.1 负载均衡策略类型

负载均衡的规则都定义在了IRule接口中，而IRule有很多不同的实现类：

![image-20210905162919415](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905164155.png) 

不同策略类型的含义如下：

![image-20210905164005682](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905164200.png) 

> **说明**：这里默认的实现是`ZoneAvoidanceRule`，它是一种轮询方案。

##### 4.3.2 自定义使用的策略

通过定义IRule的实现可以修改负载均衡的规则，从而改变我们所使用的负载均衡的策略，主要有以下两种方式：

1. **代码的方式。**我们可以在order-service工程的OrderApplication类(或配置类)中，定义一个新的IRule：

   ```java
   @Bean
   public IRule randomRule(){
       //将负载均衡策略从默认的轮询改为随机
       return new RandomRule();
   }
   ```

2. **配置文件的方式。**我们可以在order-service工程的application.yml中针对指定服务配置指定负载均衡策略：

```yaml
userservice: # 给某个微服务配置负载均衡规则，这里是userservice服务
  ribbon:
    NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule # 负载均衡规则 
```

> **注意**：我们一般使用默认的负载均衡策略即可，确需修改的，上述两种方式是有根本区别的。第一种通过代码的方式是针对全局生效的，如果order-service服务需要调用多个不同微服务的话，第一种方式就会将调用所有微服务的负载均衡策略都改为随机策略；第二种方式则是指定服务设置策略，也就是说，如果调用多个不同的服务的话，按照第二种方式进行配置，除了调用user-service服务会使用随机策略外，调用其余服务都还是使用默认的轮询策略。

#### 4.4 饥饿加载

Ribbon默认是采用懒加载，即第一次访问时才会去创建LoadBalanceClient，所以第一次请求访问时间会较长。而饥饿加载则会在项目启动时就创建，这样可以降低第一次访问的耗时。如果有需要，我们可以通过以下方式开启饥饿加载：

```yaml
# 在order-service工程的application.yml中加入以下内容
ribbon:
  eager-load:
    enabled: true # 开启饥饿加载
    clients:  
      - userservice # 指定对userservice这个服务饥饿加载
```

> **说明**：截止到目前为止的代码可以点击[cloud-parent](https://gitee.com/gongcqq/others/attach_files/821855/download/cloud-parent.zip)进行下载。

### 5.Nacos的使用

#### 5.1 Nacos注册中心

[Nacos](https://nacos.io/zh-cn/)是阿里巴巴的产品，它是[SpringCloudAlibaba](https://spring.io/projects/spring-cloud-alibaba)中的一个组件，相比于[Eureka](https://github.com/Netflix/eureka)而言功能更加丰富，在国内受欢迎程度更高。

##### 5.1.1 Nacos简介

Nacos致力于帮助我们发现、配置和管理微服务。它提供了一组简单易用的特性集，帮助我们快速实现动态服务发现、服务配置、服务元数据及流量管理。

Nacos也帮助我们更敏捷和容易地构建、交付和管理微服务平台，它是构建以"服务"为中心的现代应用架构(例如微服务范式、云原生范式)的服务基础设施。

Nacos的关键特性主要包括以下几点:

- 服务发现和服务健康监测；
- 动态配置服务；
- 动态DNS服务；
- 服务及其元数据管理。

##### 5.1.2 Nacos的安装

为了更贴近真实场景，这里介绍的是Nacos在linux下的安装使用。

1. 下载安装包并解压：

   ```shell
   # 下载安装包(2.0以上版本可能还存在一些bug，所以暂时使用1.4.1版本)
   wget https://github.com/alibaba/nacos/releases/download/1.4.1/nacos-server-1.4.1.tar.gz
   
   # 解压安装包
   tar -zxvf nacos-server-1.4.1.tar.gz
   ```

   解压后目录结构如下：

   ```shell
   [root@master ~]# tree nacos
   nacos
   ├── bin
   │   ├── shutdown.cmd
   │   ├── shutdown.sh
   │   ├── startup.cmd
   │   └── startup.sh
   ├── conf
   │   ├── 1.4.0-ipv6_support-update.sql
   │   ├── application.properties
   │   ├── application.properties.example
   │   ├── cluster.conf.example
   │   ├── nacos-logback.xml
   │   ├── nacos-mysql.sql
   │   └── schema.sql
   ├── LICENSE
   ├── NOTICE
   └── target
       └── nacos-server.jar
   ```

2. 启动nacos：

   ```shell
   # standalone代表着单机模式运行，非集群模式
   cd nacos/bin/ && sh startup.sh -m standalone
   ```

   ![image-20210905201114733](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224343.png) 

   > **说明**：启动nacos需要依赖java环境，并且版本要求是jdk1.8及其以上。

3. 访问nacos控制台：

   ```shell
   # nacos默认端口是8848，所以浏览器输入以下地址(ip要换成自己的主机ip)进行访问即可
   http://192.168.68.11:8848/nacos
   ```

   进入到界面中后，输入用户名密码进行登录即可，默认用户名密码都是`nacos`。

   ![image-20210905201725368](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224354.png) 

4. 关闭nacos服务：

   ```shell
   # 如果我们想关闭nacos的话，直接执行以下命令即可
   sh shutdown.sh
   ```

> **说明**：如果想了解更多关于nacos的安装以及详细的使用说明，可以参考[nacos官网](https://nacos.io/zh-cn/docs/quick-start.html)。

##### 5.1.3 服务注册到Nacos上

1. 在cloud-parent父工程的pom文件中的`<dependencyManagement>`标签内引入SpringCloudAlibaba的依赖：

   ```xml
   <dependency>
       <groupId>com.alibaba.cloud</groupId>
       <artifactId>spring-cloud-alibaba-dependencies</artifactId>
       <version>2.2.6.RELEASE</version>
       <type>pom</type>
       <scope>import</scope>
   </dependency>
   ```

2. 然后在user-service和order-service中的pom文件中引入nacos-discovery依赖：

   ```xml
   <dependency>
       <groupId>com.alibaba.cloud</groupId>
       <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
   </dependency>
   ```

   > **说明**：由于我们已经用不到eureka了，所以我们可以注释掉eureka的依赖。

3. 在user-service和order-service的application.yml中添加nacos地址信息：

   ```yaml
   spring:
     cloud:
       nacos:
         server-addr: 192.168.68.11:8848 # nacos服务的地址和端口
   ```

4. 启动user-service服务和order-service服务，然后在nacos控制台的`服务管理`下的`服务列表`中就可以看到我们注册到nacos上的服务信息了： 

   ![image-20210905211727576](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224406.png)   

   ![image-20210905210421700](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224413.png) 

   > **说明**：这时我们可以在浏览器输入类似`http://localhost:8080/order/101`这样的地址进行远程调用，看看服务都注册到nacos上后，order-service是否还能调到user-service，我这边测试是可以的。

##### 5.1.4 服务分级存储模型

一个**服务**可以有多个**实例**，例如我们的user-service，可以有：

- 127.0.0.1:8081
- 127.0.0.1:8082
- 127.0.0.1:8083

假如这些实例分布于全国各地的不同机房，例如：

- 127.0.0.1:8081，在上海机房
- 127.0.0.1:8082，在上海机房
- 127.0.0.1:8083，在杭州机房

Nacos会将同一机房内的实例，划分为一个**集群**。

也就是说，user-service是服务，一个服务可以包含多个集群，如杭州、上海，每个集群下可以有多个实例，形成分级模型，如图：

<img src="https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224421.png" alt="image-20210713232522531" style="zoom: 50%;" /> 

微服务互相访问时，应该尽可能访问同集群实例，因为本地访问速度更快。当本集群内不可用时，才访问其它集群。例如：

<img src="https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224430.png" alt="image-20210713232658928" style="zoom: 60%;" /> 

> **说明**：杭州机房内的order-service应该优先访问同机房的user-service。

###### 5.1.4.1 给微服务配置集群

修改user-service的application.yml文件，添加集群配置：

```yaml
spring:
  cloud:
    nacos:
      server-addr: 192.168.68.11:8848
      discovery:
        cluster-name: HZ # 集群名称，自定义，不重复即可
```

重启两个user-service实例后，我们可以在nacos控制台看到下面的结果：

![image-20210905223846857](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224438.png)  

我们也可以配置多集群，可以再复制一个user-service启动配置，添加属性：

```shell
-Dserver.port=8083 -Dspring.cloud.nacos.discovery.cluster-name=SH
```

![image-20210905223533361](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224443.png) 

![image-20210905224157724](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224450.png) 

启动UserApplication3后再次查看nacos控制台：

![image-20210905223942762](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905224458.png) 

###### 5.1.4.2 同集群优先的负载均衡

前面在讲Ribbon的负载均衡策略的时候，并没有根据同集群优先来实现负载均衡的策略。

因此Nacos中提供了一个`com.alibaba.cloud.nacos.ribbon.NacosRule`的实现，可以优先从同集群中挑选实例，下面就进行相关配置和演示。

1. 首先修改order-service的application.yml文件，添加集群配置：

   ```yaml
   spring:
     cloud:
       nacos:
         server-addr: 192.168.68.11:8848
         discovery:
           cluster-name: HZ # 这里将order-service添加到HZ集群
   ```

2. 再次修改order-service的application.yml文件，改变负载均衡的规则：

   ```yaml
   userservice:
     ribbon:
       NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule # 负载均衡规则 
   ```

3. 重启order-service服务进行访问测试：

   ![20210905230543](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905234326.gif) 

   由于我们将order-service添加到了HZ集群，并使用`NacosRule`作为负载均衡策略。所以正常情况下，当我们在浏览器访问order-service服务时，只会远程调用到同处于HZ集群的user-service服务，以上[动图](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905234326.gif)也确实印证了这一点。在获取到同集群的user-service服务列表后，会再通过随机负载均衡策略挑选一个服务进行远程调用。

   如果我们停掉同是HZ集群下的所有user-service服务，就会调用到SH集群下的user-service服务了，具体演示过程如以下[动图](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905235713.gif)：

   ![20210905235623](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210905235713.gif) 

##### 5.1.5 Nacos的权重配置

服务器设备的性能是有差异的，不排除会出现部分实例所在机器性能较好，另一些较差的情况。有时我们会希望性能好的机器承担更多的用户请求，但是如果我们使用同集群优先的负载均衡策略，则默认情况下`NacosRule`是同集群内随机挑选服务器，这样就不会考虑到机器性能的问题。

因此，Nacos提供了权重配置来控制访问频率，权重越大则访问频率越高，我们可以的将性能较好的机器的权重配置高一些，以便更多地承担用户请求。

我们可以直接在nacos的控制台上进行实例的权重修改，权重的范围是`0~1`，如果将实例的权重设置为`0`，就相当于将该实例下线，则该实例将永远不会再承担任何用户请求。

下面进行修改权重的操作演示：

![image-20211021210151843](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211021211731.png) 

![image-20211021210913378](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211021211738.png) 

![image-20211021211054987](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211021211746.png) 

上述设置完成之后，我们可以在浏览器访问`http://localhost:8080/order/101`进行测试，可以明显发现，进入到权重值配置为`0.2`的实例的流量明显少于另一个权重值为`1`的实例，说明权重的配置是生效的。

##### 5.1.6 Nacos环境隔离

Nacos提供了namespace来实现环境隔离功能。

- nacos中可以有多个namespace；
- namespace下可以有group、service等；
- 不同namespace之间相互隔离，不同namespace的服务互相是不可见的。

![image-20211023160642391](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023160659.png) 

###### 5.1.6.1 创建namespace

默认情况下，所有service、data、group都在同一个namespace下，名为`public`：

![image-20211023161010870](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023162152.png) 

我们可以点击页面新增按钮，添加一个namespace：

![image-20211023161853795](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023162209.png) 

![image-20211023162049377](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023162216.png) 

![image-20211023162129556](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023162232.png) 

###### 5.1.6.2 给微服务配置namespace

给微服务配置namespace只能通过修改配置来实现。

例如，修改order-service的application.yml文件(增加`namespace`属性)：

```yaml
spring:
  cloud:
    nacos:
      server-addr: 192.168.68.11:8848
      discovery:
        cluster-name: HZ
        namespace: 21fa5899-e0e5-4263-915e-a4ec7959c497 # 命名空间，填页面上的命名空间ID
```

启动order-service服务以及user-service服务，访问Nacos控制台，会看到下面的结果：

![image-20211023164407539](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023171657.png) 

![image-20211023164459207](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023171702.png) 

![image-20211023164539651](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023171706.png) 

可以发现，order-service服务已经被归类到`dev`命名空间下了，而user-service服务还是在默认的`public`命名空间下，由于不同命名空间下的服务是不可见的，所以即便在`public`命名空间下有三个user-service服务，当我们在浏览器通过`http://localhost:8080/order/101`进行访问测试的时候，还是会因为order-service找不到user-service服务而报错，比如下面这样：

![image-20211023170616230](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023171712.png) 

![image-20211023170632944](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023171719.png) 

> **说明**：如果我们修改user-service服务的配置文件，将它也放到`dev`命名空间下的话，order-service服务就能访问到它了。

##### 5.1.7 Nacos的实例类型

Nacos的服务实例主要分为两种类型：

- `临时实例`：这种实例如果宕机超过一定时间，会被从服务列表中剔除，这个是默认的实例类型；
- `非临时实例`：这种实例如果宕机，不会被从服务列表剔除，也可以叫永久实例。

我们可以在实例详情中查看一个实例是否为临时实例，如下所示：

![image-20211023213118649](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023235224.png) 

我们也可以通过修改服务配置的方式将服务设置为非临时实例，比如要将order-service服务修改为非临时实例，可以修改该服务的application.yml配置，如下所示：

```yaml
spring:
  cloud:
    nacos:
      discovery:
        ephemeral: false # 设置为非临时实例
```

![image-20211023222144747](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023235231.png) 

##### 5.1.8 Nacos与Eureka的区别

Nacos和Eureka整体结构类似，服务注册、服务拉取、心跳等待，但是也存在一些差异：

![image-20211023223400549](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211023235236.png) 

Nacos与eureka的共同点：

- 都支持服务注册和服务拉取；
- 都支持服务提供者心跳方式做健康检测。

Nacos与Eureka的区别：

- Nacos支持服务端主动检测提供者状态：临时实例采用心跳模式，非临时实例采用主动检测模式；
- 临时实例心跳不正常会被剔除，非临时实例则不会被剔除；
- Nacos支持服务列表变更的消息推送模式，服务列表更新更及时；
- Nacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式；Eureka采用AP方式。

#### 5.2 Nacos配置管理

Nacos除了可以做注册中心，同样可以用作配置管理。

##### 5.2.1 统一配置管理

当微服务部署的实例越来越多，达到数十、数百时，逐个修改微服务配置就会很麻烦，而且很容易出错。我们需要有一种统一的配置管理方案，可以集中管理所有实例的配置。

![image-20211024220612632](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211024220705.png) 

Nacos一方面可以将配置集中管理，另一方可以在配置变更时，及时通知微服务，实现配置的热更新。

###### 5.2.1.1 在nacos中添加配置文件

![image-20211024221117079](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211024223529.png) 

![image-20211024222035124](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211024223534.png) 

页面上配置的内容如下：

```yaml
pattern:
  dateformat: yyyy-MM-dd HH:mm:ss
```

> **注意**：并不是所有配置都应放在nacos上，像项目中可能会变动的配置以及需要热更新的配置才有放到nacos上进行管理的必要，一些像数据库的连接信息等基本不会变更的配置还是保存在微服务本地比较好。

###### 5.2.1.2 微服务拉取nacos上的配置

微服务需要拉取nacos中管理的配置，并且与本地的application.yml配置合并，才能完成项目的启动。但如果尚未读取本地的application.yml文件，我们又无法获取nacos的地址。所以这时候就需要用到`bootstrap.yml`文件了，这个文件是Spring引入的一种新的配置文件，其优先级高于application.yml。也就是说，项目在启动的时候，会优先加载该文件，所以我们只需将nacos的一些相关配置放到`bootstrap.yml`文件中即可。

![image-20211024223518936](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211024223545.png) 

1. 引入nacos-config依赖

   这里以user-service服务为例，首先在user-service服务中引入nacos-config的客户端依赖：

   ```xml
   <dependency>
       <groupId>com.alibaba.cloud</groupId>
       <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
   </dependency>
   ```

2. 在user-service项目的`resources`目录下添加一个bootstrap.yaml文件，内容如下：

   ```yaml
   spring:
     application:
       name: userservice # 服务名称，这里添加后，之前在application.yml中添加的就可以注释掉了
     profiles:
       active: dev # 开发环境，这里是dev
     cloud:
       nacos:
         server-addr: 192.168.68.11:8848 # 服务地址，记得注释掉在application.yml中重复添加的地址
         config:
           file-extension: yaml #对应nacos上配置文件的后缀名
   ```

   > **说明**：项目启动后，会根据以上服务名、环境以及后缀名去nacos上读取配置，在本例中，到nacos上读取到的配置名即为`userservice-dev.yaml`。

3. 通过代码方式验证是否成功读取到nacos上的配置：

   在user-service项目的com.user.controller.UserController类中添加如下业务逻辑：

   ```java
   @Value("${pattern.dateformat}")
   private String dateformat;
   
   @GetMapping("/now")
   public String now(){
       //根据指定格式对当前时间进行格式化处理
       log.info("日期格式：{}",dateformat);
       return LocalDateTime.now().format(DateTimeFormatter.ofPattern(dateformat));
   }
   ```

   ![image-20211024225108159](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211024225310.png) 

   启动项目后，浏览器访问`http://localhost:8081/user/now`进行验证： 

   ![image-20211024225818134](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211024225824.png) 

   ![image-20211024230120944](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211024230127.png) 

   > **说明**：通过以上浏览器访问到的结果可知，我们已经读取到naocs上的配置了，并将当前时间通过nacos上指定的配置进行了格式化展示。

##### 5.2.2 配置的热更新

修改nacos中的配置后，我们的服务无需重启即可让配置生效，这就是==配置的热更新==。

我们对配置的使用方式不同，会有不同的实现配置热更新的方式，主要有以下两种方式：

1. 使用`@Value`注解加`@RefreshScope`注解的方式；
2. 直接使用`@ConfigurationProperties`注解。

如果我们是通过`@Value`注解获取nacos配置管理中的某个配置，那么想要实现热更新的话，只需要在使用`@Value`注解所在的类上加上`@RefreshScope`注解即可，如下所示：

![image-20211026195236024](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211026195309.png) 

如果我们是在配置类中注入nacos配置管理中的配置的话，直接使用配置类的`@ConfigurationProperties`注解即可。下面创建一个配置类，其内容如下：

```java
package com.user.config;

import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.stereotype.Component;

/**
 * @Author: gongsl
 * @Date: 2021-10-26 20:01
 */
@Data
@Component
@ConfigurationProperties(prefix = "pattern")
public class ConfigProperties {
    
    private String dateformat;
}
```

然后测试类中直接使用`@Autowired`注解注入ConfigProperties类进行使用即可，比如下面这样：

```java
package com.user.controller;

import com.user.config.ConfigProperties;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.*;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;

/**
 * @Author: gongsl
 * @Date: 2021-09-02 22:13
 */
@Slf4j
@RestController
@RequestMapping("/user")
public class UserController {

    @Autowired
    private ConfigProperties properties;

    @GetMapping("/now")
    public String now(){
        //根据指定格式对当前时间进行格式化处理
        log.info("日期格式：{}",properties.getDateformat());
        DateTimeFormatter format = DateTimeFormatter.ofPattern(properties.getDateformat());
        return LocalDateTime.now().format(format);
    }
}
```

最后直接在浏览器进行访问测试就可以了。而且当我们修改nacos配置管理中的配置时，可以发现，我们不用重启自己的微服务就可以实现热更新，修改的配置是可以实时生效的。

##### 5.2.3 配置共享

###### 5.2.3.1 配置共享的案例演示

以下是在`bootstrap.yml`文件中的配置：

```yaml
spring:
  application:
    name: userservice
  profiles:
    active: dev
  cloud:
    nacos:
      server-addr: 192.168.68.11:8848
      config:
        file-extension: yaml
```

这么配置以后，当我们启动自己的服务时，不仅会读取nacos配置管理中`userservice-dev.yaml`文件里的配置，其实默认也会读取nacos配置管理中`userservice.yaml`文件里的配置，只不过我们没有在nacos上增加`userservice.yaml`文件，所以看不到有什么区别。

而`userservice.yaml`文件是不包含环境的，因此是可以被多个环境共享的，所以我们如果有什么配置是不同环境都会用到的，那就可以将配置放在`userservice.yaml`文件中。下面通过案例的方式来演示下配置共享。

1. 在nacos上增加一个环境共享的配置文件，这里是`userservice.yaml`文件，内容如下：

   ```yaml
   pattern:
     envSharedValue: 多环境共享属性值
   ```

   ![image-20211027115848780](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027115931.png) 

   ![image-20211027150730333](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027153648.png)  

2. 在项目中之前新增的`ConfigProperties`配置类里面加入上面新增的`envSharedValue`属性：

   ![image-20211027150638436](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027153655.png)  

   具体代码如下：

   ```java
   package com.user.config;
   
   import lombok.Data;
   import org.springframework.boot.context.properties.ConfigurationProperties;
   import org.springframework.stereotype.Component;
   
   @Data
   @Component
   @ConfigurationProperties(prefix = "pattern")
   public class ConfigProperties {
   
       private String dateformat;
       private String envSharedValue;
   }
   ```

3. 然后我们在测试类中新增一个方法以便进行测试，比如下面这个`envSharedTest`方法：

   ```java
   package com.user.controller;
   
   import com.user.config.ConfigProperties;
   import lombok.extern.slf4j.Slf4j;
   import org.springframework.beans.factory.annotation.Autowired;
   import org.springframework.web.bind.annotation.*;
   
   @Slf4j
   @RestController
   @RequestMapping("/user")
   public class UserController {
   
       @Autowired
       private ConfigProperties properties;
   
       @GetMapping("/test")
       public ConfigProperties envSharedTest(){
           return properties;
       }
   }
   ```

4. 接下来我们就可以启动user-service服务进行访问测试了，不过启动前还需要进行一些配置：

   为了验证`userservice.yaml`文件中的配置是可以在不同环境下共享的，我们将会启动两个user-service服务，端口为8081的服务使用的是==dev==环境，端口为8082的使用的是==test==环境。

   用目前的代码启动服务的话，默认使用的是dev环境，所以我们在使用8082端口启动服务的时候，需要通过IDEA的配置来改变环境，操作步骤如下：

   ![image-20211027144823842](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027153703.png)   

   ![image-20211027144617937](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027153719.png) 

5. 按照以上操作配置好之后，启动服务，然后浏览器进行访问测试即可：

   浏览器访问`http://localhost:8081/user/test`的结果如下：

   ![image-20211027151157208](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027153726.png)   

   **说明**：以上使用8081端口启动的服务使用的是dev环境，所以默认会读取nacos上的userservice-dev.yaml文件和userservice.yaml文件，所以==dateformat==字段和==envSharedValue==字段都能获取到对应配置中的值。

   当浏览器访问`http://localhost:8082/user/test`的时候，结果如下：

   ![image-20211027151232062](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027153733.png) 

   **说明**：以上使用8082端口启动的服务使用的是test环境，默认会去读取nacos上的userservice-test.yaml文件和userservice.yaml文件，但是nacos上我们并没有新增过userservice-test.yaml文件，所以这个文件是读取不到的。由于不会去读取userservice-dev.yaml文件，所以==dateformat==字段是获取不到值的，以上该字段的值为null是正常现象。由于也会读取userservice.yaml文件的内容，所以==envSharedValue==字段能获取到对应配置中的值。

###### 5.2.3.2 配置共享的优先级

当nacos中的配置和本地配置相同时，优先级顺序如下图所示：

![image-20211027152701337](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027153739.png) 

如果以user-service服务为例的话，优先级顺序就是下面这样：

```java
userservice-dev.yaml > userservice.yaml > application.yml
```

> **说明**：我们也可以在以上三个配置文件中都配置相同的属性及属性值，然后在代码的配置类中也增加该属性，之后浏览器进行访问测试，看看浏览器返回的该属性的属性值是哪个配置文件中的值就知道谁的优先级高了。另外，截止到目前为止的代码可以点击[cloud-parent](https://gitee.com/gongcqq/others/attach_files/863906/download/cloud-parent.zip)进行下载。

#### 5.3 Nacos集群的搭建

下面是官方给出的Nacos集群图：

![image-20210409210621117](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194542.png) 

其中包含3个nacos节点，然后一个负载均衡器代理3个nacos，这里负载均衡器我们可以使用nginx。正常而言，最起码的集群结构应该是下面这样：

![image-20210409211355037](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194548.png) 

但是为了方便起见，数据库就不再使用集群方式了，而且mysql、nginx和三个nacos都使用一台主机进行部署，这台主机的主机IP是`192.168.68.11`。为了避免端口冲突，三个nacos的端口将被分别设置为==8845==、==8846==和==8847==。然后下面就开始进行nacos的集群搭建。

##### 5.3.1 初始化数据库

Nacos默认数据存储在内嵌数据库Derby中，不属于生产可用的数据库，这里我们将nacos的数据存储到mysql数据库中。

登录到mysql数据库中后，我们首先新增一个库，这里起名为`nacos`，如下所示，可以看到我们新增的数据库：

```sql
mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| nacos              |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.00 sec)
```

然后我们在nacos库中执行初始化脚本即可。我们下载的nacos压缩包，解压后的`nacos/conf/`目录下就有这个初始化的脚本文件，文件名为`nacos-mysql.sql`，或者也可以通过如下命令进行获取，文件内容都是一样的：

```bash
wget https://gitee.com/gongcqq/others/raw/master/nacos-mysql.sql
```

执行完初始化脚本后，我们可以看到，nacos库下多了如下这些表：

```sql
mysql> show tables;
+----------------------+
| Tables_in_nacos      |
+----------------------+
| config_info          |
| config_info_aggr     |
| config_info_beta     |
| config_info_tag      |
| config_tags_relation |
| group_capacity       |
| his_config_info      |
| permissions          |
| roles                |
| tenant_capacity      |
| tenant_info          |
| users                |
+----------------------+
12 rows in set (0.00 sec)
```

##### 5.3.2 配置nacos集群

1. 使用如下命令下载并解压nacos：

   ```bash
   # 下载nacos
   wget https://github.com/alibaba/nacos/releases/download/1.4.1/nacos-server-1.4.1.tar.gz
   
   # 解压nacos
   tar -zxvf nacos-server-1.4.1.tar.gz
   ```

2. 重命名解压后文件中的集群配置文件：

   ```bash
   # 进入到配置文件目录中
   cd nacos/conf/
   
   # 将cluster.conf.example文件重命名为cluster.conf
   mv cluster.conf.example cluster.conf
   ```

3. 打开cluster.conf文件，将里面举例的主机集群配置删除掉，配置上自己的集群信息，即：

   ```xml
   192.168.68.11:8845
   192.168.68.11:8846
   192.168.68.11:8847
   ```

   ![image-20211027182647549](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194556.png) 

4. 然后修改application.properties文件，添加如下数据库配置：

   ```properties
   spring.datasource.platform=mysql
   
   db.num=1
   
   # 数据库的地址、用户名、密码要和自己的数据库保持一致
   db.url.0=jdbc:mysql://192.168.68.11:3306/nacos?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useUnicode=true&useSSL=false&serverTimezone=UTC
   db.user.0=root
   db.password.0=root
   ```

5. 接下来将解压的这个nacos再复制两份：

   ```bash
   # 由于前面已经进入到"nacos/conf/"目录下了，所以这里回到nacos的根目录
   cd ../../
   
   # 再复制两份nacos
   cp -r nacos nacos2 && cp -r nacos nacos3
   ```

6. 修改nacos的application.properties中`server.port`属性的值并启动：

   ```bash
   # 将第一个nacos的端口改为8845
   vim nacos/conf/application.properties
   
   # 将第二个nacos的端口改为8846
   vim nacos2/conf/application.properties
   
   # 将第三个nacos的端口改为8847
   vim nacos3/conf/application.properties
   
   # 端口修改完成之后，启动这三个nacos
   sh nacos/bin/startup.sh && sh nacos2/bin/startup.sh && sh nacos3/bin/startup.sh
   ```

7. 启动完成后，可以在浏览器分别访问如下地址，三个nacos都能访问到，说明都启动成功了：

   ```xml
   http://192.168.68.11:8845/nacos/
   http://192.168.68.11:8846/nacos/
   http://192.168.68.11:8847/nacos/
   ```

##### 5.3.3 配置nginx负载

1. 下载并解压nginx压缩包：

   ```bash
   # 下载nginx
   wget http://nginx.org/download/nginx-1.18.0.tar.gz
   
   # 解压nginx
   tar -zxvf nginx-1.18.0.tar.gz
   ```

2. 安装nginx：

   ```bash
   # 首先安装依赖包
   yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel
   
   # 然后依次执行以下命令
   cd nginx-1.18.0/ && ./configure
   make
   make install
   
   # 我这边防火墙已经关闭了，没有关闭的可以执行以下命令进行关闭
   systemctl stop firewalld
   ```

3. 安装nginx后的默认位置是`/usr/local/nginx/conf/`，下面修改nginx的`nginx.conf`文件，增加如下内容：

   ```xml
   upstream nacos-cluster {
       server 192.168.68.11:8845;
   	server 192.168.68.11:8846;
   	server 192.168.68.11:8847;
   }
   
   server {
       listen       80;
       server_name  localhost;
   
       location /nacos {
           proxy_pass http://nacos-cluster;
       }
   }
   ```

   ![image-20211027185846656](D:\Program Files (x86)\Typora\images\4.SpringCloud入门教程\image-20211027185846656.png) 

4. 配置修改完成后，启动nginx：

   ```bash
   # 执行以下命令启动nginx
   /usr/local/nginx/sbin/nginx
   ```

5. 浏览器访问`http://192.168.68.11/nacos`看能不能跳转到nacos的页面，我这边是可以的，如下所示：

   ![image-20211027191302925](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194605.png) 

##### 5.3.4 修改本地配置

既然nacos已经采用集群的方式进行搭建了，那么本地工程中的`application.yml`配置或者`bootstrap.yml`配置中关于nacos的地址肯定就要进行修改了。我这边地址是配置在`bootstrap.yml`配置文件中的，之前是这么配置的：

```yaml
spring:
  application:
    name: userservice
  profiles:
    active: dev
  cloud:
    nacos:
      server-addr: 192.168.68.11:8848
      config:
        file-extension: yaml
```

现在nacos集群是使用nginx做负载的，所以以上`server-addr`属性的值要配置成nginx的地址，修改后的配置如下：

```yaml
spring:
  application:
    name: userservice
  profiles:
    active: dev
  cloud:
    nacos:
      server-addr: 192.168.68.11:80
      config:
        file-extension: yaml
```

修改好本地配置后直接启动userservice服务，然后浏览器通过集群地址访问nacos，观察服务注册情况：

![image-20211027192533801](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194611.png) 

![image-20211027192727861](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194616.png) 

##### 5.3.5 演示数据同步

我们也可以在配置列表中增加一项配置，看看配置是否同步到三个nacos中了，以及数据库中是否也有保存。

当我们使用`http://192.168.68.11/nacos`地址访问nacos页面后，在页面新增一个配置，名为`userservice.yaml`，内容如下：

```yaml
cluster: nginx
```

![image-20211027193332393](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194621.png) 

然后当我们访问以下任意一个地址时，该配置都会展示在`配置列表`中，说明配置确实是同步的：

```xml
http://192.168.68.11:8845/nacos/
http://192.168.68.11:8846/nacos/
http://192.168.68.11:8847/nacos/
```

![image-20211027193604733](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194627.png) 

![image-20211027193637908](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194640.png) 

![image-20211027193705168](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027194648.png) 

我们到数据库中，也可以查到这条配置的配置信息：

```sql
mysql> select id,data_id,content,gmt_create,src_ip,type from config_info;         
+----+------------------+----------------+---------------------+---------------+------+
| id | data_id          | content        | gmt_create          | src_ip        | type |
+----+------------------+----------------+---------------------+---------------+------+
|  1 | userservice.yaml | cluster: nginx | 2021-10-27 11:28:28 | 192.168.68.11 | yaml |
+----+------------------+----------------+---------------------+---------------+------+
1 row in set (0.00 sec)
```

### 6.openFeign远程调用

以前使用RestTemplate进行远程调用时，代码是这样的：

![image-20211027222511417](D:\Program Files (x86)\Typora\images\4.SpringCloud入门教程\image-20211027222511417.png) 

使用以上的方式进行远程调用会存在以下问题：

- 代码可读性差，编程体验不统一；
- 参数复杂URL难以维护。

而[openFeign](https://github.com/OpenFeign/feign)则是一个声明式的http客户端，其作用就是帮助我们优雅地实现http请求的发送，解决上面提到的问题。

#### 6.1 使用案例

1. 首先在order-service服务的pom文件中引入相关依赖：

   ```xml
   <dependency>
       <groupId>org.springframework.cloud</groupId>
       <artifactId>spring-cloud-starter-openfeign</artifactId>
   </dependency>
   ```

2. 然后在order-service的启动类上添加`@EnableFeignClients`注解以开启Feign的功能：

   ```java
   package com.order;
   
   import org.mybatis.spring.annotation.MapperScan;
   import org.springframework.boot.SpringApplication;
   import org.springframework.boot.autoconfigure.SpringBootApplication;
   import org.springframework.cloud.openfeign.EnableFeignClients;
   
   @EnableFeignClients
   @MapperScan("com.order.mapper")
   @SpringBootApplication
   public class OrderApplication {
   
       public static void main(String[] args) {
           SpringApplication.run(OrderApplication.class, args);
       }
   }
   ```

3. 之后我们在order-service中新建一个client包，并在该包下新增一个`UserClient`接口，作为Feign的客户端：

   ```java
   package com.order.client;
   
   import com.order.pojo.User;
   import org.springframework.cloud.openfeign.FeignClient;
   import org.springframework.web.bind.annotation.GetMapping;
   import org.springframework.web.bind.annotation.PathVariable;
   
   /**
    * @Author: gongsl
    * @Date: 2021-10-27 22:02
    */
   @FeignClient("userservice")
   public interface UserClient {
   
       @GetMapping("/user/{id}")
       User findByUserId(@PathVariable("id") Long id);
   }
   ```

   这个客户端主要基于SpringMVC的注解来声明远程调用的信息，比如：

   - `服务名称`：userservice

   - `请求方式`：GET
   - `请求路径`：/user/{id}
   - `请求参数`：id
   - `返回值类型`：User

   这样依赖，openFeign就可以帮助我们发送http请求了，就无需使用RestTemplate来发送了。

4. 接下来我们修改order-service中OrderServiceImpl类的queryOrderById方法内容，以便使用openFeign来实现远程调用：

   ```java
   package com.order.service.impl;
   
   import com.order.client.UserClient;
   import com.order.mapper.OrderMapper;
   import com.order.pojo.Order;
   import com.order.pojo.User;
   import com.order.service.OrderService;
   import org.springframework.beans.factory.annotation.Autowired;
   import org.springframework.stereotype.Service;
   import org.springframework.transaction.annotation.Transactional;
   
   /**
    * @Author: gongsl
    * @Date: 2021-09-02 23:35
    */
   @Service
   @Transactional
   public class OrderServiceImpl implements OrderService {
   
       @Autowired
       private OrderMapper orderMapper;
       
       @Autowired
       UserClient userClient;
   
       @Override
       public Order queryOrderById(Long orderId) {
           Order order = orderMapper.findById(orderId);
           User user = userClient.findByUserId(order.getUserId());
           order.setUser(user);
           return order;
       }
   }
   ```

   ![image-20211027224756563](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027235001.png) 

5. 最后启动order-service服务和user-service服务，之后浏览器输入`http://localhost:8080/order/101`进行访问测试：

   ![image-20211027225703397](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027235008.png) 

   ![image-20211027225800563](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027235015.png) 

   > **说明**：在openFeign中默认是集成了Ribbon的，所以如果有多个服务提供者的话，是会实现负载均衡的。

#### 6.2 自定义配置

openFeign可以支持很多的自定义配置，如下表所示：

| 类型                 | 作用             | 说明                                                         |
| -------------------- | ---------------- | ------------------------------------------------------------ |
| `feign.Logger.Level` | 修改日志级别     | 包含四种不同的级别：==NONE==、==BASIC==、==HEADERS==、==FULL== |
| feign.codec.Decoder  | 响应结果的解析器 | http远程调用的结果做解析，例如解析json字符串为java对象       |
| feign.codec.Encoder  | 请求参数编码     | 将请求参数编码，便于通过http请求发送                         |
| feign. Contract      | 支持的注解格式   | 默认是SpringMVC的注解                                        |
| feign. Retryer       | 失败重试机制     | 请求失败的重试机制，默认是没有，不过会使用Ribbon的重试       |

> **说明**：一般情况下，默认值就能满足我们的使用，如果要自定义时，代码方式的话，只需要创建自定义的@Bean覆盖默认Bean即可。

下面以修改日志级别为例来演示如何自定义配置，openFeign的日志级别主要分为四种：

- `NONE`：不记录任何日志信息，这是默认值；
- `BASIC`：仅记录请求的方法，URL以及响应状态码和执行时间；
- `HEADERS`：在BASIC的基础上，额外记录了请求和响应的头信息；
- `FULL`：记录所有请求和响应的明细，包括头信息、请求体、元数据。

##### 6.2.1 配置文件的方式

我们可以在order-service服务的application.yml文件中加入以下内容：

```yaml
feign:
  client:
    config:
      default: # 这里用default就是全局配置，如果是写服务名称，则是针对某个微服务的配置
        loggerLevel: FULL # 日志级别
```

假如我们只想针对userservice服务修改日志级别，可以写成下面这样：

```yaml
feign:  
  client:
    config: 
      userservice: # 针对某个微服务的配置
        loggerLevel: FULL # 日志级别
```

配置完成之后，启动项目，然后浏览器输入`http://localhost:8080/order/101`进行访问测试即可。如果控制台没有打印对应的日志，那么我们可以在order-service服务的application.yml文件中再加上如下配置：

```yaml
logging:
  level:
    com:
      order: debug # com.order是包路径
```

![image-20211027233735489](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027235026.png) 

##### 6.2.2 java代码的方式

我们先创建一个config包，然后创建一个类，最后再声明一个Logger.Level的对象：

```java
package com.order.config;

import feign.Logger;
import org.springframework.context.annotation.Bean;

/**
 * @Author: gongsl
 * @Date: 2021-10-27 23:39
 */
public class DefaultFeignConfiguration {
    @Bean
    public Logger.Level feignLogLevel(){
        // 日志级别设置为BASIC
        return Logger.Level.BASIC;
    }
}

```

如果要**全局生效**，就需要将其放到启动类的`@EnableFeignClients`这个注解中：

```java
@EnableFeignClients(defaultConfiguration = DefaultFeignConfiguration.class)
```

![image-20211027234541209](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027235032.png) 

如果只是要**局部生效**，则把它放到对应的`@FeignClient`这个注解中：

```java
@FeignClient(value = "userservice", configuration = DefaultFeignConfiguration .class) 
```

![image-20211027234738933](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027235036.png) 

如果java代码书写没问题，但控制台没有打印对应的日志，那么我们可以在order-service服务的application.yml文件中再加上如下配置：

```yaml
logging:
  level:
    com:
      order: debug # com.order是包路径
```

![image-20211027234926132](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211027235042.png) 

#### 6.3 使用优化

openFeign底层发起http请求，是依赖于其它框架的，其底层客户端实现有以下三种选择：

- `URLConnection`：这个是默认实现，但不支持连接池；
- `Apache HttpClient`：支持连接池；
- `OKHttp`：支持连接池。

因此提高openFeign的性能主要手段就是使用**连接池**代替默认的`URLConnection`，这里我们用Apache的`HttpClient`来进行演示。

1. 在order-service中引入httpClient的依赖：

   ```xml
   <dependency>
       <groupId>io.github.openfeign</groupId>
       <artifactId>feign-httpclient</artifactId>
   </dependency>
   ```

2. 在order-service的application.yml中添加连接池等配置：

   ```yaml
   feign:
     httpclient:
       enabled: true # 开启feign对HttpClient的支持
       max-connections: 200 # 最大的连接数
       max-connections-per-route: 50 # 每个路径的最大连接数
   ```

3. 在`org.springframework.cloud.openfeign.FeignClientFactoryBean#loadBalance`上打断点，然后Debug方式启动order-service服务，可以发现，其底层client已经变成了Apache HttpClient：

   ![image-20211029221200734](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211029231533.png) 

#### 6.4 最佳实践

所谓最佳实践，就是使用过程中总结的经验，最好的一种使用方式。

仔细观察可以发现，openFeign的客户端与服务提供者的controller代码非常相似：

**UserClient：**

![image-20211029225507561](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211029231541.png) 

**UserController：**

![image-20211029225704610](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211029231545.png) 

那么有没有办法简化这种重复的代码编写呢？以下主要提供两种方法：

##### 6.4.1 继承方式

一样的代码可以通过继承来共享：

1. 定义一个API接口，利用定义方法，并基于SpringMVC注解做声明；
2. openFeign的客户端和Controller都集成该接口。

![image-20211029225946848](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211029231552.png)

**说明**：以上方式的优点是用简单的方式实现了代码共享， 但是也有明显的缺点，那就是服务提供方、服务消费方紧耦合，并且参数列表中的注解映射并不会继承，因此Controller中必须再次声明方法、参数列表、注解等。

##### 6.4.2 抽取方式

这种方式是指将openFeign的客户端抽取为独立模块，并把接口相关的实体类、openFeign相关的配置等都放到这个模块中，提供给所有服务消费者使用。例如，将UserClient、User、Feign的默认配置都抽取到一个`feign-api`工程中，打成包后，所有微服务引用该依赖包，即可直接使用。

![image-20211029230523851](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211029231925.png) 

以上方式可能会出现一个问题，以UserClient为例，抽取为独立工程后，包路径可能和之前未抽取的不一定，在这种情况下，order-service的`@EnableFeignClients`注解将无法扫描到`UserClient`，基于该问题，有以下两种解决方式：

**方式一：**

注解中指定指定要扫描的包路径：

```java
//假设UserClient的包路径是"cn.feign.clients"，order-service服务中的注解就可以这么写
@EnableFeignClients(basePackages = "cn.feign.clients")
```

**方式二：**

注解中指定需要加载的Client接口：

```java
//直接加载指定的接口，可以指定多个，如果只有一个，下面的大括号也可以省略
@EnableFeignClients(clients = {UserClient.class})
```

### 7.Gateway服务网关

Spring Cloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0以及 Project Reactor 等响应式编程和事件流技术开发的网关，它旨在为微服务架构提供一种简单有效的统一API路由管理方式。

#### 7.1 网关的作用

Gateway网关是我们服务的守门神，是所有微服务的统一入口，它的核心功能特性包括==请求路由==、==权限控制==和==限流==。

`权限控制`：网关作为微服务入口，需要校验用户是否有请求资格，如果没有则进行拦截。

`路由和负载均衡`：一切请求都必须先经过gateway网关，但网关不处理业务，而是根据某种规则，把请求转发到某个微服务，这个过程叫做路由。当然路由的目标服务有多个时，还需要做负载均衡。

`限流`：当请求流量过高时，在网关中按照下游的微服务能够接受的速度来放行请求，避免服务压力过大。

增加网关后的服务架构图：

![image-20211030161700586](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211031165353.png)

> **说明**：在SpringCloud中网关的实现包括`gateway`和`zuul`两种，Zuul是基于Servlet的实现，属于阻塞式编程。而SpringCloud Gateway则是基于Spring5中提供的WebFlux，属于响应式编程的实现，具备更好的性能。

#### 7.2 gateway快速入门

下面就演示下网关的基本路由功能。基本步骤如下：

- 创建SpringBoot工程gateway，引入网关依赖；
- 编写启动类；
- 编写基础配置和路由规则；
- 启动网关服务进行测试。

1. 创建gateway服务，引入依赖：

   创建一个子工程，名为**gateway**，然后在该工程的pom文件中引入以下依赖：

   ```xml
   <dependency>
       <groupId>org.springframework.cloud</groupId>
       <artifactId>spring-cloud-starter-gateway</artifactId>
   </dependency>
   
   <dependency>
       <groupId>com.alibaba.cloud</groupId>
       <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
   </dependency>
   ```

2. 编写启动类，内容如下：

   ```java
   package com.gateway;
   
   import org.springframework.boot.SpringApplication;
   import org.springframework.boot.autoconfigure.SpringBootApplication;
   
   /**
    * @Author: gongsl
    * @Date: 2021-10-30 16:32
    */
   @SpringBootApplication
   public class GatewayApplication {
   
       public static void main(String[] args) {
           SpringApplication.run(GatewayApplication.class, args);
       }
   }
   ```

3. 创建application.yml文件，编写基础配置和路由规则：

   ```yaml
   server:
     port: 10010 # 网关端口
   spring:
     application:
       name: gateway # 服务名称
     cloud:
       nacos:
         server-addr: 192.168.68.11:8848 # nacos地址
       gateway:
         routes: # 网关路由配置
           - id: user-service # 路由id，自定义，只要唯一即可
             # uri: http://127.0.0.1:8081 # 路由的目标地址，http就是固定地址
             uri: lb://userservice # 路由的目标地址，lb就是指负载均衡，后面跟服务名称
             predicates: # 路由断言，也就是判断请求是否符合路由规则的条件
               - Path=/user/** # 这个是按照路径匹配，只要以/user/开头就符合要求
   ```

   > **说明**：我们将符合`Path`规则的一切请求，都代理到`uri`参数指定的地址，在本例中，我们是将`/user/**`开头的请求，代理到`lb://userservice`，lb是指负载均衡，然后根据后面的服务名拉取服务列表，实现负载均衡。

4. 启动测试gateway服务和user-service服务进行测试：

   ![image-20211031145422931](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211031165402.png) 

   浏览器访问`http://localhost:10010/user/1`时，请求就会被转发到`http://userservice/user/1`这个url，因为符合`/user/**`规则，如下图所示：

   ![image-20211031163147942](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211031165410.png) 

**以上过程，整个访问的流程图如下所示：**

![image-20210714211742956](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211031165416.png) 

**总结：**

网关搭建步骤：

1. 创建项目，引入nacos服务发现和gateway依赖；

2. 配置application.yml，包括服务基本信息、nacos地址、路由。

路由配置包括：

1. `路由id`：路由的唯一标识；

2. `路由目标(uri)`：路由的目标地址，http代表固定地址，lb代表根据服务名负载均衡；

3. `路由断言(predicates)`：判断路由的规则；

4. `路由过滤器(filters)`：对请求或响应做处理。

> **说明**：路由过滤器上文没有介绍到，下面就重点来介绍下路由断言和路由过滤器的用法。

#### 7.3 断言工厂

我们在配置文件中写的断言规则只是字符串，这些字符串会被Predicate Factory读取并处理，然后转变为路由判断的条件，例如`Path=/user/**`是按照路径匹配的，这个规则是由以下这个类来处理的：

```java
org.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory
```

像上面这样的断言工厂在[SpringCloud Gateway](https://docs.spring.io/spring-cloud-gateway/docs/2.2.9.RELEASE/reference/html/#gateway-request-predicates-factories)中还有十几个:

![image-20211101212100655](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211101212220.png) 

#### 7.4 过滤器工厂

[GatewayFilter](https://docs.spring.io/spring-cloud-gateway/docs/2.2.9.RELEASE/reference/html/#gatewayfilter-factories)是网关中提供的一种过滤器，可以对进入网关的请求和微服务返回的响应做处理：

![image-20211101212350891](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211101212356.png) 

##### 7.4.1 路由过滤器的种类

Spring提供了31种不同的路由过滤器工厂，例如：

| 名称                 | 说明                         |
| :------------------- | :--------------------------- |
| AddRequestHeader     | 给当前请求添加一个请求头     |
| AddRequestParameter  | 给当前请求添加一个请求参数   |
| AddResponseHeader    | 给响应结果中添加一个响应头   |
| RemoveRequestHeader  | 移除当前请求的一个请求头     |
| RemoveResponseHeader | 从响应结果中移除有一个响应头 |
| RequestRateLimiter   | 限制请求的流量               |
| PrefixPath           | 给请求路径添加一个前缀       |
| SetStatus            | 设置响应的状态码             |

##### 7.4.2 案例演示

1. 修改gateway服务的application.yml文件，增加`filters`相关内容：

   ```yaml
   server:
     port: 10010
   spring:
     application:
       name: gateway
     cloud:
       nacos:
         server-addr: 192.168.68.11:8848
       gateway:
         routes:
           - id: user-service
             uri: lb://userservice
             predicates:
               - Path=/user/**
             filters:
               - AddRequestHeader=X-Request-red, blue
   ```

   ![image-20211102205906100](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211102205917.png) 

   > **说明**：当前过滤器是写在userservice路由下的，所以仅仅对访问userservice的请求有效。

2. 然后在user-service服务的`com.user.controller.UserController`文件中增加如下代码：

   ```java
   @GetMapping("/header")
       public String getHeader(@RequestHeader(value = "X-Request-red",
                                              required = false) String request) {
           return "X-Request-red："+request;
       }
   ```

   ![image-20211102180327030](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211102180715.png) 

3. 重启服务后，浏览器输入`http://localhost:10010/user/header`进行访问测试：

   ![image-20211102180429817](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211102180722.png) 

   > **说明**：通过浏览器返回的结果可知，请求头确实已经添加成功了，并且我们也可以成功地获取到其内容。

##### 7.4.3 默认过滤器

如果要对所有的路由都生效，则可以将过滤器工厂写到default下。格式如下：

```yaml
server:
  port: 10010
spring:
  application:
    name: gateway
  cloud:
    nacos:
      server-addr: 192.168.68.11:8848
    gateway:
      routes:
        - id: user-service
          uri: lb://userservice
          predicates:
            - Path=/user/**
      default-filters:
        - AddResponseHeader=X-Response-Default-Red, Default-Blue
```

![image-20211102205823811](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211102205937.png) 

#### 7.5 全局过滤器

上一节学习的过滤器，网关提供了31种，但每一种过滤器的作用都是固定的。如果我们希望拦截请求，做自己的业务逻辑则没办法实现。

##### 7.5.1 全局过滤器的作用

全局过滤器[GlobalFilter](https://docs.spring.io/spring-cloud-gateway/docs/2.2.9.RELEASE/reference/html/#global-filters)的作用也是处理一切进入网关的请求和微服务响应，与`GatewayFilter`的作用一样。区别在于GatewayFilter通过配置定义，处理逻辑是固定的，而`GlobalFilter`的逻辑需要自己写代码实现。

定义方式是实现GlobalFilter接口：

```java
public interface GlobalFilter {
    /**
     *  处理当前请求，有必要的话通过{@link GatewayFilterChain}将请求交给下一个过滤器处理
     *
     * @param exchange 请求上下文，里面可以获取Request、Response等信息
     * @param chain 用来把请求委托给下一个过滤器 
     * @return {@code Mono<Void>} 标示当前过滤器业务结束
     */
    Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain);
}
```

在GlobalFilter中编写自定义逻辑，可以实现下列功能：

- 登录状态判断；
- 权限校验；
- 请求限流等。

##### 7.5.2 自定义全局过滤器

下面通过自定义一个过滤器的方式来进行请求参数的校验，具体逻辑如下：

1. 在gateway工程中创建一个filters包，然后再在下面创建一个自定义的`MyDefaultFilter`类：

   ```java
   package com.gateway.filters;
   
   import org.springframework.cloud.gateway.filter.GatewayFilterChain;
   import org.springframework.cloud.gateway.filter.GlobalFilter;
   import org.springframework.core.annotation.Order;
   import org.springframework.http.HttpStatus;
   import org.springframework.stereotype.Component;
   import org.springframework.util.MultiValueMap;
   import org.springframework.web.server.ServerWebExchange;
   import reactor.core.publisher.Mono;
   
   /**
    * @Author: gongsl
    * @Date: 2021-11-02 21:21
    */
   @Order(-1)
   @Component
   public class MyDefaultFilter implements GlobalFilter {
   
       @Override
       public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
           //1.获取请求参数
           MultiValueMap<String, String> params = exchange.getRequest().getQueryParams();
           //2.获取指定参数的参数值
           String user = params.getFirst("user");
           //3.进行参数值的校验
           if ("admin".equals(user)) {
               //满足要求，放行
               return chain.filter(exchange);
           }
           //4.不满足要求的则要进行拦截
           //4.1 禁止访问，设置状态码
           exchange.getResponse().setStatusCode(HttpStatus.FORBIDDEN);
           //4.2 结束流程
           return exchange.getResponse().setComplete();
       }
   }
   
   ```

   > **说明**：如果想要自定义全局过滤器，只要继承`GlobalFilter`接口即可。另外，上面的`@Order`注解是用于设置过滤器的执行顺序的，其值越小，越先执行，我们也可以实现`org.springframework.core.Ordered`接口，通过里面的`getOrder`方法同样可以设置执行顺序，和使用注解的方式效果是一样的。

2. 然后直接重启项目后浏览器进行访问测试即可：

   ![image-20211102214232750](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211102220941.png) 

   ![image-20211102214257251](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211102220947.png) 

   > **说明**：由以上访问结果可知，只有当请求参数中包含`user=admin`这样的参数及其参数值时，才能访问成功，否则就会被拒绝访问，并且响应码也是我们设置的403。

##### 7.5.3 过滤器的执行顺序

当请求进入到网关后，可能会碰到三类过滤器：当前路由的过滤器、DefaultFilter以及GlobalFilter。

请求路由后，会将当前路由的过滤器、DefaultFilter以及GlobalFilter合并到一个过滤器链(集合)中，排序后依次执行每个过滤器：

![image-20211102215930728](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211102220954.png) 

具体的排序规则：

- 每一个过滤器都必须指定一个int类型的order值，**order值越小，优先级越高，执行顺序越靠前**；
- GlobalFilter通过实现Ordered接口，或者添加@Order注解来指定order值，是由我们自己指定的；
- 而当前路由的过滤器和DefaultFilter的order则是由Spring指定的，默认是按照声明顺序从1递增；
- 当过滤器的order值一样时，会按照`DefaultFilter > 当前路由的过滤器 > GlobalFilter`的顺序执行。

详细内容，可以查看源码：

源码中的`org.springframework.cloud.gateway.route.RouteDefinitionRouteLocator#getFilters()`方法是先加载defaultFilters，然后再加载某个route的filters，最后再合并。

而`org.springframework.cloud.gateway.handler.FilteringWebHandler#handle()`方法会加载全局过滤器，与前面的过滤器合并后根据order排序，然后组织过滤器链。

#### 7.6 跨域问题

##### 7.6.1 什么是跨域问题

跨域主要包括：

- ==域名不同==：比如`www.taobao.com` 和 `www.taobao.org`；
- ==域名相同，端口不同==：比如`localhost:8080`和`localhost:8081`。

跨域问题：浏览器禁止请求的发起者与服务端发生跨域ajax请求，请求被浏览器拦截的问题。

解决方案：**CORS**，不了解的也可以参考[相关博客](https://www.ruanyifeng.com/blog/2016/04/cors.html)。

##### 7.6.2 gateway的跨域解决方案

网关处理跨域同样采用的是CORS方案，我们只需在gateway服务的application.yml文件中，添加下面的配置即可：

```yaml
spring:
  cloud:
    gateway:
      # ...
      globalcors: # 全局的跨域处理
        add-to-simple-url-handler-mapping: true # 解决options请求被拦截问题
        corsConfigurations:
          '[/**]': # 对哪些请求进行跨域处理，这里是指一切请求
            allowedOrigins: # 允许哪些网站的跨域请求 
              - "http://localhost:8090"
            allowedMethods: # 允许的跨域ajax的请求方式
              - "GET"
              - "POST"
              - "DELETE"
              - "PUT"
              - "OPTIONS"
            allowedHeaders: "*" # 允许在请求中携带的头信息
            allowCredentials: true # 是否允许携带cookie
            maxAge: 360000 # 指这次跨域检测的有效期，有效期范围内符合要求的，浏览器将不再询问，直接放行
```

> **说明**：截止到目前为止的代码可以点击[cloud-parent](https://gitee.com/gongcqq/others/attach_files/869761/download/cloud-parent.zip)进行下载。

### 8.Sentinel的使用

#### 8.1 雪崩问题及解决方案

在微服务架构中，会频繁进行服务之间的调用。一旦某个被调用方服务出现了故障，高并发情况下，由于资源无法及时得到释放，将会很快耗尽上游调用方的tomcat资源，从而拖垮上游服务，依次类推，整个微服务链路都将受到影响，从而出现雪崩问题，如下图所示：

![image-20211103180337537](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180350.png)  

![image-20211103180321223](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180357.png)   

> 微服务调用链路中的某个服务故障，引起整个链路中的所有微服务都不可用，这就是`雪崩`。

解决雪崩问题的常见方式有四种：

- `超时处理`：设定超时时间，请求超过一定时间没有响应就返回错误信息，不会无休止地等待；

- `舱壁模式`：限定每个业务能使用的线程数，避免耗尽整个tomcat的资源，因此也叫线程隔离；
- `熔断降级`：由**断路器**统计业务执行的异常比例，如果超出阈值则会**熔断**该业务，拦截访问该业务的一切请求；
- `流量控制`：限制业务访问的QPS，避免服务因流量的突增而故障。

**超时处理**可以在一定程度上有效缓解服务压力，但并不能完全解决雪崩问题。比如下图，我们可以在进行服务调用时设置一个1秒的等待时间，超过这个时间没返回就结束调用，以便尽快释放线程资源，不至于因为服务C的故障拖垮服务A，但是如果1秒内并发量很大的话，之前的线程资源还没来得及释放，就又会占用大量新的资源，这样还是可能导致tomcat的资源被耗尽，从而出现雪崩问题。

![image-20211103130502401](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180406.png) 

**舱壁模式**使用的是线程隔离的方法，就是给每个业务限制线程数，这样一来，即便某个业务对应的下游服务出现了故障，也不会将自己拖垮，这种方式可以避免雪崩问题的发生。但是也可能会造成资源的浪费，毕竟即便某个业务对应的下游服务已经故障了，业务也不再去调用了，但是该业务还是占用着自身服务的线程数，这些空闲的线程也无法被别的业务所使用。

![image-20211103150151889](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180411.png) 

**熔断降级**的方式要比舱壁模式好一些，这种方式是统计业务的异常比例，超出指定阈值就拦截该业务的一切请求，也不会造成线程数的浪费。

![image-20211103152829683](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180421.png) 

**流量控制**属于一种预防手段，通过限制业务量的方式，避免因为业务量过高而使服务发生故障。

![image-20211103153404212](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180434.png) 

目前业界知名的`Sentinel`和`Hystrix`都包含了以上提到的解决方案，但是它们俩也是有区别的，具体区别如下：

![image-20211103153949743](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180443.png) 

> 通过以上区别对比可知，`Sentinel`相对优秀一些，所以下文将会对`Sentinel`进行详细讲解。

#### 8.2 Sentinel的简介

##### 8.2.1 认识Sentinel

随着微服务的流行，服务和服务之间的稳定性变得越来越重要。[Sentinel](https://sentinelguard.io/zh-cn/index.html)是面向分布式服务架构的流量控制组件，主要以流量为切入点，从流量控制、熔断降级、系统自适应保护等多个维度来帮助我们保障微服务的稳定性，它具有以下特征：

- `丰富的应用场景`：Sentinel承接了阿里巴巴近10年的双十一大促流量的核心场景，例如秒杀(即突发流量控制在系统容量可以承受的范围)、消息削峰填谷、集群流量控制、实时熔断下游不可用应用等；
- `完备的实时监控`：Sentinel同时提供实时的监控功能，我们可以在控制台中看到接入应用的单台机器秒级数据，甚至500台以下规模的集群的汇总运行情况；
- `广泛的开源生态`：Sentinel提供开箱即用的与其它开源框架/库的整合模块，例如与Spring Cloud、Dubbo、gRPC的整合，我们只需要引入相应的依赖并进行简单的配置即可快速地接入Sentinel；
- `完善的SPI扩展点`：Sentinel同时还提供了简单易用且完善的SPI扩展接口，我们可以通过实现扩展接口来快速地定制逻辑，例如定制规则管理、适配动态数据源等。

##### 8.2.2 Sentinel的功能和设计理念

###### 8.2.2.1 流量控制

流量控制在网络传输中是一个常用的概念，它用于调整网络包的发送数据。然而，从系统稳定性角度考虑，在处理请求的速度上，也有非常多的讲究。任意时间到来的请求往往是随机不可控的，而系统的处理能力是有限的。我们需要根据系统的处理能力对流量进行控制。Sentinel作为一个调配器，可以根据需要把随机的请求调整成合适的形状，如下图所示：

![image-20211103161757517](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180451.png) 

流量控制有以下几个角度:

- 资源的调用关系，例如资源的调用链路，资源和资源之间的关系；
- 运行指标，例如 QPS、线程池、系统负载等；
- 控制的效果，例如直接限流、冷启动、排队等。

Sentinel的设计理念是让我们自由选择控制的角度，并进行灵活组合，从而达到想要的效果。

###### 8.2.2.2 熔断降级

**什么是熔断降级**

除了流量控制以外，降低调用链路中的不稳定资源也是Sentinel的使命之一。由于调用关系的复杂性，如果调用链路中的某个资源出现了不稳定，最终会导致请求发生堆积。

![image-20211103162319901](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180500.png) 

Sentinel和Hystrix的原则是一致的：当调用链路中某个资源出现不稳定，例如，表现为timeout，异常比例升高的时候，则对这个资源的调用进行限制，并让请求快速失败，避免因影响到其它的资源，最终产生雪崩的效果。

**熔断降级的设计理念**

在限制的手段上，Sentinel采取了和Hystrix完全不一样的方法。

Hystrix通过[线程池](https://github.com/Netflix/Hystrix/wiki/How-it-Works#benefits-of-thread-pools)的方式，来对资源进行了隔离。这样做的好处是资源和资源之间做到了最彻底的隔离。缺点是除了增加了线程切换的成本，还需要预先给各个资源做线程池大小的分配。

Sentinel对这个问题采取了两种手段：

- 通过并发线程数进行限制：

  和资源池隔离的方法不同，Sentinel通过限制资源并发线程的数量，来减少不稳定资源对其它资源的影响。这样不但没有线程切换的损耗，也不需要预先分配线程池的大小。当某个资源出现不稳定的情况下，例如响应时间变长，对资源的直接影响就是会造成线程数的逐步堆积。当线程数在特定资源上堆积到一定的数量之后，对该资源的新请求就会被拒绝，堆积的线程完成任务后才开始继续接收请求。

- 通过响应时间对资源进行降级：

  除了对并发线程数进行控制以外，Sentinel还可以通过响应时间来快速降级不稳定的资源。当依赖的资源出现响应时间过长后，所有对该资源的访问都会被直接拒绝，直到过了指定的时间窗口之后才重新恢复。

###### 8.2.2.3 系统负载保护

Sentinel同时提供[系统维度的自适应保护能力](https://sentinelguard.io/zh-cn/docs/system-adaptive-protection.html)。防止雪崩，是系统防护中重要的一环。当系统负载较高的时候，如果还持续让请求进入，可能会导致系统崩溃，无法响应。在集群环境下，网络负载均衡会把本应这台机器承载的流量转发到其它的机器上去。如果这个时候其它的机器也处在一个边缘状态，那么这个增加的流量就会导致这台机器也崩溃，最后导致整个集群不可用。

针对这种情况，Sentinel提供了对应的保护机制，让系统的入口流量和系统的负载达到一个平衡，保证系统在能力范围之内处理最多的请求。

#### 8.3 安装和使用Sentinel

为了方便我们操作，Sentinel官方提供了UI控制台，我们可以直接到[GitHub](https://github.com/alibaba/Sentinel/releases)上下载相关jar包，如果网速不行，也可以使用[备用地址](https://gongsl.lanzoui.com/izSNMw3kq0b)进行下载。然后直接使用`java -jar sentinel-dashboard-1.8.1.jar`命令进行启动即可，默认使用的端口是8080，控制台默认用户名和密码都是==sentinel==。

![image-20211103172557041](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180510.png) 

![image-20211103172746259](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180514.png) 

![image-20211103172834899](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180521.png) 

启动控制台后，下面在我们的微服务中引入Sentinel的相关依赖。在order-service工程的pom文件中加入以下内容：

```xml
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>
</dependency>
```

然后我们再在order-service工程的application.yml文件中配置Sentinel的控制台地址：

```yaml
spring:
  cloud:
    sentinel:
      transport:
        dashboard: 192.168.68.11:8080
```

以上配置完成之后，启动order-service工程和user-service工程，然后调用order-service的任意接口，以便触发Sentinel监控，比如多次调用`http://localhost:8080/order/101`接口，然后到Sentinel控制台查看监控情况：

![image-20211103175520668](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211103180533.png) 

#### 8.4 限流规则

##### 8.4.1 快速入门

在sentinel的控制台界面上，有一个`簇点链路`的菜单，这个链路其实就是项目内的调用链路，链路中**被监视**的每个接口就是一个资源。默认情况下sentinel会监控SpringMVC的每一个端点(Endpoint)，其实就是我们代码中controller层里面每个使用了`@RequestMapping`注解并设置了调用地址的方法。SpringMVC的每一个端点都对应着调用链路中的一个资源，流控、熔断等都是**针对簇点链路中的资源**来设置的，因此我们可以点击对应资源后面的按钮来设置规则：

![image-20211104150121681](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104150627.png) 

我们以名为`/order/{orderId}`的资源为例，点击操作中的**流控**按钮就会弹出一个表单，我们可以在表单中对该资源添加流控规则，如下图所示：

![image-20211104152228956](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165609.png) 

![image-20211104152330201](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165616.png)  

> **说明**：以上规则的含义是，限制`/order/{orderId}`这个资源的单机QPS为5，即每秒只允许5次请求，超出的请求会被拦截并报错。

然后我们利用jmeter压测工具访问`http://localhost:8080/order/101`这个地址进行测试，线程属性配置如下，两秒内访问20次，即每秒访问10次。

![image-20211104153007486](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165627.png) 

测试结果如下所示：

![image-20211104153702299](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165634.png) 

> 可以发现，每秒只有前五次是访问成功的，后五次都是失败的，由失败原因可知，是被sentinel限流了。

##### 8.4.2 流控模式

在添加限流规则时，点击高级选项，可以选择三种流控模式：

- `直接`：统计当前资源的请求，触发阈值时对当前资源直接限流，也是默认的模式；
- `关联`：统计与当前资源相关的另一个资源，触发阈值时，对当前资源限流；
- `链路`：统计从指定链路访问到本资源的请求，触发阈值时，对指定链路限流。

![image-20211104154520470](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165642.png) 

###### 流控模式-直接

这是一种默认的流控模式，该模式的效果在快速入门的时候已经演示过了，所以这里就不再赘述了。下面主要演示`关联`模式和`链路`模式的效果。

###### 流控模式-关联

**使用场景**：比如用户支付时需要修改订单状态，同时用户要查询订单。查询和修改操作会争抢数据库锁，产生竞争。业务需求是修改业务为主，因此当修改订单业务触发阈值时，需要对查询订单业务限流。

为了方便演示，下面在`OrderController`类中新增如下内容：

```java
//查询操作
@GetMapping("/select")
public String select() {
    return "select method!";
}

//修改操作
@GetMapping("/update")
public String update() {
    return "update method!";
}
```

重启order-service服务后依次访问一下查询操作和修改操作的url，以便触发sentinel监控，然后在sentinel控制台配置限流规则。由于是要对查询业务限流，所以就对该资源进行限流规则的配置：

![image-20211104161455950](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165649.png) 

![image-20211104161605316](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165654.png) 

![image-20211104161720754](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165701.png) 

> **说明**：以上将`/order/select`资源的阈值设置为5的含义是，当对`/order/update`的访问触发阈值(即每秒有超过五个以上请求进行访问)时，就会对`/order/select`资源进行限流。

然后我们利用jmeter压测工具访问`http://localhost:8080/order/update`这个地址进行测试，线程属性配置如下，即每秒访问十次，这个QPS肯定已经超过了`/order/select`资源的阈值。

![image-20211104163614543](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165706.png) 

线程属性配置完成之后，启动访问，此时`/order/select`资源的访问都是没有问题的：

![image-20211104164011811](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165713.png) 

在持续访问`/order/update`资源的时候，我们再在浏览器访问`http://localhost:8080/order/select`，由于修改操作的QPS已经超过了对查询操作设置的阈值，所以当我们访问查询操作的时候，是会被限流的，如下所示：

![image-20211104164649737](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104165718.png) 

> **说明**：如果存在两个有竞争关系的资源，并且一个优先级高，一个优先级低，那么在限流的时候就可以使用关联模式。

###### 流控模式-链路

**使用场景**：比如有查询订单和创建订单业务，两者都需要查询商品。实际场景下，查询订单的请求肯定远高于创建订单的请求，如果查询订单的并发过高，肯定会影响到创建订单的业务。所以我们可以针对从查询订单进入到查询商品的请求做一个统计，并进行限流，而从创建订单进入到查询商品的请求不做限制。

为了方便演示，我们在order-service工程中加入以下代码：

- 在`OrderService`接口中增加如下代码：

  ```java
  void queryGoods();
  ```

- 在`OrderServiceImpl`类中增加如下代码：

  ```java
  @Override
  @SentinelResource("goods")
  public void queryGoods() {
      System.err.println(">>>查询商品...");
  }
  ```

  > **说明**：sentinel默认只会监控controller层中的端点，如果想让它也监控Service层的端点的话，就需要用到上述`@SentinelResource`注解才行，注解中的值是资源名。

- 在`OrderController`类中增加如下代码：

  ```java
  //查询订单
  @GetMapping("/query")
  public String query() {
      orderService.queryGoods();
      return "query order!";
  }
  
  //创建订单
  @GetMapping("/save")
  public String save() {
      orderService.queryGoods();
      return "save order!";
  }
  ```

- 由于sentinel默认会将Controller方法做context整合，导致链路模式的流控失效，所以我们需要在配置文件中关闭这个整合，以便上述在Service层添加的@SentinelResource生效。修改application.yml，添加如下配置：

  ```yaml
  spring:
    cloud:
      sentinel:
        web-context-unify: false # 关闭context整合
  ```

添加完以上代码配置后，重启order-service服务，然后依次访问一次查询订单和创建订单的url，以便触发sentinel监控，之后就可以对goods资源配置流控规则了，如下所示：

![image-20211104175603739](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104194250.png) 

![image-20211104180625860](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104194258.png) 

![image-20211104180727848](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104194306.png) 

然后我们利用jmeter压测工具同时访问以下两个url：

```http
http://localhost:8080/order/query
http://localhost:8080/order/save
```

然后线程属性配置都像下面这样，即每秒4个请求，都是超过了goods资源的阈值了的，同时进行访问测试，观察看看是否只会对入口为查询订单的请求做限流。

![image-20211104181111872](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104194316.png) 

测试结果如下，对于创建订单的请求，没有做任何限流操作，都是成功的。

![image-20211104182923477](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104194320.png) 

但是对于查询订单的请求，由于goods资源的阈值配置的是2，而查询订单请求的QPS为4，所以总是有一半的请求被限流。

![image-20211104182814367](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104194326.png) 

##### 8.4.3 流控效果

**流控效果**是指请求达到流控阈值时应该采取的措施，包括以下三种：

- `快速失败`：达到阈值后，新的请求会被立即拒绝并抛出FlowException异常，这个是默认的处理方式；
- `Warm Up`：预热模式，对超出阈值的请求同样是拒绝并抛出异常。但这种模式阈值会动态变化，从一个较小值逐渐增加到最大阈值；
- `排队等待`：让所有的请求按照先后次序排队执行，而且两个请求的间隔不能小于指定时长。

![image-20211104195251671](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104195303.png) 

###### 流控效果-快速失败

这是一种默认的流控效果，前面在进行限流规则配置时，流控效果都是使用的这种效果，所以这里就不再赘述了，下面主要演示`Warm Up`和`排队等待`这两种流控效果。

###### 流控效果-Warm Up

Warm Up也叫预热模式，是应对服务冷启动的一种方案。该效果请求阈值的初始值是==threshold / coldFactor==，持续到指定时长后，逐渐提高到threshold值，而coldFactor的默认值是3。

比如，我们设置QPS的threshold值为10，预热时间为5秒，那么初始阈值就是10/3，然后在5秒后逐渐增长到10。

![image-20211104200535498](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104203927.png) 

下面就给`/order/{orderId}`这个资源设置限流，阈值设置为10，然后利用warm up效果，预热时长为5秒，配置如下：

![image-20211104202443277](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104203935.png) 

![image-20211104202517902](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104203945.png) 

然后在jmeter中访问`http://localhost:8080/order/101`这个地址，线程属性配置如下：

![image-20211104202710860](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104203951.png) 

测试结果是，前五秒差不多只有三个是成功后，但是五秒后就都是成功的啦，说明达到预热效果了。

![image-20211104203114142](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104203957.png) 

![image-20211104203210151](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104204003.png) 

我们也可以在sentinel的控制台看到明显的效果，前几秒通过的QPS是在逐步增加的，而拒绝的QPS是逐步减少的，到最后，就全是通过的QPS了。

![image-20211104203638346](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104204009.png) 

###### 流控效果-排队等待

当请求超过设置的阈值时，`快速失败`和`Warm Up`都会拒绝新的请求并抛出异常。而`排队等待`则是让所有请求进入一个队列中，然后按照阈值允许的时间间隔依次执行。后来的请求必须等待前面执行完成，如果请求预期的等待时间超出最大时长，则会被拒绝。

比如QPS为5，即每秒最多处理5个请求，如果使用排队等待这种效果的话，也就是每200ms处理一个队列中的请求。假设我们将超时时间设置为2秒，意味着预期等待超过2000ms的请求会被拒绝并抛出异常。其实这种方式，也可以达到削峰的效果，对于突然到达的大量请求，并不是将阈值外的请求全部抛弃，而是放在队列中依次执行，如下图：

![image-20211104205316788](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104211550.png) 

下面就给`/order/{orderId}`这个资源设置限流，阈值设置为10，然后使用排队等待的效果，超时时间设置为5秒，配置如下所示：

![image-20211104210349686](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104211557.png) 

![image-20211104210440451](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104211605.png) 

然后在jmeter中访问`http://localhost:8080/order/101`这个地址，线程属性配置如下：

![image-20211104210530279](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104211613.png) 

启动测试后，虽然请求的QPS是15，已经超过了阈值，但是并没有出现报错的请求，因为这时候多出来的请求都被放到了队列中在等待执行。慢慢地，队列中有一些请求的等待时间超过了超时时间，所以就像下面这样，开始出现被拒绝的QPS。多出来的请求被限流后，请求慢慢趋于稳定，所以请求又能在队列中依次执行了，所以最后慢慢又没有被拒绝的QPS了。

![image-20211104210806626](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211104211617.png) 

##### 8.4.4 热点参数限流

之前的限流是统计访问某个资源的所有请求，判断是否超过设定的QPS阈值。而热点参数限流则是对相同请求的不同参数值进行统计，判断是否超过设定的QPS阈值。如下图：

![image-20211105152010874](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211105161700.png)

下面进行一下配置演示。由于**热点参数限流对默认的SpringMVC资源无效**，而我们的controller层又都是SpringMVC资源，所以这里需要借助`@SentinelResource`注解。我们在order-service工程的OrderController类中对我们将要访问的资源增加@SentinelResource注解，注解中的值随便填写一个不重复的即可，如下所示：

```java
@SentinelResource("hot")
@GetMapping("/{orderId}")
public Order queryOrderByUserId(@PathVariable Long orderId) {
    log.info(">>>根据id查询商品信息，id：{}",orderId);
    return orderService.queryOrderById(orderId);
}
```

重启工程后访问一次` /order/{orderId}`地址，触发sentinel监控，然后在sentinel控制台配置热点规则，如下所示：

![image-20211105154854165](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211105161705.png) 

然后在jmeter中进行压力测试，首先配置好要访问的三个地址：

```http
http://localhost:8080/order/101
http://localhost:8080/order/102
http://localhost:8080/order/103
```

然后线程属性配置都是下面这样，即每秒5个请求：

![image-20211105155219617](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211105161719.png) 
启动测试后，观察测试结果，可以发现，对于`http://localhost:8080/order/101`地址的访问情况是这样的：

![image-20211105160806227](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211105161723.png) 

> 即每秒的五个请求中，总是只有两个是成功的，三个是失败的。

而对于`http://localhost:8080/order/102`地址的访问情况是这样的：

![image-20211105161030052](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211105161731.png) 

> 即每秒的五个请求中，总是有四个是成功的，一个是失败的。前面对参数值为102的请求进行过热点参数限流，设置其阈值为4，而不是2，通过测试结果可知，设置是生效的。

而对于`http://localhost:8080/order/103`地址的访问情况是这样的：

![image-20211105161432453](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20211105161737.png) 

> 即每个请求都是成功的。由于之前对于参数值为103的请求设置的阈值为10，而压测的QPS为5，所以总是达不到阈值的，同时说明对于参数值为103的热点参数限流也是生效的。

#### 8.5 隔离和降级



















  









