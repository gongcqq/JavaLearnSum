### 1.Kubernetes简介

#### 1.1 Kubernetes是什么

1. Kubernetes，简称k8s，是用8代替八个字符"ubernete"而成的缩写；
2. Kubernetes是一个开源的容器编排引擎，用来对容器化应用进行自动化部署、扩缩和管理，Kubernetes的目标是让部署容器化的应用简单并且高效；
3. Kubernetes是一个可移植、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。它还拥有一个庞大且快速增长的生态系统，Kubernetes的服务、支持和工具广泛可用。

#### 1.2 应用部署方式的演进

![container](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210301114626.jpg) 

##### 传统部署时代：

早期，各个组织机构在物理服务器上运行应用程序。无法为物理服务器中的应用程序定义资源边界，这会导致资源分配的问题。例如，如果多个应用程序在一个物理服务器上运行，那么在某些实例中，一个应用程序可能会占用大部分资源，从而导致其他应用程序的性能不佳。

解决这个问题的方案是在不同的物理服务器上运行每个应用程序。但是由于资源利用不足而无法扩展， 并且维护许多物理服务器的成本很高。

##### 虚拟化部署时代：

为了解决传统部署方式带来的问题，引入了虚拟化。虚拟化技术允许你在单个物理服务器的CPU上运行多个虚拟机（VM）。 虚拟化可以实现应用程序在虚拟机之间的隔离，并在一个应用程序的信息不能被另一个应用程序自由访问的情况下提供一定的安全级别。

虚拟化技术能够更好地利用物理服务器上的资源，并具有更好的可伸缩性，因为应用程序可以很容易地添加或更新，降低硬件成本等等，通过虚拟化，你可以将一组物理资源表示为一次性虚拟机集群。

每个VM都是一台完整的机器，在虚拟硬件之上运行所有组件，包括它自己的操作系统。

##### 容器部署时代：

容器类似于VM，但是它们放宽了隔离属性，以便在应用程序之间共享操作系统（OS）。因此，容器被认为是轻量级的。与VM类似，容器有自己的文件系统、CPU、内存、进程空间等等。由于它们与底层基础架构分离，因此可以跨云和OS发行版本进行移植。

容器之所以变得流行，是因为它具有很多优秀，比如：

- 应用程序创建和部署的敏捷性：与使用VM镜像相比，容器镜像的创建更加简单和高效；
- 持续开发、集成和部署：通过快速有效的回滚(由于镜像的不可变性)，提供可靠和频繁的容器镜像构建和部署；
- 关注开发与运维的分离：在构建/发布时(而不是在部署时)创建应用程序容器镜像，从而将应用程序与基础架构分离；
- 可观察性：不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号；
- 跨开发、测试和生产的环境一致性：在便携式计算机上运行与在云中运行相同；
- 跨云和操作系统发行版本的可移植性：可在Ubuntu、RHEL、CoreOS、本地、Google Kubernetes Engine和其他任何地方运行；
- 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行OS到使用逻辑资源在OS上运行应用程序；
- 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分，并且可以动态部署和管理，而不是在一台大型单机上整体运行；
- 资源隔离：可预测的应用程序性能；
- 资源利用：高效率和高密度。

#### 1.3 Kubernetes的作用

容器是打包和运行应用程序的良好方式。在生产环境中，我们需要管理运行应用程序的容器，并确保不会出现停机。假设一个容器发生了故障，则需要启动另一个容器，如果能够通过系统自动完成该操作，就会变得更容易。而Kubernetes就可以解决这一问题。

Kubernetes为我们提供了一个灵活运行分布式系统的框架。它负责应用程序的扩展和故障转移，提供部署模式等。例如，Kubernetes可以轻松地管理系统的Canary部署。

Kubernetes可以为我们提供以下功能：

- **批处理**

  Kubernetes可以提供一次性任务、定时任务，满足批量数据处理和分析的场景。

- **服务发现和负载均衡**

  Kubernetes可以使用DNS名称或自己的IP地址公开容器。如果进入到容器的流量很高，Kubernetes能够负载平衡并分发网络流量，从而使部署稳定。

- **存储编排**

  Kubernetes允许我们自动挂载我们选择的存储系统，例如本地存储、公共云提供商等等。

- **自动部署和回滚**

  我们可以使用Kubernetes为部署的容器描述所需的状态，它可以以可控的速度将实际状态更改为所需状态。例如，我们可以自动化Kubernetes为我们的部署创建新的容器，删除现有的容器并将其所有资源用到新容器中。

- **自动装箱**

  假设为Kubernetes提供了一个节点集群，Kubernetes可以使用这些节点来运行容器化的任务。告诉Kubernetes每个容器需要多少CPU和内存(RAM)。Kubernetes可以将容器放到我们的节点上，并充分利用我们的资源。

- **自我修复**

  Kubernetes会重启失败的容器、替换容器、杀死不响应用户定义的健康检查的容器，并且在它们准备好提供服务之前不会将它们通告给客户端。

- **密钥与配置管理**

  Kubernetes允许我们存储和管理敏感信息，例如密码、OAuth令牌和SSH密钥。我们可以在不重建容器镜像的情况下部署和更新密钥以及应用程序配置，也无需在堆栈配置中暴露密钥。

#### 1.4 Kubernetes的架构

当我们部署完Kubernetes, 即拥有了一个完整的集群。一个Kubernetes集群由一组被称作节点的机器组成。这些节点上运行着Kubernetes所管理的容器化应用。集群应至少拥有一个工作节点。

##### 1.4.1 Kubernetes的集群节点

- **Master Node**

  该节点是Kubernetes的集群控制节点，对集群进行调度管理，接受集群外用户到集群的操作请求。Master Node由==API Server==、==Scheduler==、==etcd==、==Controller-Manager==组成。

- **Worker Node**

  该节点是集群的工作节点，节点上运行着用户的业务应用容器。Worker Node包括==kubelet==、==kube-proxy==以及容器运行环境(==Container Runtime==)，比如docker就是一种容器运行环境。

##### 1.4.2 Kubernetes架构图

以下是Kubernetes的架构图：

![k8s架构图](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210302094338.jpg) 

上图中涉及到的各个组件的含义，下文会有详细讲解。

### 2.Kubernetes集群的搭建

#### 2.1 前置知识点

目前生产部署Kubernetes集群主要有两种方式：

- **kubeadm**

  Kubeadm是一个Kubernetes部署工具，提供`kubeadm init`和`kubeadm join`来快速部署Kubernetes集群。具体详情也可以访问[官方地址](https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/)。

- **二进制包**

  我们可以从github下载发行版的二进制包，手动部署每个组件，组成Kubernetes集群。

> Kubeadm方式降低了部署的门槛，但屏蔽了很多细节，遇到问题很难排查。如果想要更加可控，还是推荐使用二进制包方式部署Kubernetes集群，虽然手动部署麻烦点，但是期间可以学习到很多的工作原理，也利于后期维护。

#### 2.2 kubeadm方式搭建k8s集群(单master)

kubeadm是官方社区推出的一个用于快速部署k8s集群的工具，这个工具能通过两条指令完成一个k8s集群的部署：

1. 使用`kubeadm init`创建一个Master节点；
2. 使用`kubeadm join <Master节点的IP和端口>`将Node节点加入到当前集群中。

##### 2.2.1 安装要求

- 一台或多台机器，操作系统为CentOS7；

- 硬件配置：2GB或更多RAM，2个CPU或更多CPU；

- 集群中所有机器之间网络互通；

- 可以访问外网，因为需要联网拉取镜像；

- 需要禁止swap分区。

##### 2.2.2 最终目标

1. 在所有节点上安装Docker和kubeadm；
2. 部署Kubernetes Master；
3. 部署容器网络插件；
4. 部署Kubernetes Node，将节点加入到Kubernetes集群中；
5. 部署Dashboard Web页面，可视化查看Kubernetes资源。

##### 2.2.3 主机规划

| 角色   | IP            |
| ------ | ------------- |
| master | 192.168.68.11 |
| node1  | 192.168.68.12 |
| node2  | 192.168.68.13 |

##### 2.2.4 系统初始化

###### 2.2.4.1 关闭防火墙

```shell
#临时关闭
systemctl stop firewalld

#永久关闭
systemctl disable firewalld
```

###### 2.2.4.2 关闭 selinux

```shell
#临时关闭
setenforce 0

#永久关闭
sed -i 's/enforcing/disabled/' /etc/selinux/config
```

###### 2.2.4.3 关闭swap

```shell
#临时关闭
swapoff -a

#永久关闭
sed -ri 's/.*swap.*/#&/' /etc/fstab 
```

###### 2.2.4.4 设置主机名

```shell
#在192.168.68.11主机上执行以下命令
hostnamectl set-hostname master

#在192.168.68.12主机上执行以下命令
hostnamectl set-hostname node1

#在192.168.68.13主机上执行以下命令
hostnamectl set-hostname node2
```

###### 2.2.4.5 在master中添加hosts

```shell
cat >> /etc/hosts << EOF
192.168.68.11 master
192.168.68.12 node1
192.168.68.13 node2
EOF
```

###### 2.2.4.6 将桥接的IPv4流量传递到iptables的链

```shell
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

#生效命令
sysctl --system
```

###### 2.2.4.7 时间同步

```shell
yum install -y ntpdate
ntpdate -u time.windows.com
```

> **注意**：以上系统初始化涉及到的命令，如无特殊说明，均需在所有主机上执行。

##### 2.2.5 安装Docker、kubeadm、kubelet

Kubernetes默认的容器运行环境为docker，因此要先安装Docker。我这边所有的主机上之前都安装过docker了，所以docker的安装这里就不再进行介绍了。

###### 2.2.5.1 添加阿里云YUM软件源

```shell
#对所有主机执行以下操作命令
cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

###### 2.2.5.2 安装kubeadm，kubelet和kubectl

由于版本更新频繁，所以这里指定版本号进行安装部署：

```shell
#对所有主机执行安装命令
yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0

#对所有主机执行以下命令，让kubelet开机启动
systemctl enable kubelet
```

##### 2.2.6 部署Kubernetes Master

```bash
#在master主机上执行以下命令
kubeadm init --apiserver-advertise-address=192.168.68.11 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.18.0 --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16
```

**命令中的参数含义如下：**

![kubeadm-init](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210304172944.png)    

命令的执行需要一段时间(期间可以重新开个窗口，使用`docker images`查看镜像下载的情况)，当出现如下内容时，说明已经执行成功了。

![image-20210302154448491](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210302154452.png)  

这个图中也有接下来需要我们执行的步骤，这些步骤下面也会说到。

**注意**：执行上面的`kubeadm init`那一长串命令的时候，也有可能会报下面的错：

![image-20210305220911407](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210305221028.png) 

如果真的报上图的错的话，依次执行以下命令后重启主机，然后再次执行`kubeadm init`那一长串命令即可。

```shell
#先使用cat命令查看ip_forward的值是否为0，如果为0就继续执行下面的命令
cat /proc/sys/net/ipv4/ip_forward

#上面的值如果为0的话，这里给改成1即可
echo "1" > /proc/sys/net/ipv4/ip_forward

#重启网络服务
service network restart

#最后为了保险起见，可以使用reboot命令重启主机
reboot
```


> 由于默认拉取镜像的地址(k8s.gcr.io)国内无法访问，所以`kubeadm init`命令后面的image-repository参数的值使用的是阿里云镜像仓库地址。

**使用kubectl工具：**

```shell
#在master节点依次执行以下命令
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

上面的命令执行完成之后，我们也可以通过`kubectl get nodes`命令进行查看，会显示如下内容：

```shell
AME     STATUS     ROLES    AGE   VERSION
master   NotReady   master   11m   v1.18.0
```

##### 2.2.7 加入Kubernetes Node

```shell
#在每个worker node主机上执行以下命令，每个人的这个命令都是不一样的，我们在执行完kubeadm init命令后，会输出下面这个命令，直接复制即可
kubeadm join 192.168.68.11:6443 --token ba295y.k5pqeuidfvqrrx2x \
    --discovery-token-ca-cert-hash sha256:0b7edbb6ebe7adf0a67f6a5ec7b8d8493263440f406628327800d9bafbe6c4f9
```

==注意==：在执行`kubeadm join`命令的时候，如果也出现执行`kubeadm init`命令时候的错误，那就还是按照上面提到的解决方案进行解决即可。

以上命令的token默认有效期为24小时，当过期之后，该token就不可用了。如果需要重新创建token，操作如下：

```shell
#在master节点执行以下命令
kubeadm token create --print-join-command

#我们也可以在master节点执行以下命令来查看token信息
kubeadm token list
```

当我们使用`kubeadm join`命令向集群中加入新节点后，在master主机上再次使用`kubectl get nodes`命令，可以发现节点已经添加成功了，如下所示：

```shell
NAME     STATUS     ROLES    AGE   VERSION
master   NotReady   master   44m   v1.18.0
node1    NotReady   <none>   15m   v1.18.0
node2    NotReady   <none>   14m   v1.18.0
```

##### 2.2.8 部署CNI网络插件

```shell
#我们可以使用如下命令来部署CNI网络插件(以下命令要在master主机执行)
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

如果出现"Unable to connect to the server"错误，那说明该地址已经被墙了，如果不知道怎么科学上网的话，可以使用如下命令获取并执行我翻墙下载的这个配置文件：

```shell
#在master主机上下载压缩包
wget https://files.cnblogs.com/files/gongcqq/kube-flannel.tar.gz

#解压压缩包，以便获取kube-flannel.yml文件
tar -zxvf kube-flannel.tar.gz

#执行kube-flannel.yml文件。如果网络环境不好，可以按照下面"注意事项"中把相关镜像先下载下来，再执行该文件
kubectl apply -f kube-flannel.yml
```

以上命令执行完成之后，再在master主机使用`kubectl get nodes`命令进行查看，发现节点状态都从之前的==NotReady==变成了==Ready==，如下所示：

```shell
NAME     STATUS   ROLES    AGE    VERSION
master   Ready    master   139m   v1.18.0
node1    Ready    <none>   110m   v1.18.0
node2    Ready    <none>   110m   v1.18.0
```

我们也可以在master主机上使用`kubectl get pods -n kube-system`命令查看pod的运行情况，查询结果如下：

![image-20210302174746085](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210302174748.png) 

**注意事项**：上面在使用`kubectl apply -f kube-flannel.yml`命令的时候会拉取一个flannel镜像，如果网络环境不好，可能会导致拉取失败，从而导致集群部署失败。我这边已经将该镜像打成了名为[flannel-v0.13.1-rc2.tar](https://gongsl.lanzous.com/iYJpGmkeiaj)的tar包，我们可以下载下来通过ftp的方式传到集群的各个主机上。假设都传到了主机的"/root"目录下，那么直接使用如下命令还原镜像即可：

```shell
docker load -i /root/flannel-v0.13.1-rc2.tar
```

##### 2.2.9 测试kubernetes集群

在Kubernetes集群中创建一个pod，验证是否正常运行：

```shell
#在master主机上创建并运行一个pod
kubectl create deployment nginx --image=nginx

#暴露端口
kubectl expose deployment nginx --port=80 --type=NodePort
```

以上命令执行完成之后，我们可以通过`kubectl get pod,svc`命令查看pod的运行情况：

![image-20210302181219698](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210302181739.png) 

由上图可知，刚才创建的pod正在运行中，且nginx暴露给外部访问的端口是**30412**，如果在浏览器中使用集群中任何一个主机加这个端口都能访问到nginx的话，就说明集群搭建成功了。我使用集群中的三个主机加上端口分别进行了访问，都是可以访问到nginx的，说明kubernetes集群搭建成功了。

```bash
http://192.168.68.11:30412
http://192.168.68.12:30412
http://192.168.68.13:30412
```

![image-20210302182217942](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210302182224.png) 

##### 2.2.10 部署Dashboard Web页面

**注意**：部署Dashboard Web页面章节涉及的所有命令均是在master节点上执行的。

1. 获取recommended.yaml文件

```shell
#可以使用如下命令进行下载
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.3/aio/deploy/recommended.yaml
```

以上地址如果访问不了的话，可以使用如下方式下载：

```shell
#下载recommended.yaml的压缩包
wget https://files.cnblogs.com/files/gongcqq/recommended.tar.gz

#解压压缩包以获取recommended.yaml文件
tar -zxvf recommended.tar.gz
```

默认Dashboard只能集群内部访问，修改Service为NodePort类型，暴露到外部：

![image-20210306004457104](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210306004502.png) 

> **注意**：如果是通过上面第二个地址中的recommended.tar.gz获取到的recommended.yaml文件的话，就不用再增加NodePort类型了，因为压缩包中的recommended.yaml文件是已经增加过的。

2. 执行recommended.yaml文件

```shell
kubectl apply -f recommended.yaml
```

执行完成后，可以通过`kubectl get pods -n kubernetes-dashboard`命令进行状态查看，下图就是正常状态：

![image-20210306004943314](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210306004947.png) 

3. 创建用户并生成token

```shell
#创建用户
kubectl create serviceaccount dashboard-admin -n kube-system

#用户授权
kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin

#生成用户token
kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk '/dashboard-admin/{print $1}')
```

下图就是用户的token信息：

![image-20210306005317155](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210306005320.png) 

4. 登录Dashboard页面

浏览器输入集群中任一主机ip加30001端口都可访问到Dashboard页面，这里以master主机为例，如下所示：

```http
https://192.168.68.11:30001/
```

进入到登录页面后，需要填写用户token，填写完成后，直接登录即可：

![image-20210306010103826](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210306010108.png) 

下面就是登录成功后的主界面：

![image-20210306010249999](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210306010255.png) 

### 3.k8s的命令行工具kubectl

kubectl是Kubernetes集群的命令行工具，通过kubectl能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。

#### 3.1 基本语法

```shell
kubectl [command] [TYPE] [NAME] [flags]
```

- `command`：指定要对一个或多个资源执行的操作，例如 ==create==、==get==、==describe==、==delete==。

- `TYPE`：指定资源类型，资源类型不区分大小写，可以指定单数、复数或缩写形式。例如，以下命令输出的结果相同:

```shell
kubectl get pod pod1
kubectl get pods pod1
kubectl get po pod1
```

- `NAME`：指定资源的名称，名称区分大小写。如果省略名称，可以使用`kubectl get pods`显示所有资源的详细信息。
- `flags`：指定可选的参数。例如，可以使用`-s`或`-server`参数指定Kubernetes API服务器的地址和端口。

#### 3.2 kubectl子命令使用分类

##### 基础命令：

![image-20210314173053517](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210314173506.png) 

##### 部署和集群管理命令：

![image-20210314173233468](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210314173515.png) 

##### 故障诊断和调试命令：

![image-20210314173354442](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210314173524.png) 

##### 其他命令：

![image-20210314173434052](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210314173542.png) 

### 4.资源清单详解

k8s集群中对资源管理和资源对象编排部署都可以通过声明样式(YAML)文件来解决，也就是可以把需要对资源对象的操作编辑到YAML格式文件中，我们把这种文件叫做资源清单文件，通过kubectl命令直接操作资源清单文件就可以实现对大量的资源对象进行编排部署。

在k8s中，一般使用YAML格式的文件来创建符合我们预期期望的pod,这样的YAML文件称为资源清单。

#### 4.1 资源清单中的常用字段

##### 4.1.1 必须存在的属性

| 参数名                  | 字段类型 | 说明                                                         |
| ----------------------- | -------- | ------------------------------------------------------------ |
| version                 | String   | K8S API的版本，目前基本是v1，可以使用`kubectl api-versions`命令进行查询 |
| kind                    | String   | 这里指的是yaml文件定义的资源类型和角色，比如：Deployment     |
| metadata                | Object   | 元数据对象，固定值，不可更改                                 |
| metadata.name           | String   | 元数据对象的名称，由我们自定义                               |
| metadata.namespace      | String   | 元数据对象的命名空间，由我们自定义，默认值是default          |
| spec                    | Object   | 详细定义对象，固定值，不可更改                               |
| spec.containers[]       | list     | 这里是spec对象的容器列表定义，是个列表                       |
| spec.containers[].name  | String   | 这里定义容器的名称                                           |
| spec.containers[].image | String   | 这里定义要用到的镜像的名称                                   |

##### 4.1.2 spec的主要属性

| 参数名                                      | 字段类型 | 说明                                                         |
| :------------------------------------------ | :------- | :----------------------------------------------------------- |
| spec.containers[].name                      | String   | 用于定义容器的名称                                           |
| spec.containers[].image                     | String   | 用于定义要用到的镜像的名称                                   |
| spec.containers[].imagePullPolicy           | String   | 定义镜像的拉取策略，有`Always`，`Never`，`IfNotPresent`三种策略可选。Always意思是每次都尝试重新拉取镜像；Never指的是仅使用本地镜像；IfNotPresent指的是如果本地有镜像就使用本地镜像，没有就拉取在线镜像。如果没有设置策略的话，默认使用的是Always。 |
| spec.containers[].command[]                 | List     | 指定容器的启动命令，可以指定多个。不指定则使用镜像打包时使用的启动命令。 |
| spec.containers[].args[]                    | List     | 指定容器的启动命令参数，可以指定多个                         |
| spec.containers[].workingDir                | String   | 指定容器的工作目录                                           |
| spec.containers[].volumeMounts[]            | List     | 指定容器内部的存储卷配置                                     |
| spec.containers[].volumeMounts[].name       | String   | 指定可以被容器挂载的存储卷的名称                             |
| spec.containers[].volumeMounts[].mountPath  | String   | 指定可以被容器挂载的存储卷的路径                             |
| spec.containers[].volumeMounts[].readOnly   | String   | 设置存储卷路径读写的模式，true或者false，默认为读写模式。    |
| spec.containers[].ports[]                   | List     | 指定容器需要用到的端口列表                                   |
| spec.containers[].ports[].name              | String   | 指定端口名称                                                 |
| spec.containers[].ports[].containerPort     | String   | 指定容器需要监听的端口号                                     |
| spec.containers[].ports[].hostPost          | String   | 指定容器所在主机需要监听的端口号，默认跟上面containerPort相同。需要注意，设置了hostPost同一台主机无法启动该容器的相同副本(因为主机的端口号不能相同，这样会冲突)。 |
| spec.containers[].ports[].protocol          | String   | 指定端口协议，支持TCP和UDP，默认值为TCP                      |
| spec.containers[].env[]                     | List     | 指定容器运行时需要设置的环境变量列表                         |
| spec.containers[].env[].name                | String   | 指定环境变量名称                                             |
| spec.containers[].env[].value               | String   | 指定环境变量值                                               |
| spec.containers[].resources                 | Object   | 指定资源限制和资源请求的值                                   |
| spec.containers[].resources.limits          | Object   | 指定设置容器运行时资源的运行上限                             |
| spec.containers[].resources.limits.cpu      | String   | 指定CPU的限制                                                |
| spec.containers[].resources.limits.memory   | String   | 指定MEM内存的限制                                            |
| spec.containers[].resources.requests        | Object   | 指定容器启动和调度室的限制设置                               |
| spec.containers[].resources.requests.cpu    | String   | CPU请求，容器启动时初始化的可用数量                          |
| spec.containers[].resources.requests.memory | String   | 内存请求，容器启动时初始化的可用数量                         |
| spec.restartPolicy                          | String   | 定义Pod重启策略，可以选择值为`Always`、`OnFailure`、`Never`，默认值为Always。使用Always表示Pod一旦终止运行，无论容器是如何终止的，kubelet服务都将重启它；OnFailure表示只有Pod以非零退出码终止时，kubelet才会重启该容器，如果容器是正常结束(退出码为0)，则kubelet将不会重启它；Never表示Pod终止后，kubelet会将退出码报告给Master，但不会重启该Pod。 |
| spec.nodeSelector                           | Object   | 定义Node的Label过滤标签，以key:value格式指定。               |
| spec.imagePullSecrets                       | Object   | 定义pull镜像时使用secret名称，以name:secretkey格式指定。     |

##### 4.1.3 kubectl explain命令

上面只是列举了资源清单中的部分属性，如果我们想要获取更全的属性的话，可以使用`kubectl explain`命令。

以pod为例，我们想要知道pod对应的yaml文件中包含哪些属性的话，可以使用`kubectl explain pod`命令，如下所示：

![image-20210507195359748](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210507195415.png) 

上图展示的属性都是第一级别的根属性，比如我们想要知道上面spec属性下的containers属性下的volumeMounts属性下包含哪些属性，使用`kubectl explain pod.spec.containers.volumeMounts`命令即可，如下所示：

![image-20210507200700400](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210507200705.png) 

如果我们还想查看其他属性，也都可以使用类似方式。如果不想查看pod下的属性，比如想查看deployment下有哪些属性，那就使用`kubectl explain deployment`命令即可。

#### 4.2 举例说明

##### 4.2.1 查看已有资源的资源详情

以Deployment的资源清单为例，命令如下：

```shell
#查看有哪些Deployment资源
kubectl get deploy -o wide

#根据名称获取资源清单详情，下面的"harbor-test"就是Deployment的名称
kubectl get deploy harbor-test -o yaml
```

获取到的资源清单详情如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2020-09-29T08:57:26Z"
  generation: 1
  name: harbor-test
  namespace: default
  resourceVersion: "56211880"
  selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/harbor-test
  uid: 38711c5c-b616-4cb8-906f-24b8cb7a5729
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: harbor
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: harbor
      name: harbor-test
    spec:
      containers:
      - image: ai/pushgateway:v0.8.0
        imagePullPolicy: IfNotPresent
        name: harbor-test
        ports:
        - containerPort: 9090
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2020-09-29T08:57:26Z"
    lastUpdateTime: "2020-09-29T08:57:31Z"
    message: ReplicaSet "harbor-test-7864497b8f" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  - lastTransitionTime: "2021-04-08T02:00:10Z"
    lastUpdateTime: "2021-04-08T02:00:10Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
```

##### 4.2.2 查看新建资源的资源详情

如果之前并没有已经存在的资源，我们也可以新建一个资源，并查看其资源清单详情。

以Deployment的资源清单为例，命令如下：

```shell
#该命令意思就是新建一个名为"gongsl"的Deployment资源
kubectl create deploy gongsl --image=nginx:1.17 -o yaml --dry-run=client
```

获取到的资源清单详情如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: gongsl
  name: gongsl
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gongsl
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: gongsl
    spec:
      containers:
      - image: nginx:1.17
        name: nginx
        resources: {}
status: {}
```

**注意事项：**

1. 上面新建资源使用的命令后面加上的`--dry-run=client`表示尝试运行，也就是说，加上这个后，并不会真的新建一个Deployment资源，只是模拟一下这个效果，如果想要真的新建一个Deployment资源的话，去掉它即可；
2. 低版本的k8s中，想要尝试运行的话，命令后面加的并不是`--dry-run=client`，而是`--dry-run`。

### 5.k8s核心技术-Pod

#### 5.1 Pod的基本概念

1. Pod是k8s系统中可以创建和管理的最小单元；
2. 每个Pod都是应用的一个实例，有独立的IP地址；
3. k8s直接处理或调度的是Pod而不是容器，Pod是由一个或多个容器组成；
4. 一个Pod中的多个容器彼此间共享网络和存储资源；
5. 每个Pod中都有一个被称为"根容器"的Pause容器，Pause容器对应的镜像属于k8s平台的一部分。

#### 5.2 Pod存在的意义

1. 我们可以使用docker创建容器，但是docker是单进程设计，而pod是多进程设计，可以运行多个应用程序；
2. 彼此间频繁调用的多个应用，可以放在同一个pod中，这样可以通过localhost或者socket方式调用，更加方便。

#### 5.3 Pod的生命周期

![Pod生命周期](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210508130811.jpg)  

**生命周期图解：**

1. Pod一旦被创建，首先会创建`Pause`根容器，根容器一旦被创建，即使Pod中的应用容器创建失败，根容器也会继续运行，只有Pod被删除后，根容器才会随之消失；
2. 根容器创建完成后，如果存在Init容器，这时就会启动Init容器，上图中的`Init C`就是指Init容器；
3. `Init C`可以有一个，可以有多个，也可以没有。`Init C`在初始化完成之后就会死亡，如果有多个`Init C`的话，必须等前一个初始化完成后才能执行下一个。只有当所有的`Init C`全部执行完成后，才会去运行主容器；
4. 初始化操作完成之后就会进行主容器的操作，上面的`container`就是指主容器，主容器也可以有多个；
5. 在主容器的运行过程中，在开始和结束的时候可以分别进行一个`START`和`STOP`的操作。也就是说，我们可以在主容器运行前通过`START`操作来执行一些指令，也可以在主容器退出的时候通过`STOP`操作来执行一些指令；
6. 上面的`readiness`指的是就绪检测，`Liveness`指的是生存检测；
7. 我们并不需要在容器刚运行的时候就进行就绪检测，我们可以根据自身需要决定什么时候进行就绪检测，比如我们可以在容器运行五秒后再进行就绪检测等。同理，生存检测也是一样的，我们可以自定义什么时候进行生存检测；
8. 在Pod被拉起后，Pod中的容器里面的服务有可能还没有加载好，还不能被外网访问，这时候我们可以通过命令、TCP连接、HTTP协议获取状态等就绪检测方式进行检测，检测到服务已经加载好后再把Pod的状态改为运行状态，这样就可以保证外网的正常访问；
9. 如果存在就绪检测，那么在就绪检测没有执行成功前，Pod的状态是不会变成运行状态的；
10. 在Pod的运行过程中，有可能会存在进程还在运行，但是已经无法正常对外提供服务的情况。由于进程还在，所以主容器就会继续运行，主容器运行，Pod就还是运行状态，这种情况肯定是有问题的。这时候就可以通过生存检测的方式对Pod中的容器进行检测，当检测到容器内部已经不能正常对外提供访问的时候，我们可以通过重启或者重建Pod等操作使其恢复正常。

#### 5.4 关于Init容器

##### 5.4.1 Init容器的特点

1. Init容器总是运行到完成，在Pod的生命周期中，Init容器不会一直处于运行状态；
2. 如果有多个Init容器，这些Init容器会按照顺序逐个运行，前一个Init容器必须运行成功，下一个才能运行；
3. 只有Init容器都运行完成之后，才会运行我们自己的应用容器；
4. 如果Pod重启，Pod中的所有Init容器都会重新执行；
5. 更改Init容器的`image`字段，等同于重启该Pod；
6. 在Pod中的每个应用容器和Init容器的名称必须唯一，与任何其它容器重名，都会在校验时抛出错误。

##### 5.4.2 Init容器使用案例

下面例子定义的Pod中具有2个Init容器。第一个等待名为`myservice`的Service的启动，第二个则是等待名为`mydb`的Service的启动。只有这两个Init容器都启动完成，Pod才会启动应用容器。

**通过myapp.yaml文件创建Pod：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]
```

**创建Pod的命令：**

```bash
kubectl apply -f myapp.yaml
```

**检查Pod的运行状态：**

```shell
kubectl get -f myapp.yaml -o wide
```

![image-20210508153927669](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210508154015.png) 

由上图状态可知，两个Init容器都没有启动成功。

**查看名为init-myservice的Init容器的日志：**

```shell
kubectl logs myapp-pod -c init-myservice
```

![image-20210508154628311](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210508155201.png) 

由上图可知，首先启动的名为`init-myservice`的Init容器一直在等待名为`myservice`的Service的启动。

**查看名为init-mydb的Init容器的日志：**

```shell
kubectl logs myapp-pod -c init-mydb
```

![image-20210508154920923](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210508154923.png) 

由上图可知，前一个Init容器还没启动成功，所以名为`init-mydb`的Init容器一直处于等待中。

**通过services.yam文件创建Service：**

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
---
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9377
```

**创建Service的命令：**

```shell
kubectl create -f services.yaml
```

**查看Service是否创建成功：**

```shell
kubectl get svc -o wide
```

![image-20210508160211945](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210508160216.png) 

当所需的Service创建完成后，再次查看Pod的状态，这时Pod就已经变成运行状态了，如下所示：

![image-20210508160416890](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210508160445.png) 

#### 5.5 Pod的健康检查-探针

##### 5.5.1 探针的类型

探针是由kubelet对容器进行的定期诊断。要执行诊断，需要kubelet调用由容器实现的Handler(处理程序)。有三种类型的处理程序：

- `ExecAction`：在容器内执行指定命令。如果命令退出时返回码为0则认为诊断成功；
- `TCPSocketAction`：对容器的IP地址上的指定端口执行TCP检查。如果端口打开，则诊断被认为是成功的；
- `HTTPGetAction`：对容器的IP地址上指定端口和路径执行HTTP Get请求。如果响应的状态码大于等于200且小于 400，则诊断被认为是成功的。

每次探测都将获得以下三种结果之一：

- `Success`：容器通过了诊断；
- `Failure`：容器未通过诊断；
- `Unknown`：诊断失败，因此不会采取任何行动。

针对运行中的容器，`kubelet`可以选择是否执行以下三种探针，以及如何针对探测结果作出反应：

- `livenessProbe`：指示容器是否正在运行。如果存活态探测失败，则kubelet会杀死容器，并且容器将根据其重启策略决定下一步操作。如果容器不提供存活探针，则默认状态为`Success`；
- `readinessProbe`：指示容器是否准备好为请求提供服务。如果就绪态探测失败，端点控制器将从与Pod匹配的所有服务的端点列表中删除该Pod的IP地址。初始延迟之前的就绪态的状态值默认为`Failure`。如果容器不提供就绪态探针，则默认状态为`Success`；
- `startupProbe`: 指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被 禁用，直到此探针成功为止。如果启动探测失败，`kubelet`将杀死容器，并且容器将根据其重启策略决定下一步操作。 如果容器没有提供启动探测，则默认状态为`Success`。

##### 5.5.2 探针的使用场景

###### 5.5.2.1 何时使用livenessProbe(存活探针)：

- 如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃，则不一定需要存活态探针；`kubelet`将根据Pod的重启策略自动执行修复操作；
- 如果你希望容器在探测失败时被杀死并重新启动，那么需要指定一个存活态探针，并指定重启策略`restartPolicy`为`Always`或`OnFailure`。

###### 5.5.2.2 何时使用readinessProbe(就绪探针)：

- 如果要仅在探测成功时才开始向Pod发送请求流量，请指定就绪态探针。在这种情况下，就绪态探针可能与存活态探针相同，但是规约中的就绪态探针的存在意味着Pod将在启动阶段不接收任何数据，并且只有在探针探测成功后才开始接收数据；
- 如果我们的容器需要加载大规模的数据、配置文件或者在启动期间执行迁移操作，可以添加一个就绪态探针；
- 如果我们希望容器能够自行进入维护状态，也可以指定一个就绪态探针，它检查与存活探测不同的特定于就绪的端点。

###### 5.5.2.3 何时使用startupProbe(启动探针)：

- 对于所包含的容器需要较长时间才能启动就绪的Pod而言，启动探针是有用的。你不再需要配置一个较长的存活态探测时间间隔，只需要设置另一个单独的配置，对启动期间的容器执行探测，从而允许使用远远超出存活态时间间隔所允许的时长；
- 如果我们的容器启动时间超出 `initialDelaySeconds + failureThreshold × periodSeconds` 总值，我们应该设置一个启动探测，对存活态探针所使用的同一端点执行检查。 `periodSeconds` 的默认值是 10 秒。你应该将其 `failureThreshold` 设置得足够高，以便容器有充足的时间完成启动，并且避免更改存活态探针所使用的默认值。这一设置有助于减少死锁状况的发生。

##### 5.5.3 就绪探针的使用

上面提到可以通过==exec==、==httpGet==、==tcpSocket==这三种方式对容器进行诊断，而无论是就绪探针还是存活探针，都是支持这三种方式的，下面就针对这三种方式演示就绪探针的使用。

###### 5.5.3.1 使用exec进行就绪检测

新增一个exec.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    readinessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 1
      periodSeconds: 3
```

通过yaml文件创建Pod:

```shell
kubectl apply -f exec.yaml
```

查看Pod的状态：

```shell
kubectl get pod -o wide
```

![image-20210509175823128](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509175825.png) 

使用`kubectl describe`命令查看Pod的详细信息：

![image-20210509180730845](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509180733.png) 

进入容器中新增就绪检测需要的文件：

```shell
kubectl exec pod-nginx -it -- bash
或
kubectl exec pod-nginx -it -- sh
```

![image-20210509181959874](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509182005.png) 

**使用探针时，有一些常用的属性，下面介绍一下它们各自的含义：**

- `initialDelaySeconds`：指容器启动后要等待多少秒后就绪探针和存活探针才被初始化，默认值和最小值都是0；
- `periodSeconds`：指的是执行探测的时间间隔(单位是秒)，默认是10秒，最小值是1；
- `timeoutSeconds`：指探测超时后等待多少秒，默认值是1秒，最小值是1；
- `successThreshold`：指探测器在失败后，被视为成功的最小连续成功数，默认值是1。存活探测和启动探测的这个值必须是1，最小值是1；
- `failureThreshold`：指当探测失败后，Kubernetes的重试次数。存活探测情况下的放弃意味着重新启动容器，就绪探测情况下的放弃，Pod会被打上未就绪的标签，默认值是3，最小值是1。

###### 5.5.3.2 使用httpGet进行就绪检测

新增一个httpGet.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    readinessProbe:
      httpGet:
        port: 80
        path: /test.html
      initialDelaySeconds: 1
      periodSeconds: 3
```

通过yaml文件创建Pod:

```shell
kubectl apply -f httpGet.yaml
```

查看Pod的状态：

```shell
kubectl get pod -o wide
```

![image-20210509183945663](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509183948.png) 

使用`kubectl logs`命令查看Pod日志：

![image-20210509184450785](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509184554.png) 

进入容器中新增就绪检测需要的文件：

```shell
kubectl exec pod-nginx -it -- bash
```

![image-20210509190005862](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509190014.png) 

###### 5.5.3.3 使用tcpSocket进行就绪检测

新增一个tcpSocket.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    readinessProbe:
      tcpSocket:
        port: 81
      initialDelaySeconds: 1
      periodSeconds: 3
```

通过yaml文件创建Pod:

```shell
kubectl apply -f tcpSocket.yaml
```

查看Pod的状态：

```shell
kubectl get pod -o wide
```

![image-20210509190647617](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509190825.png) 

使用`kubectl describe`命令查看Pod的详细信息：

![image-20210509191049164](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509191058.png) 

默认情况下nginx是使用80端口运行的，nginx容器中并没有使用81端口运行的应用程序，所以就出现了上图中的错误。

使用`kubectl delete pod pod-nginx`命令删除该Pod，然后将yaml文件改成下面的样子后重新创建Pod：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    readinessProbe:
      tcpSocket:
        port: 80  #只是将这里的端口改成了80，其他地方都没有变动
      initialDelaySeconds: 1
      periodSeconds: 3
```

![image-20210509192218822](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509192222.png) 

##### 5.5.4 存活探针的使用

存活探针和就绪探针一样，同样是有==exec==、==httpGet==、==tcpSocket==这三种探测方式，下面就依次介绍一下。

###### 5.5.4.1 使用exec进行存活检测

新增一个exec-liveness.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: exec-liveness
spec:
  containers:
  - name: liveness
    image: busybox
    imagePullPolicy: IfNotPresent
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
```

**注意**：如果通过以上配置文件创建了Pod，那么kubelet会在容器内执行命令`cat /tmp/healthy`来进行探测，如果命令执行成功并且返回值为0，kubelet就会认为这个容器是健康存活的；如果这个命令返回非0值，kubelet会杀死这个容器并重新启动它。

通过yaml文件创建Pod:

```shell
kubectl apply -f exec-liveness.yaml
```

Pod创建后，当容器启动时，会执行如下的命令：

 ```shell
 /bin/sh -c "touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600"
 ```

以上命令表明这个容器生命的前30秒，`/tmp/healthy`文件是存在的。所以在这最开始的30秒内，存活探针在进行检测的时候，执行命令`cat /tmp/healthy`会返回成功代码，30秒之后再执行该命令就会返回失败代码了。

在Pod创建后的30秒内查看Pod的状态以及详细信息：

```shell
kubectl get pod -o wide
kubectl describe pod exec-liveness|tail -n 10
```

![image-20210509215420137](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509215423.png)  

由上图可知，Pod创建的30秒内，Pod的运行是正常的，存活检测也是没有任何问题的。

在Pod创建30秒后查看Pod的状态以及详细信息：

![image-20210509215752404](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509215756.png) 

由上图可知，由于30秒后`/tmp/healthy`文件已经被删除了，所以存活检测失败了。

再过一段时间后我们会发现，容器被杀死并且被重启了，重启完成后，再使用`kubectl get pod -o wide`命令查看Pod的状态时，输出结果中的RESTARTS的值变成了1，表示已经进行了一次重启。

![image-20210509220358522](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509220720.png) 

###### 5.5.4.2 使用httpGet进行存活检测

新增一个httpGet-liveness.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: http-liveness
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    livenessProbe:
      httpGet:
        port: 80
        path: /index.html
      initialDelaySeconds: 1
      periodSeconds: 3
```

默认情况下，nginx容器中是包含上面的index.html的，所以创建容器后，存活检测肯定是成功的。

通过yaml文件创建Pod:

```shell
kubectl apply -f httpGet-liveness.yaml
```

查看Pod的状态、详细信息以及日志信息：

![image-20210509230027051](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509230040.png) 

由上图可知，存活检测是检测成功的，下面通过手动删除容器中index.html文件的方式，看看存活检测是否能够检测出。

![image-20210509232519403](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509232522.png) 

由上图可知，删除index.html文件后，存活检测已经检测出问题了，并报了404错误，过一段时候后，容器就会被重启。

![image-20210509233318300](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509234429.png) 

由于index.html文件是nginx容器自带的，所以当容器重启完成之后，该文件就又会出现，这时候存活检测就又会检测成功了，可以通过`kubectl logs http-liveness`命令查看日志情况。

###### 5.5.4.3 使用tcpSocket进行存活检测

新增一个tcpSocket-liveness.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tcp-liveness
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    livenessProbe:
      tcpSocket:
        port: 80
      initialDelaySeconds: 1
      periodSeconds: 3
```

通过yaml文件创建Pod:

```shell
kubectl apply -f tcpSocket-liveness.yaml
```

查看Pod的状态、详细信息以及日志信息：

![image-20210509234423456](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509234434.png) 

由上图可知，通过tcpSocket方式进行的存活检测是检测成功的。

#### 5.6 Pod的生命周期事件

这里的事件主要是指`postStart`和`preStop`，当一个容器启动后，k8s会发送postStart事件，在容器被终结时，k8s会发送一个preStop事件。所以我们可以通过这两个事件，对容器启动时和终止时分别进行一些自定义的操作。

定义一个lifecycle-events.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-demo
spec:
  containers:
  - name: lifecycle-demo-container
    image: nginx
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh", "-c", "echo Hello postStart handler > /usr/share/message"]
      preStop:
        exec:
          command: ["/bin/sh", "-c", "echo Hello preStop handler > /usr/share/message"]
```

通过yaml创建Pod：

```shell
kubectl apply -f lifecycle-events.yaml
```

查看postStart是否生效：

```shell
kubectl exec lifecycle-demo -it -- cat /usr/share/message
```

![image-20210510143809792](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210510143821.png) 

**注意**：k8s只有在Pod结束(Terminated)的时候才会发送preStop事件，这个时间我们已经无法进入容器了，所以这里就没法演示效果了。

### 6.k8s核心概念

#### 6.1 名称空间-Namespace

Namespace在很多情况下用于实现多用户的资源隔离，通过将集群内部的资源对象分配到不同的名称空间中，形成逻辑上的分组，便于不同的分组在共享使用整个集群资源的同时还能被分别管理。k8s集群在启动后，会创建一个名为`default`的默认名称空间，如果不特别指明名称空间，那么用户创建的Pod等资源都会被系统默认分配到这个默认的名称空间中。

> **说明：**我们应避免使用前缀`kube-`来创建名称空间，因为它是为Kubernetes系统名称空间保留的。

并非所有对象都在名称空间中。k8s的大多数资源(比如Pod，service，副本控制器等)都会位于某些名称空间中，但是名称空间资源本身并不在名称空间中，而且底层资源(比如node，persistentvolume等)也都不属于任何名称空间。我们可以通过以下命令查看哪些Kubernetes资源在名称空间中，哪些不在名称空间中：

```shell
#查询位于名称空间中的资源
kubectl api-resources --namespaced=true

#查询不在名称空间中的资源
kubectl api-resources --namespaced=false
```

##### 6.1.1 查看名称空间

```shell
#使用以下三个命令中的任何一个都可以，效果是一样的
kubectl get namespaces
kubectl get namespace
kubectl get ns
```

![image-20210531170507346](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210531170518.png) 

初始状态下，Kubernetes默认会具有三个名称空间：

- `default` 无名称空间对象的默认名称空间；
- `kube-system` 由Kubernetes系统创建的对象的名称空间；
- `kube-public` 自动创建且被所有用户可读的名称空间(包括未经身份认证的)。此名称空间通常在某些资源在整个集群中可见且可公开读取时被集群使用，此名称空间的公共方面只是一个约定，而不是一个必要条件。

##### 6.1.2 创建名称空间

- 新建一个名为`my-namespace.yaml`的yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ns-test
```

- 通过yaml文件方式进行创建：

```shell
kubectl create -f my-namespace.yaml
```

或者我们也可以直接通过命令的方式创建名称空间：

```shell
kubectl create ns ns-abc
```

查看上面创建的两个名称空间：

![image-20210531172310025](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210531172313.png) 

##### 6.1.3 删除名称空间

比如我们想要删除上面创建的名为`ns-abc`的名称空间，命令如下：

```shell
kubectl delete ns ns-abc
```

> **注意**：以上命令会删除对应名称空间下的所有内容。

##### 6.1.4 指定名称空间创建资源

这里以创建Pod为例，新增一个`Pod.yaml`文件，文件内容如下：

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: sys-test
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-name
  namespace: sys-test
spec:
  containers:
  - name: c-name
    image: nginx:1.20
    imagePullPolicy: IfNotPresent
  restartPolicy: OnFailure
```

> 由于`sys-test`这个名称空间之前并不存在，所以以上yaml文件中也加上了创建名称空间的内容。

通过`kubectl apply -f Pod.yaml`命令创建Pod，然后通过`kubectl get pod -n sys-test -o wide`命令进行查看：

![image-20210531203330730](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210531203333.png) 

#### 6.2 标签-Label

Label是Kubernetes系统中一个核心概念。一个Label就是一个key=value的键值对，其中key与value由用户自己指 定。Label可以附加到各种资源对象上，如Node、Pod、Service等，一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上，Label通常在资源对象定义时确定，也可以在对象创建后动态添加或删除。

##### 6.2.1 标签的基本用法

Label最常见的用法是使用`metadata.labels`字段来为对象添加Label，通过`spec.selector`来引用对象。示例：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-name
  namespace: sys-test
  labels:
    app: deploy-label
spec:
  replicas: 3
  selector:
    matchLabels:
      app: test-label
  template:
    metadata:
      labels:
        app: test-label
    spec:
      containers:
      - name: c-name
        image: nginx:1.20
        imagePullPolicy: IfNotPresent
```

![image-20210531212203425](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210531212211.png) 


> **注意**：在以上yaml文件中，`spec.selector.matchLabels`下的标签要和`spec.template.metadata.labels`下的标签保持一致，可以不和`metadata.labels`下的标签(即**app: deploy-label**)保持一致。

##### 6.2.2 根据标签选择资源

假设同一个名称空间下有很多Pod或者其他别的资源，我们也可以通过标签来进行资源的选择，比如我们想要展示指定标签的Pod，命令如下：

```shell
#查看Pod的标签情况
kubectl get pod -n sys-test --show-labels

#根据指定标签(标签key值为app，value值为pod-label)展示相应的Pod
kubectl get pod -l app=pod-label -n sys-test
```

![image-20210531214854544](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210531214858.png) 

##### 6.2.3 多标签选择的使用

如果想通过多个标签作为条件来筛选的话，标签之间使用逗号隔开即可，如下所示：

```shell
#查看Pod的标签情况
kubectl get pod -n sys-test --show-labels

#给某一个Pod加个其他的标签(env=dev)，方便下面多标签筛选用
kubectl label pod <pod-name> env=dev -n sys-test

#使用多标签进行筛选
kubectl get pod -l app=test-label,env=dev -n sys-test
```

![image-20210601094856214](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601094859.png) 

##### 6.2.4 基于集合的用法

提前准备一批Pod，标签情况如下：

![image-20210601103623492](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601103626.png) 

- 筛选包含app标签，并且type标签值不等于pig的所有Pod：

```shell
#由于写该文档使用的字体的缘故，所有不等于都会被自动转换成!=，特此说明
kubectl get pod -l 'app,type!=pig' -n sys-test --show-labels
```

![image-20210601113056491](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601113059.png) 

- 筛选app标签值等于test-label，type标签值不等于pig的所有Pod：

```shell
kubectl get pod -l 'app=test-label,type!=pig' -n sys-test --show-labels
```

![image-20210601113320829](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601113323.png) 

- 筛选包含app标签，并且type标签值包含pig和dog的所有Pod：

```shell
kubectl get pod -l 'app,type in (pig,dog)' -n sys-test --show-labels
```

![image-20210601113718595](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601113721.png) 

- 筛选不包含type标签的所有Pod：

```shell
kubectl get pod -l '!type' -n sys-test --show-labels
```

![image-20210601114005064](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601114007.png) 

- 筛选包含type标签，但是标签值不包含dog和pig的所有Pod：

```shell
kubectl get pod -l 'type,type notin (dog,pig)' -n sys-test --show-labels
```

![image-20210601114201274](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601114204.png) 

##### 6.2.5 selector字段的用法

前面有提到过，Label最常见的用法是使用`metadata.labels`字段来为对象添加Label，通过`spec.selector`来引用对象，而selector下又包含了==matchLabels==和==matchExpressions==两个字段，比如Job、Deployment等，如下图：

![image-20210601115404447](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601115407.png) 

下面是这两个字段的一个基本的使用格式：

```yaml
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - {key: tier, operator: In, values: [cache]}
    - {key: environment, operator: NotIn, values: [dev]}
```

> **注意**：如果同时使用了`matchLabels`和`matchExpressions`，那么必须在`matchLabels`和`matchExpressions`中给出的所有条件都满足才能匹配。

##### 6.2.6 标签的新增和删除

```shell
#新增标签(标签的key值为app，value值为test)
kubectl label pod pod-name app=test -n sys-test

#根据key值删除标签(标签的key值为app)
kubectl label pod pod-name app- -n sys-test
```

![image-20210601142329645](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601142333.png) 

### 7.k8s核心技术-控制器

k8s提供了很多内置的controller(控制器)，或者叫工作负载资源，通过这些控制器可以控制Pod的具体状态和行为。

控制器的主要分类：

- `Deployment`和`ReplicaSet`(替换原来的`ReplicationController`)。Deployment很适合用来管理我们在集群上的无状态应用，Deployment中的所有Pod都是相互等价的，并且可以在需要的时候被换掉；
- `StatefulSet`。让我们能够运行一个或者多个以某种方式跟踪应用状态的Pods，很适合管理集群上的有状态应用。例如，假设我们需要将数据作持久存储，那我们就可以运行一个StatefulSet，将每个Pod与某个PersistentVolume对应起来。我们在StatefulSet中各个Pod内运行的代码可以将数据复制到同一StatefulSet中的其它Pod中以提高整体的服务可靠性；

- `DaemonSet`。定义提供节点本地支撑设施的Pods。这些Pods可能对于我们集群的运维是非常重要的，例如作为网络链接的辅助工具或者作为网络[插件](https://kubernetes.io/zh/docs/concepts/cluster-administration/addons/)的一部分等等。每次我们向集群中添加一个新节点时，如果该节点与某DaemonSet的规约匹配，则控制面会为该DaemonSet调度一个Pod到该新节点上运行；
- `Job`和`CronJob`。定义一些一直运行到结束并停止的任务。Job用来表达的是一次性的任务，而CronJob会通过cron表达式定时创建Job，然后通过Job运行一些运行到结束并停止的任务。

> 由于`ReplicationController`的一些缺陷，已经不再推荐使用了，所以下文就不再进行介绍了。而`Deployment`是可以管理`ReplicaSet`的，并且拥有更多的功能，所以我们一般不会直接创建`ReplicaSet`，而是创建`Deployment`，而它会自动创建`ReplicaSet`，再通过`ReplicaSet`去创建Pod。kubernetes官网建议我们使用`Deployment`而不是直接使用`ReplicaSet`，所以下文中也不会再介绍`ReplicaSet`了。

#### 7.1 Deployment

Deployment为Pod和ReplicaSet提供了一个声明式定义方法，用来替代以前的ReplicationController来方便地管理应用。需要注意的是，Deployment是先创建ReplicaSet，然后再通过ReplicaSet去创建Pod。

典型的应用场景包括：

- 创建Deployment以将ReplicaSet上线，ReplicaSet在后台创建Pod；
- 滚动升级和应用回滚；
- 应用扩缩容；
- 暂停和恢复Deployment。

##### 7.1.1 创建Deployment

下面是Deployment的一个简单案例，创建一个名为nginx-deployment.yaml的文件：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
        ports:
        - containerPort: 80 
```

在以上案例中：

- 创建了一个名为**nginx-deployment**(由`metadata.name`字段标明)的Deployment；
- 该Deployment创建了3个(由`spec.replicas`字段标明)Pod副本；
- 以上案例中的`spec.selector`字段定义Deployment如何查找要管理的Pods。在这里，我们选择在Pod模板中定义的标签(`app: nginx`)。当然，也可以存在更复杂的选择规则，只要Pod模板本身满足所给规则即可。

通过`kubectl apply -f nginx-deployment.yaml`命令创建Deployment，通过`kubectl get all -o wide`命令查看是否创建成功：

![image-20210601165710858](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601173137.png) 

> **注意**：我们也可以通过`--record`标志将所执行的命令写入资源注解`kubernetes.io/change-cause`中。这对于以后的检查是有用的。例如，要查看针对每个Deployment修订版本所执行过的命令。

假设我们是使用`kubectl apply -f nginx-deployment.yaml --record`命令创建Deployment的，那么可以发现这条命令已经保存到资源注解的`kubernetes.io/change-cause`中了，如下所示：

```shell
kubectl get deploy nginx-deployment -o yaml|grep -10 " annotations"
```

![image-20210601174215278](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601174219.png) 

**注**：默认情况下应该是只保存最后一次操作记录，而且查询的记录不会保存，我尝试过扩缩容操作，扩缩容命令记录也能保存下来，所以猜测应该是只会保存最后一次修改的记录。

在Deployment创建成功后，可以使用以下任一命令进行查看，三个命令查询的效果都是一样的，如果想查看更多信息，只要在命令后面加上`-o wode`即可，比如`kubectl get deploy -o wide`。

```shell
kubectl get deploy
kubectl get deployment
kubectl get deployments
```

比如执行`kubectl get deploy`命令后，则会输出类似下面的内容：

```shell
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           16m
```

- `NAME`：列出了集群中Deployment的名称；
- `READY`：显示应用程序的可用的**副本数**。显示的模式是"就绪个数/期望个数"；
- `UP-TO-DATE`：显示为了达到期望状态已经更新的副本数；
- `AVAILABLE`：显示应用可供用户使用的副本数；
- `AGE`：显示应用程序运行的时间。

> **注意**：期望副本数是根据yaml中的`spec.replicas`字段设置的。

如果我们要查看每个Pod的标签，可以执行`kubectl get pods --show-labels`命令，输出结果如下：

![image-20210601180144901](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601180147.png) 

从输出结果可以发现，每个Pod都有一个**pod-template-hash**标签，这个标签我们不要去修改它，Deployment控制器会将该标签添加到Deployment所创建或收留的每个ReplicaSet中，此标签可确保Deployment的子ReplicaSets不重叠。标签通过对ReplicaSet的`PodTemplate`进行哈希处理会生成一个哈希值，该标签会存在于ReplicaSet可能拥有的任何现有Pod中。

##### 7.1.2 更新Deployment

我们可以通过`kubectl set image`命令对Deployment进行更新操作，具体操作如下：

```shell
#查看Deployment目前的基本信息
kubectl get deploy -o wide

#将名为nginx-deployment的Deployment中nginx的镜像版本更新至1.20.1
kubectl set image deployment/nginx-deployment nginx=nginx:1.20.1

#实时状态查看
kubectl rollout status deployment/nginx-deployment
```

![image-20210601211503658](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601211506.png) 

当Deployment更新完成之后，可以使用`kubectl get rs -o wide`命令查看ReplicaSet的情况，如下图所示，可以发现，虽然Deployment已经更新完了，且创建了新的ReplicaSet，但是老的ReplicaSet并不会被删除，不过其副本数已经为零了。

![image-20210601212756188](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601212800.png) 

我们也可以使用`kubectl edit deploy nginx-deployment`命令来修改Deployment中containers字段下的image属性值，来达到更新Deployment的目的。

> **注意**：假如我们要创建一个Deployment以生成`nginx:1.14.2`的5个副本，但紧接着就更新这个Deployment以创建5个`nginx:1.16.1`的副本，而此时假设只有3个`nginx:1.14.2`副本已创建。在这种情况下，Deployment会立即开始杀死那3个`nginx:1.14.2`的Pods，并开始创建`nginx:1.16.1`的Pods。它不会等待`nginx:1.14.2`的5个副本都创建完成后才开始执行Deployment的变更动作。

##### 7.1.3 回滚Deployment

新建一个名为deployment.yaml的文件，内容如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-name
  labels:
    app: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-label
  template:
    metadata:
      labels:
        app: nginx-label
    spec:
      containers:
      - name: c-nginx
        image: nginx:1.16
        ports:
        - containerPort: 80
```

首先通过`kubectl apply -f deployment.yaml --record`命令执行文件。这里需要先升级几个版本，然后才能演示回滚到之前的版本，命令如下：

```shell
#依次执行以下命令，多升级几个版本
kubectl set image deploy deploy-name c-nginx=nginx:1.17 --record
kubectl set image deploy deploy-name c-nginx=nginx:1.18 --record
kubectl set image deploy deploy-name c-nginx=nginx:1.19 --record
kubectl set image deploy deploy-name c-nginx=nginx:1.20 --record

#以上几个命令都执行完成之后，可以通过如下命令查看修订历史(以上命令后面没加"--record"的话，查出来的就是空)
kubectl rollout history deploy deploy-name
```

![image-20210602100842626](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210602100845.png) 

我们也可以通过`--revision`查看某一个版本的详情，命令如下：

```shell
#查看版本3的详情
kubectl rollout history deploy deploy-name --revision=3
```

![image-20210602101231944](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210602101234.png) 

接下来进行回滚操作，假设我们要回滚到上一版本(即版本4)，可以使用如下命令：

```shell
kubectl rollout undo deploy deploy-name
```

![image-20210602104556192](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210602104558.png) 

由上图可知，回滚到版本4成功后，在修订历史中，之前的版本4就变成了现在的版本6了。如果我们再次执行回滚到上一版本的命令，就会回滚到版本5，这样一来，nginx的版本就又会变成1.20了。如果再次查看修订历史，之前的版本5则会变成版本7，如下图所示：

![image-20210602105554704](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210602105557.png) 

如果我们不想只回滚到上一版本，也可以通过使用`--to-revision`来回滚到特定的修订版本，案例如下：

```shell
#回滚到版本2
kubectl rollout undo deploy deploy-name --to-revision=2
```

![image-20210602110838731](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210602110841.png) 

##### 7.1.4 缩放Deployment

###### 7.1.4.1 kubectl scale命令

我们可以使用`kubectl scale`命令来增加或者减少Deployment的副本数，案例如下：

```shell
#假设现在已经运行了一个副本数为3个的Deployment，可以通过如下命令将副本数扩容到5个
kubectl scale deploy deploy-name --replicas=5

#也可以使用如下命令将副本数缩容到2个
kubectl scale deploy deploy-name --replicas=2
```

![image-20210602114939473](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210602114942.png) 

###### 7.1.4.2 HPA实现水平扩缩容

我们还可以使用[Horizontal Pod Autoscaler](https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/)(简称HPA)进行[水平自动缩放](https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)，HPA 可以基于 CPU 利用率自动扩缩ReplicationController、Deployment、ReplicaSet和StatefulSet中的Pod数量。除了CPU利用率，也可以基于其他应程序提供的自定义度量指标来执行自动扩缩。Pod自动扩缩不适用于无法扩缩的对象，比如DaemonSet等。

**部署metrics-server服务：**

使用HPA需要metrics-server环境，所以这里先部署metrics-server环境。具体步骤如下：

- 下载components.yaml文件：

  ```shell
  #最好从以下地址获取components.yaml文件，这个是修改过的，直接从官方github获取的可能无法使用
  wget https://files.cnblogs.com/files/gongcqq/components.tar.gz
  
  #解压获取components.yaml文件
  tar -zxvf components.tar.gz
  ```

- 执行components.yaml文件：

  ```shell
  #components.yaml文件中会用到一个镜像，网速不好的可以使用如下命令在worker节点提前下载
  docker pull gongcqq/metrics-server:v0.4.2
  
  #使用如下命令执行解压出来的名为components-0.4.2.yaml的yaml文件
  kubectl apply -f components-0.4.2.yaml
  
  #然后使用如下命令查看metrics-server相关的Pod是否正常运行，并已处于READY状态
  kubectl get pod -n kube-system -o wide
  ```

  ![image-20210603143058580](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603143102.png) 

- 检查metrics-server是否可以正常使用：

  ```shell
  #检查相关apiservice是否可用
  kubectl get apiservice |grep "v1beta1.metrics.k8s.io"
  
  #使用kubectl top命令检查是否有数据
  kubectl top nodes
  ```

  ![image-20210603143829060](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603143831.png) 

- 修改kube-apiserver.yaml文件：

  ```shell
  #先使用vim编辑器打开文件
  vim /etc/kubernetes/manifests/kube-apiserver.yaml
  
  #然后在spec.containers.command字段中添加如下内容，并保存退出，见下图
  --enable-aggregator-routing=true
  ```

  ![image-20210603150653553](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603150658.png) 

  ```shell
  #修改kube-apiserver.yaml文件后API Server会自动重启生效，稍等一会儿后再次使用如下命令查看
  kubectl top nodes
  kubectl get apiservice |grep "v1beta1.metrics.k8s.io"
  ```

  ![image-20210603155539667](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603155541.png) 

  由上图可知，metrics-server已经部署成功了。

  > **注意**：我的kubernetes集群是使用kubeadm方式安装的，如果是使用二进制方式安装的Kubernetes集群，是否还可以通过修改kube-apiserver.yaml文件的方式来解决metrics-server不可用的问题，暂未可知。

**关于水平自动扩缩容的案例如下：**

metrics-server部署成功后，就可以测试HPA的水平扩缩容了，下面是一个简单的案例。

- 新建一个名为hpa-example.yaml的yaml文件，内容如下：

  ```yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: hpa-deploy
  spec:
    selector:
      matchLabels:
        run: hpa-label
    replicas: 1
    template:
      metadata:
        labels:
          run: hpa-label
      spec:
        containers:
        - name: hpa-example
          image: gongcqq/hpa-example:1.0
          ports:
          - containerPort: 80
          resources:
            limits:
              cpu: 500m
            requests:
              cpu: 200m
  ---
  apiVersion: v1
  kind: Service
  metadata:
    name: hpa-service
    labels:
      run: hpa-label
  spec:
    ports:
    - port: 80
    selector:
      run: hpa-label
  ```

  > 需要注意的是，以上yaml文件中，一定要有`spec.resources`属性，并要设置其下的limits字段以及requests字段中cpu的相关的值，不然下面没法根据CPU进行自动扩缩容。

- 使用`kubectl apply -f hpa-example.yaml`命令进行执行，然后使用`kubectl get deploy,pod,svc -o wide`命令进行查看：

  ![image-20210603160242740](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603160245.png)  

- 使用如下命令创建一个HPA，实现自动扩缩容：

  ```shell
  #下面命令的意思是，当现有Deployment中Pod的平均CPU利用率达到50%的时候，就进行自动扩缩容，k8s中会有相关算法决定最终的副本数的个数。"--max"参数表示副本数最大扩到10个，"--min"参数表示最小缩到1个。
  kubectl autoscale deploy hpa-deploy --cpu-percent=50 --min=1 --max=10
  ```

- 创建好HPA后，可以通过`kubectl get hpa`命令进行查看：

  ![image-20210603160539464](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603160541.png)  

  > **注意**：当前的CPU利用率为`0%`或者`<unknown>`，这是由于我们尚未发送任何请求到服务器，当负载增加后，这里的利用率才会出现具体百分比。

- 我们可以通过如下命令启动一个容器，并通过一个循环向**hpa-service**服务器发送无限的查询请求来增加CPU负载：

  ```shell
  #重新开一个终端窗口执行以下命令
  kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while true; do wget -q -O- http://hpa-service; done"
  ```

  ![image-20210602194909060](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210602194913.png) 

- 一段时间后，我们再使用`kubectl get hpa`命令进行查看，结果如下图所示：

  ![image-20210603160959992](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210603160959992.png) 

- 使用`kubectl get deploy,pod -o wide`命令查看Pod情况，看看是否进行了自动扩容，结果如下图所示：

  ![image-20210603161601362](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603161605.png) 

#### 7.2 StatefulSet





#### 7.3 DaemonSet





#### 7.4 Job







#### 7.5 CronJob











































































### 8.k8s中的集群调度

#### 8.1 调度的基本概述

1. kube-scheduler是k8s集群的默认调度器，该调度器会将Pod放到合适的Node上，然后对应Node上的Kubelet才能够运行这些Pod；
2. 对每一个新建的Pod或者是未被调度的Pod，kube-scheduler会选择一个最优的Node去运行这个Pod。然而，Pod内的每一个容器对资源都有不同的需求，而且Pod本身也有不同的资源需求。因此，Pod在被调度到Node上之前，根据这些特定的资源调度需求，需要对集群中的Node进行一次过滤；
3. 集群中，满足一个Pod调度请求的所有Node称之为**可调度节点**。如果没有任何一个Node能满足Pod的资源请求，那么这个Pod将一直停留在未调度状态直到调度器能够找到合适的Node；
4. 调度器会先在集群中找到一个Pod的所有可调度节点，然后根据一系列函数对这些可调度节点进行打分，最终选出其中得分最高的Node来运行Pod。之后，调度器将这个调度决定通知给kube-apiserver，这个过程叫做**绑定**；
5. 在做调度决定时需要考虑的因素包括：单独和整体的资源请求、硬件/软件/策略限制、亲和以及反亲和要求、数据局域性、负载间的干扰等等。

#### 8.2 kube-scheduler的调度流程

kube-scheduler给一个Pod做调度选择包含两个步骤：

- 过滤；
- 打分。

过滤阶段会将所有满足Pod调度需求的Node选出来。例如，PodFitsResources过滤函数会检查候选Node的可用资源能否满足Pod的资源请求。在过滤之后，得出一个Node列表，里面包含了所有可调度节点；通常情况下，这个Node列表包含不止一个Node。如果这个列表是空的，代表这个Pod不可调度。

在打分阶段，调度器会为Pod从所有可调度节点中选取一个最合适的Node。根据当前启用的打分规则，调度器会给每一个可调度节点进行打分。

最后，kube-scheduler会将Pod调度到得分最高的Node上。如果存在多个得分最高的Node，kube-scheduler会从中随机选取一个。

#### 8.3 固定节点调度

某些场景下，我们会希望Pod只在固定节点上运行，这时候就需要用到固定节点调度了，下面会介绍两种调度的方式。

##### 8.3.1 通过指定节点名称的方式

通过`kubectl get node`命令查看一下集群中的节点名称，我这边worker node有**node1**和**node2**两个。

```shell
[root@master ~]# kubectl get node
NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   59d   v1.18.0
node1    Ready    <none>   59d   v1.18.0
node2    Ready    <none>   59d   v1.18.0
```

创建一个nodeName.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  nodeName: node1
```

上面创建Pod的文件中，spec属性下的`nodeName`属性就是用来进行固定节点调度的，该属性的值配置的是node1，表示该Pod只会运行在node1节点上。

通过yaml创建Pod：

```shell
kubectl apply -f nodeName.yaml
```

查看Pod的运行状态：

```shell
kubectl get pod -o wide
```

![image-20210510201406729](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210510201745.png) 

由上图可知，该Pod确实是运行在node1节点上了的。不过这里只有一个Pod，具有随机性，并不能说明就是`nodeName`属性起了作用。所以下面通过创建Deployment的方式来进行验证，并把副本数设置成10，如果10个Pod都在node1节点上的话，肯定就是`nodeName`属性起了作用。对应的yaml文件内容如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-nginx
  labels:
    app: deploy-label
spec:
  selector:
    matchLabels:
      app: deploy-label
  replicas: 10
  template:
    metadata:
      labels:
        app: deploy-label
    spec:
      nodeName: node1
      containers:
      - name: nginx
        image: nginx
```

通过yaml文件创建Pod后，使用`kubectl get pod -o wide`命令查看Pod的运行状态：

![image-20210510203719164](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210510203732.png) 

由上图可知，通过Deployment方式创建的10个Pod都运行在了node1节点上，说明确实是`nodeName`属性起了作用。

##### 8.3.2 通过标签选择的方式

这种方式需要用到`nodeSelector`属性。首先需要在指定的节点上打标签，然后再在创建Pod的时候使用`nodeSelector`属性选择这个标签，通过这种方式即可完成固定节点调度。

给node1节点打标签：

```shell
#下面的disktype是自定义的标签的key值，ssd是自定义的标签的value值
kubectl label nodes node1 disktype=ssd
```

通过如下两种方式查看节点标签是否新增成功：

```shell
kubectl get nodes node1 --show-labels
kubectl describe node node1|grep -10 Labels
```

![image-20210510210257990](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210510210300.png) 

节点标签新增成功后，新增nodeSelector.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd
```

上面创建Pod的文件中，spec属性下的`nodeSelector`属性就是用来进行固定节点调度的，该属性下的disktype就是节点标签的key值，ssd就是节点标签的value值。

通过yaml文件创建Pod：

```shell
kubectl apply -f nodeSelector.yaml
```

查看Pod的运行状态：

```shell
kubectl get pod -o wide
```

![image-20210510211312836](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210510211330.png) 

由上图可知，该Pod确实已经运行在node1节点上了，为了防止是由于偶然性，这里仍然通过创建Deployment的方式来进行验证，副本数设置成5个，对应的yaml文件内容如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-nginx
  labels:
    app: deploy-label
spec:
  selector:
    matchLabels:
      app: deploy-label
  replicas: 5
  template:
    metadata:
      labels:
        app: deploy-label
    spec:
      containers:
      - name: nginx
        image: nginx
      nodeSelector:
        disktype: ssd
```

通过yaml文件创建Pod后，使用`kubectl get pod -o wide`命令查看Pod的运行状态：

![image-20210510212314782](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210510212317.png) 

由上图可知，通过Deployment方式创建的5个Pod都运行在了node1节点上，说明确实是`nodeSelector`属性起了作用。

#### 8.4 亲和性与反亲和性

使用`nodeSelector`属性可以提供一种非常简单的方法来将Pod约束到具有特定标签的节点上。而亲和性与反亲和性功能则极大地扩展了我们可以表达约束的类型，关键的增强点包括：

- 语言更具表现力，提供了更多的匹配规则；
- 规则是具有偏好性的，并不是硬性要求，所以当调度器不能满足要求的时候，Pod仍然可以被调度；
- 亲和性功能包含两种类型的亲和性，即节点的亲和性与反亲和性以及Pod间的亲和性与反亲和性；
- 我们可以使用节点上的Pod的标签来进行约束，而不是使用节点本身的标签。亲和性与反亲和性允许哪些Pod可以或者不可以被放置在一起。

##### 8.4.1 节点的亲和性与反亲和性

节点的亲和性概念上类似于`nodeSelector`，它使我们可以根据节点上的标签来约束Pod可以被调度到哪些节点。

目前有如下两种类型的节点亲和性：

- `requiredDuringSchedulingIgnoredDuringExecution` ：**硬策略**。该策略指定了将Pod调度到一个节点上必须满足的规则(就像`nodeSelector`一样，但使用了更具表现力的语法)；
- `preferredDuringSchedulingIgnoredDuringExecution`：**软策略**。该策略会指定调度器尝试将Pod调度到指定节点上，是具有偏好性的，并不是说Pod就一定要运行到指定的节点上。

> 如果使用硬策略将Pod调度到指定节点上后，节点的相关标签在运行时发生了变更，从而不再满足Pod上的亲和性规则的话，Pod将仍然会继续在该节点上运行。`requiredDuringSchedulingRequiredDuringExecution`这种策略可能会在未来的k8s中出现，使用这种策略会将Pod从不再满足Pod的节点亲和性要求的节点上驱逐。

节点的亲和性是通过Pod.spec的`affinity`字段下的`nodeAffinity`字段进行指定的。

下面是一个使用节点亲和性的Pod案例：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: nginx
```

以上节点亲和性的规则表示，Pod只能放置在具有标签键`e2e-az-name`且标签值为`e2e-az1`或`e2e-az2`的节点上。在满足这些条件的节点中，具有标签键为`another-node-label-key`且标签值为`another-node-label-value`的节点将会被优先使用。

我们可以在上面的例子中看到operator属性值为`In`的操作符的使用。节点亲和性语法支持下面这些操作符：

- `In`：标签值在某个列表中；
- `NotIn`：标签值不在某个列表中；
- `Exists`：某个标签值存在；
- `DoesNotExist`：某个标签值不存在；
- `Gt`：标签值大于某个值(字符串比较)；
- `Lt`：标签值小于某个值(字符串比较)。

> 在这些操作符中，我们可以使用`NotIn`和`DoesNotExist`来实现节点的反亲和性行为。

**注意事项：**

1. 如果同时指定了`nodeSelector`和`nodeAffinity`，两者必须都要满足，才能将Pod调度到候选节点上；
2. 如果指定了多个与`nodeAffinity`类型关联的`nodeSelectorTerms`，只要其中有一个`nodeSelectorTerms`满足的话，Pod就可以调度到指定节点上；
3. 如果指定了多个与`nodeSelectorTerms`关联的`matchExpressions`，则只有当所有的`matchExpressions`满足要求，Pod才会调度到指定节点上；
4. 如果我们修改或删除了Pod所调度到的节点的标签，Pod不会被删除；
5. 软策略中的`weight`属性是权重的意思，取值范围是1-100。如果存在多个软策略的话，对于亲和性规则而言，权重越大越亲和；对于反亲和性规则而言，权重越大越不亲和。

##### 8.4.2 Pod间的亲和性与反亲和性

**基本概念和注意事项：**

- Pod间的亲和性与反亲和性同样有两种策略，即硬策略`requiredDuringSchedulingIgnoredDuringExecution`和软策略`preferredDuringSchedulingIgnoredDuringExecution`；

- Pod间亲和性与反亲和性使我们可以基于已经在节点上运行的Pod的标签来约束Pod可以调度到的节点，而不是基于节点上的标签；
- Pod间亲和性与反亲和性需要大量的处理，这可能会显著减慢大规模集群中的调度，所以不建议在超过数百个节点的集群中使用它们；
- Pod反亲和性需要对节点进行一致的标记，即集群中的每个节点必须具有适当的标签能够匹配`topologyKey`。如果某些或所有节点缺少指定的`topologyKey`标签，可能会导致意外行为；
- Pod间的亲和性通过spec属性中`affinity`字段下的`podAffinity`字段进行指定；而Pod间的反亲和性是通过spec中`affinity`字段下的`podAntiAffinity`字段进行指定；
- Pod间的亲和性与反亲和性的合法操作符有`In`，`NotIn`，`Exists`，`DoesNotExist`。

**Pod间亲和性与反亲和性的基本案例：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: "kubernetes.io/hostname"
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: "kubernetes.io/hostname"
  containers:
  - name: with-pod-affinity
    image: nginx
```

在上面的案例中，分别定义了一条Pod亲和性规则和一条Pod反亲和性规则。

在这个案例中，`podAffinity`配置的是硬策略，而`podAntiAffinity`配置的是软策略。案例中`podAffinity`属性下配置的亲和性规则是指将该Pod调度到`topologyKey`属性表示的拓扑域上，这个区域上至少需要存在一个正在运行的包含键为security值为S1的标签的Pod。案例中`podAntiAffinity`属性下配置的反亲和性规则是指，如果`topologyKey`属性表示的拓扑域上存在正在运行的包含键为security值为S2的标签的Pod，那么该Pod将不会被调度到这个区域上的节点中。

**关于topologyKey属性的解释说明：**

- `topologyKey`属性是用于表示一个拓扑域的，并不是表示一个节点，一个拓扑域可以包含多个节点；
- `topologyKey`后跟的是节点标签的key值，假设一个k8s集群中有A、B、C三个节点，它们都拥有名为**env**这个相同key值的节点标签，但是标签的value值分别是**dev**、**test**、**prod**，那么这时候拓扑域和集群中的节点就是一对一的关系，这时候将Pod调度到拓扑域上就是指将Pod调度到k8s集群的某一节点上。如果A、B两个节点的标签的value值都是**dev**，而C节点的标签的value值是**test**的话，拓扑域和节点就是一对多的关系了，这时候的A、B节点是处于同一拓扑域的，C节点是处于另一个拓扑域的；
- Pod间的亲和性规则并不是说两个Pod一定会处于同一节点上，只有当拓扑域和节点是一对一关系的时候才会被调度到同一节点上，如果拓扑域中包含多个节点的话，配置了Pod间亲和性规则后，k8s只会将该Pod调度到和另一个Pod相同拓扑域中的某一个节点上，并不一定就和另一个Pod是同一个节点，但一定和这个Pod处于同一拓扑域；
- 以上案例中，`topologyKey`属性对应的值是"kubernetes.io/hostname"，这个是k8s给集群中所有的节点默认加的标签的key值，value值就是每个节点的节点名，集群中节点的节点名是不会重复的，所以案例中的拓扑域和节点之间就是一对一的关系。

原则上，topologyKey可以是任何合法的标签键。然而，出于性能和安全原因，topologyKey有如下限制：

1. 对于亲和性与`requiredDuringSchedulingIgnoredDuringExecution`要求的Pod反亲和性，`topologyKey`不允许为空；
2. 对于在Pod反亲和性中使用了`requiredDuringSchedulingIgnoredDuringExecution`硬策略的情况，引入了准入控制器`LimitPodHardAntiAffinityTopology`来限制`topologyKey`不为"kubernetes.io/hostname"。如果我们想使它可用于自定义拓扑结构，则必须修改准入控制器或者禁用它；
3. 对于`preferredDuringSchedulingIgnoredDuringExecution`要求的Pod反亲和性，空的`topologyKey`的意思就是"所有拓扑结构"，这里的"所有拓扑结构"就是`kubernetes.io/hostname`、`topology.kubernetes.io/zone`和`topology.kubernetes.io/region`的组合。

**Pod间亲和性与反亲和性的进阶案例：**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  selector:
    matchLabels:
      app: store
  replicas: 2
  template:
    metadata:
      labels:
        app: store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: redis-server
        image: redis:6.0
```

以上案例是使用Pod间的反亲和性规则使创建的两个Pod不在同一节点，创建Pod后效果如下：

![image-20210511231937158](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210511232617.png) 

由上图可知，通过Deployment创建的两个副本Pod确实都不在同一节点上，我这边集群中worker节点只有两个，所以这里副本数才设置成2，上述案例设置了反亲和性规则的硬策略，如果副本数大于worker节点的个数的话，就会出现由于节点不够导致多出的Pod一直处于`Pending`状态的情况。假设我把副本数设置为5的话，运行结果就是下面这样：

![image-20210511233328432](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210511233331.png)  

**Pod间亲和性与反亲和性的高级案例：**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  selector:
    matchLabels:
      app: web-store
  replicas: 2
  template:
    metadata:
      labels:
        app: web-store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web-store
            topologyKey: "kubernetes.io/hostname"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: web-app
        image: nginx:1.20
```

这个案例是要结合上面那个进阶案例进行使用的，所以在使用这个yaml文件创建Deployment之前，要保证前面那个进阶案例对应的yaml文件已经创建成功，下面大致说一下这个yaml文件创建后达到的效果。

前面进阶案例的yaml文件中使用反亲和性规则使得创建的两个Pod分配在了不同的节点上，现在这个yaml文件中也使用反亲和性规则使得创建的两个Pod分配在不同的节点上，并且还对上面进阶案例中创建的Pod使用了亲和性规则，所以最终会出现两个节点上都分别有一个进阶案例中创建的Pod和这个案例中创建的Pod，具体截图如下：

![image-20210511235556468](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210511235559.png) 

#### 8.5 污点和容忍

节点亲和性是Pod的一种属性，它可以使Pod被吸引到一类特定的节点上。这可能是出于一种偏好，也可能是硬性要求。污点(Taint)则相反，它使节点能够排斥一类特定的Pod。

容忍(Tolerations)是应用于Pod上的，允许(但并不要求)Pod调度到带有与之匹配的污点的节点上。

污点和容忍(Tolerations)相互配合，可以用来避免Pod被分配到不合适的节点上。每个节点上都可以应用一个或多个污点，这表示对于那些不能容忍这些污点的Pod，是不会被该节点接受的。

##### 8.5.1 污点(Taint)

可以使用[kubectl taint](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#taint)命令给节点增加污点，节点被添加上污点之后就和Pod之间存在了一种相斥的关系，可以让节点拒绝Pod的调度执行，甚至将该节点上已经存在的Pod驱逐出去。

每个污点的组成如下：

```shell
key=value:effect
```

上面的key是污点的标签的键，value是污点的标签的值，其中value值可以为空，effect用来描述污点的作用，它支持如下三个选项：

- `NoSchedule`：表示k8s将不会把Pod调度到具有该污点的节点上；
- `PreferNoSchedule`：表示k8s将尽量避免把Pod调度到具有该污点的节点上；
- `NoExecute`：表示k8s将不会把Pod调度到具有该污点的节点上，同时会将该节点上已经存在的Pod驱逐出去。

 污点的新增、查看和删除：

```shell
# 新增污点。其中key值为check，value值为gongsl，effect为NoExecute
kubectl taint nodes node1 check=gongsl:NoExecute
# 查看污点
kubectl describe nodes node1|grep Taint
# 删除污点。其中key值为check
kubectl taint nodes node1 check:NoExecute-
```

下面进行一个案例演示：

![image-20210512143544365](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210512143551.png) 

当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括：

1. `node.kubernetes.io/not-ready`：节点未准备好。这相当于节点状态`Ready`的值为`False`；
2. `node.kubernetes.io/unreachable`：节点控制器访问不到节点。这相当于节点状态`Ready`的值为`Unknown`；
3. `node.kubernetes.io/out-of-disk`：节点磁盘耗尽；
4. `node.kubernetes.io/memory-pressure`：节点存在内存压力；
5. `node.kubernetes.io/disk-pressure`：节点存在磁盘压力；
6. `node.kubernetes.io/network-unavailable`：节点网络不可用；
7. `node.kubernetes.io/unschedulable`: 节点不可调度；
8. `node.cloudprovider.kubernetes.io/uninitialized`：如果kubelet启动时指定了一个"外部"云平台驱动，它将给当前节点添加一个污点将其标志为不可用。在cloud-controller-manager的一个控制器初始化这个节点后，kubelet将删除这个污点。

##### 8.5.2 容忍(Tolerations)

设置了污点的节点将根据taint的effect(NoSchedule、PreferNoSchedule、NoExecute)和Pod之间产生互斥的关系，Pod将在一定程度上不会被调度到该节点上。但我们可以在Pod上设置容忍(Tolerations)，设置了容忍的Pod将可以容忍污点的存在，可以被调度到存在污点的的节点上。

在k8s中，是使用Pod中spec属性下的`tolerations`属性进行Pod容忍的设置的。下面是Pod容忍可能涉及的相关设置：

```yaml
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  tolerationSeconds: 3600
- key: "key2"
  operator: "Exists"
  effect: "NoSchedule"
```

- 上面key，vaule，effect这些属性的属性值要与节点上设置的污点组成部分中的`key=value:effect`保持一致；
- 上面的operator属性的值默认为`Equal`，此时的value属性值需要和污点的value值相对应，operator属性的值还可以是`Exists`，此时将会忽略value属性，所以这时value属性是不用写的；
- 上面的tolerationSeconds属性用于描述当Pod需要被驱逐时仍可以在节点上继续保留运行的时间，单位是秒，也就是说多少秒后Pod才会被驱逐。

**特殊情况说明：**

1. 如果一个容忍度的key为空且operator为`Exists`，表示这个容忍度与任意的key、value和effect都匹配，即这个容忍度能容忍任意污点(taint)。

```yaml
tolerations:
- operator: "Exists"
```

2. 当不指定effect值时，表示容忍指定键名对应的所有污点效果选项。

```yaml
tolerations:
- key: "key1"
  operator: "Exists"
```

**关于Pod容忍的案例：**

先新建两个Pod，对应的yaml文件如下所示：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx1
spec:
  containers:
  - name: nginx
    image: nginx:1.20
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: "key1"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 120
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx2
spec:
  containers:
  - name: nginx
    image: nginx:1.20
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: "key2"
    operator: "Exists"
    effect: "NoExecute"
```

![image-20210513140837642](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210513140858.png) 

既然都运行在了node1节点上，那就给node1节点打污点，演示驱逐效果：

```shell
#执行如下命令，给node1节点打一个污点，这里用的是只有key值没有value值的污点
kubectl taint nodes node1 key1:NoExecute
```

节点打污点后，由于名为pod-nginx2的Pod没有匹配键值为key1的容忍，所以该Pod会被立即驱逐，名为pod-nginx1的Pod虽然有匹配键值为key1的容忍，但是由于将`tolerationSeconds`设置为了120，所以120秒后，该Pod也会被驱逐。

![image-20210513142840535](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210513142843.png) 

我们可以给一个节点添加多个污点，也可以给一个Pod添加多个容忍。k8s处理多个污点和容忍的过程就像一个过滤器：从一个节点的所有污点开始遍历，过滤掉那些Pod中存在与之相匹配的容忍度的污点。余下未被过滤的污点的effect值决定了Pod是否会被分配到该节点，特别是以下情况：

- 如果未被过滤的污点中存在至少一个effect值为`NoSchedule`的污点，则k8s不会将分配到该节点；
- 如果未被过滤的污点中不存在effect值为`NoSchedule`的污点，但是存在effect值为`PreferNoSchedule`的污点，则Kubernetes会尝试不将Pod分配到该节点；
- 如果未被过滤的污点中存在至少一个effect值为`NoExecute`的污点，则Kubernetes不会将Pod分配到该节点(如果 Pod还未在节点上运行)，或者将Pod从该节点驱逐(如果Pod已经在节点上运行)。

例如，假设我们给一个节点添加了如下污点：

```shell
kubectl taint nodes node1 key1=value1:NoSchedule
kubectl taint nodes node1 key1=value1:NoExecute
kubectl taint nodes node1 key2=value2:NoSchedule
```

假定有一个Pod，它有两个容忍度：

```shell
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
```

在这种情况下，上述Pod不会被分配到上述节点，因为其没有容忍度和第三个污点相匹配。但是如果在给节点添加上述污点之前，该Pod已经在上述节点运行，那么它还可以继续运行在该节点上，因为不能和上述Pod相匹配的上述节点的第三个污点的effect值并不是NoExecute，所以不会驱逐已经运行在节点上的Pod。

前文提到过污点的effect值`NoExecute`会影响已经在节点上运行的Pod：

- 如果Pod不能忍受effect值为`NoExecute`的污点，那么Pod将马上被驱逐；
- 如果Pod能够忍受effect值为`NoExecute`的污点，但是在容忍度定义中没有指定`tolerationSeconds`，则Pod还会一直在这个节点上运行；
- 如果Pod能够忍受effect值为`NoExecute`的污点，而且指定了`tolerationSeconds`，则Pod还能在这个节点上继续运行这个指定的时间长度。

**注意事项：**

- k8s会自动给Pod添加一个key为`node.kubernetes.io/not-ready`的容忍度并配置 `tolerationSeconds=300`，除非用户提供的Pod配置中已经已存在了key为`node.kubernetes.io/not-ready`的容忍度；

- k8s也会给Pod添加一个key为`node.kubernetes.io/unreachable`的容忍度并配置`tolerationSeconds=300`，除非用户提供的Pod配置中已经已存在了key为`node.kubernetes.io/unreachable`的容忍度；

- [DaemonSet](https://kubernetes.io/zh/docs/concepts/workloads/controllers/daemonset/)中的Pod被创建时，针对以下污点自动添加的`NoExecute`的容忍度将不会指定`tolerationSeconds`：

  - `node.kubernetes.io/unreachable`
  - `node.kubernetes.io/not-ready`

  这保证了出现上述问题时DaemonSet中的Pod永远不会被驱逐。

DaemonSet控制器自动为所有守护进程添加如下`NoSchedule`容忍度以防DaemonSet崩溃：

- `node.kubernetes.io/memory-pressure`
- `node.kubernetes.io/disk-pressure`
- `node.kubernetes.io/out-of-disk`(*只适合关键Pod*)
- `node.kubernetes.io/unschedulable`(1.10或更高版本)
- `node.kubernetes.io/network-unavailable`(*只适合主机网络配置*)

添加上述容忍度确保了向后兼容，我们也可以选择自由向DaemonSet添加容忍度。

































