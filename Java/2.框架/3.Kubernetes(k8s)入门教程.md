### 1.Kubernetes简介

#### 1.1 Kubernetes是什么

1. Kubernetes，简称k8s，是用8代替八个字符"ubernete"而成的缩写；
2. Kubernetes是一个开源的容器编排引擎，用来对容器化应用进行自动化部署、扩缩和管理，Kubernetes的目标是让部署容器化的应用简单并且高效；
3. Kubernetes是一个可移植、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。它还拥有一个庞大且快速增长的生态系统，Kubernetes的服务、支持和工具广泛可用。

#### 1.2 应用部署方式的演进

![container](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210301114626.jpg) 

##### 传统部署时代

早期，各个组织机构在物理服务器上运行应用程序。无法为物理服务器中的应用程序定义资源边界，这会导致资源分配的问题。例如，如果多个应用程序在一个物理服务器上运行，那么在某些实例中，一个应用程序可能会占用大部分资源，从而导致其他应用程序的性能不佳。

解决这个问题的方案是在不同的物理服务器上运行每个应用程序。但是由于资源利用不足而无法扩展， 并且维护许多物理服务器的成本很高。

##### 虚拟化部署时代

为了解决传统部署方式带来的问题，引入了虚拟化。虚拟化技术允许你在单个物理服务器的CPU上运行多个虚拟机（VM）。 虚拟化可以实现应用程序在虚拟机之间的隔离，并在一个应用程序的信息不能被另一个应用程序自由访问的情况下提供一定的安全级别。

虚拟化技术能够更好地利用物理服务器上的资源，并具有更好的可伸缩性，因为应用程序可以很容易地添加或更新，降低硬件成本等等，通过虚拟化，你可以将一组物理资源表示为一次性虚拟机集群。

每个VM都是一台完整的机器，在虚拟硬件之上运行所有组件，包括它自己的操作系统。

##### 容器部署时代

容器类似于VM，但是它们放宽了隔离属性，以便在应用程序之间共享操作系统（OS）。因此，容器被认为是轻量级的。与VM类似，容器有自己的文件系统、CPU、内存、进程空间等等。由于它们与底层基础架构分离，因此可以跨云和OS发行版本进行移植。

容器之所以变得流行，是因为它具有很多优秀，比如：

- 应用程序创建和部署的敏捷性：与使用VM镜像相比，容器镜像的创建更加简单和高效；
- 持续开发、集成和部署：通过快速有效的回滚(由于镜像的不可变性)，提供可靠和频繁的容器镜像构建和部署；
- 关注开发与运维的分离：在构建/发布时(而不是在部署时)创建应用程序容器镜像，从而将应用程序与基础架构分离；
- 可观察性：不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号；
- 跨开发、测试和生产的环境一致性：在便携式计算机上运行与在云中运行相同；
- 跨云和操作系统发行版本的可移植性：可在Ubuntu、RHEL、CoreOS、本地、Google Kubernetes Engine和其他任何地方运行；
- 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行OS到使用逻辑资源在OS上运行应用程序；
- 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分，并且可以动态部署和管理，而不是在一台大型单机上整体运行；
- 资源隔离：可预测的应用程序性能；
- 资源利用：高效率和高密度。

#### 1.3 Kubernetes的作用

容器是打包和运行应用程序的良好方式。在生产环境中，我们需要管理运行应用程序的容器，并确保不会出现停机。假设一个容器发生了故障，则需要启动另一个容器，如果能够通过系统自动完成该操作，就会变得更容易。而Kubernetes就可以解决这一问题。

Kubernetes为我们提供了一个灵活运行分布式系统的框架。它负责应用程序的扩展和故障转移，提供部署模式等。例如，Kubernetes可以轻松地管理系统的Canary部署。

Kubernetes可以为我们提供以下功能：

- **批处理**

  Kubernetes可以提供一次性任务、定时任务，满足批量数据处理和分析的场景。

- **服务发现和负载均衡**

  Kubernetes可以使用DNS名称或自己的IP地址公开容器。如果进入到容器的流量很高，Kubernetes能够负载平衡并分发网络流量，从而使部署稳定。

- **存储编排**

  Kubernetes允许我们自动挂载我们选择的存储系统，例如本地存储、公共云提供商等等。

- **自动部署和回滚**

  我们可以使用Kubernetes为部署的容器描述所需的状态，它可以以可控的速度将实际状态更改为所需状态。例如，我们可以自动化Kubernetes为我们的部署创建新的容器，删除现有的容器并将其所有资源用到新容器中。

- **自动装箱**

  假设为Kubernetes提供了一个节点集群，Kubernetes可以使用这些节点来运行容器化的任务。告诉Kubernetes每个容器需要多少CPU和内存(RAM)。Kubernetes可以将容器放到我们的节点上，并充分利用我们的资源。

- **自我修复**

  Kubernetes会重启失败的容器、替换容器、杀死不响应用户定义的健康检查的容器，并且在它们准备好提供服务之前不会将它们通告给客户端。

- **密钥与配置管理**

  Kubernetes允许我们存储和管理敏感信息，例如密码、OAuth令牌和SSH密钥。我们可以在不重建容器镜像的情况下部署和更新密钥以及应用程序配置，也无需在堆栈配置中暴露密钥。

#### 1.4 Kubernetes的架构

当我们部署完Kubernetes, 即拥有了一个完整的集群。一个Kubernetes集群由一组被称作节点的机器组成。这些节点上运行着Kubernetes所管理的容器化应用。集群应至少拥有一个工作节点。

##### 1.4.1 Kubernetes的集群节点

- **Master Node**

  该节点是Kubernetes的集群控制节点，对集群进行调度管理，接受集群外用户到集群的操作请求。Master Node由==API Server==、==Scheduler==、==etcd==、==Controller-Manager==组成。

- **Worker Node**

  该节点是集群的工作节点，节点上运行着用户的业务应用容器。Worker Node包括==kubelet==、==kube-proxy==以及容器运行环境(==Container Runtime==)，比如docker就是一种容器运行环境。

##### 1.4.2 Kubernetes架构图

以下是Kubernetes的架构图：

![k8s架构图](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210302094338.jpg) 

上图中涉及到的各个组件的含义，下文会有详细讲解。

### 2.Kubernetes集群的搭建

#### 2.1 前置知识点

目前生产部署Kubernetes集群主要有两种方式：

- **kubeadm**

  Kubeadm是一个Kubernetes部署工具，提供`kubeadm init`和`kubeadm join`来快速部署Kubernetes集群。具体详情也可以访问[官方地址](https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/)。

- **kubespray**

  这种方式本质上也是使用kubeadm部署k8s集群，不过该方式属于一键式部署，所以比kubeadm部署更简单。

- **二进制包**

  我们可以从github下载发行版的二进制包，手动部署每个组件，组成Kubernetes集群。

> Kubeadm方式降低了部署的门槛，但屏蔽了很多细节，遇到问题很难排查。如果想要更加可控，还是推荐使用二进制包方式部署Kubernetes集群，虽然手动部署麻烦点，但是期间可以学习到很多的工作原理，也利于后期维护。

#### 2.2 kubeadm方式搭建k8s集群(单master)

kubeadm是官方社区推出的一个用于快速部署k8s集群的工具，这个工具能通过两条指令完成一个k8s集群的部署：

1. 使用`kubeadm init`创建一个Master节点；
2. 使用`kubeadm join <Master节点的IP和端口>`将Node节点加入到当前集群中。

##### 2.2.1 安装要求

- 一台或多台机器，操作系统为CentOS7；

- 硬件配置：2GB或更多RAM，2个CPU或更多CPU；

- 集群中所有机器之间网络互通；

- 可以访问外网，因为需要联网拉取镜像；

- 需要禁止swap分区。

##### 2.2.2 最终目标

1. 在所有节点上安装Docker和kubeadm；
2. 部署Kubernetes Master；
3. 部署容器网络插件；
4. 部署Kubernetes Node，将节点加入到Kubernetes集群中；
5. 部署Dashboard Web页面，可视化查看Kubernetes资源。

##### 2.2.3 主机规划

| 角色   | 主机名 | IP            |
| ------ | ------ | ------------- |
| master | master | 192.168.68.11 |
| node   | node1  | 192.168.68.12 |
| node   | node2  | 192.168.68.13 |

##### 2.2.4 系统初始化

###### 2.2.4.1 关闭防火墙

```shell
#临时关闭
systemctl stop firewalld

#永久关闭
systemctl disable firewalld
```

###### 2.2.4.2 关闭 selinux

```shell
#临时关闭
setenforce 0

#永久关闭
sed -i 's/enforcing/disabled/' /etc/selinux/config
```

###### 2.2.4.3 关闭swap

```shell
#临时关闭
swapoff -a

#永久关闭
sed -ri 's/.*swap.*/#&/' /etc/fstab 
```

###### 2.2.4.4 设置主机名

```shell
#在192.168.68.11主机上执行以下命令
hostnamectl set-hostname master

#在192.168.68.12主机上执行以下命令
hostnamectl set-hostname node1

#在192.168.68.13主机上执行以下命令
hostnamectl set-hostname node2
```

###### 2.2.4.5 在master中添加hosts

```shell
cat >> /etc/hosts << EOF
192.168.68.11 master
192.168.68.12 node1
192.168.68.13 node2
EOF
```

###### 2.2.4.6 将桥接的IPv4流量传递到iptables的链

```shell
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

#生效命令
sysctl --system
```

###### 2.2.4.7 时间同步

```shell
yum install -y ntpdate
ntpdate -u time.windows.com
```

> **注意**：以上系统初始化涉及到的命令，如无特殊说明，均需在所有主机上执行。

##### 2.2.5 安装Docker、kubeadm、kubelet

Kubernetes默认的容器运行环境为docker，因此要先安装Docker。我这边所有的主机上之前都安装过docker了，所以docker的安装这里就不再进行介绍了。

###### 2.2.5.1 添加阿里云YUM软件源

```shell
#对所有主机执行以下操作命令
cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

###### 2.2.5.2 安装kubeadm，kubelet和kubectl

由于版本更新频繁，所以这里指定版本号进行安装部署：

```shell
#对所有主机执行安装命令
yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0

#对所有主机执行以下命令，让kubelet开机启动
systemctl enable kubelet
```

##### 2.2.6 部署Kubernetes Master

```bash
#在master主机上执行以下命令
kubeadm init --apiserver-advertise-address=192.168.68.11 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.18.0 --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16
```

**命令中的参数含义如下：**

![kubeadm-init](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210304172944.png)    

命令的执行需要一段时间(期间可以重新开个窗口，使用`docker images`查看镜像下载的情况)，当出现如下内容时，说明已经执行成功了。

![image-20210302154448491](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210302154452.png)  

这个图中也有接下来需要我们执行的步骤，这些步骤下面也会说到。

**注意**：执行上面的`kubeadm init`那一长串命令的时候，也有可能会报下面的错：

![image-20210305220911407](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210305221028.png) 

如果真的报上图的错的话，依次执行以下命令后重启主机，然后再次执行`kubeadm init`那一长串命令即可。

```shell
#先使用cat命令查看ip_forward的值是否为0，如果为0就继续执行下面的命令
cat /proc/sys/net/ipv4/ip_forward

#上面的值如果为0的话，这里给改成1即可
echo "1" > /proc/sys/net/ipv4/ip_forward

#重启网络服务
service network restart

#最后为了保险起见，可以使用reboot命令重启主机
reboot
```


> 由于默认拉取镜像的地址(k8s.gcr.io)国内无法访问，所以`kubeadm init`命令后面的image-repository参数的值使用的是阿里云镜像仓库地址。

**使用kubectl工具：**

```shell
#在master节点依次执行以下命令
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

上面的命令执行完成之后，我们也可以通过`kubectl get nodes`命令进行查看，会显示如下内容：

```shell
AME     STATUS     ROLES    AGE   VERSION
master   NotReady   master   11m   v1.18.0
```

##### 2.2.7 加入Kubernetes Node

```shell
#在每个worker node主机上执行以下命令，每个人的这个命令都是不一样的，我们在执行完kubeadm init命令后，会输出下面这个命令，直接复制即可
kubeadm join 192.168.68.11:6443 --token ba295y.k5pqeuidfvqrrx2x \
    --discovery-token-ca-cert-hash sha256:0b7edbb6ebe7adf0a67f6a5ec7b8d8493263440f406628327800d9bafbe6c4f9
```

==注意==：在执行`kubeadm join`命令的时候，如果也出现执行`kubeadm init`命令时候的错误，那就还是按照上面提到的解决方案进行解决即可。

以上命令的token默认有效期为24小时，当过期之后，该token就不可用了。如果需要重新创建token，操作如下：

```shell
#在master节点执行以下命令
kubeadm token create --print-join-command

#我们也可以在master节点执行以下命令来查看token信息
kubeadm token list
```

当我们使用`kubeadm join`命令向集群中加入新节点后，在master主机上再次使用`kubectl get nodes`命令，可以发现节点已经添加成功了，如下所示：

```shell
NAME     STATUS     ROLES    AGE   VERSION
master   NotReady   master   44m   v1.18.0
node1    NotReady   <none>   15m   v1.18.0
node2    NotReady   <none>   14m   v1.18.0
```

##### 2.2.8 部署CNI网络插件

```shell
#我们可以使用如下命令来部署CNI网络插件(以下命令要在master主机执行)
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

如果出现"Unable to connect to the server"错误，那说明该地址已经被墙了，如果不知道怎么科学上网的话，可以使用如下命令获取并执行我翻墙下载的这个配置文件：

```shell
#在master主机上下载压缩包
wget https://files.cnblogs.com/files/gongcqq/kube-flannel.tar.gz

#解压压缩包，以便获取kube-flannel.yml文件
tar -zxvf kube-flannel.tar.gz

#执行kube-flannel.yml文件。如果网络环境不好，可以按照下面"注意事项"中把相关镜像先下载下来，再执行该文件
kubectl apply -f kube-flannel.yml
```

以上命令执行完成之后，再在master主机使用`kubectl get nodes`命令进行查看，发现节点状态都从之前的==NotReady==变成了==Ready==，如下所示：

```shell
NAME     STATUS   ROLES    AGE    VERSION
master   Ready    master   139m   v1.18.0
node1    Ready    <none>   110m   v1.18.0
node2    Ready    <none>   110m   v1.18.0
```

我们也可以在master主机上使用`kubectl get pods -n kube-system`命令查看pod的运行情况，查询结果如下：

![image-20210302174746085](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210302174748.png) 

**注意事项**：上面在使用`kubectl apply -f kube-flannel.yml`命令的时候会拉取一个flannel镜像，如果网络环境不好，可能会导致拉取失败，从而导致集群部署失败。我这边已经将该镜像打成了名为[flannel-v0.13.1-rc2.tar](https://gongsl.lanzous.com/iYJpGmkeiaj)的tar包，我们可以下载下来通过ftp的方式传到集群的各个主机上。假设都传到了主机的"/root"目录下，那么直接使用如下命令还原镜像即可：

```shell
docker load -i /root/flannel-v0.13.1-rc2.tar
```

##### 2.2.9 测试kubernetes集群

在Kubernetes集群中创建一个pod，验证是否正常运行：

```shell
#在master主机上创建并运行一个pod
kubectl create deployment nginx --image=nginx

#暴露端口
kubectl expose deployment nginx --port=80 --type=NodePort
```

以上命令执行完成之后，我们可以通过`kubectl get pod,svc`命令查看pod的运行情况：

![image-20210302181219698](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210302181739.png) 

由上图可知，刚才创建的pod正在运行中，且nginx暴露给外部访问的端口是**30412**，如果在浏览器中使用集群中任何一个主机加这个端口都能访问到nginx的话，就说明集群搭建成功了。我使用集群中的三个主机加上端口分别进行了访问，都是可以访问到nginx的，说明kubernetes集群搭建成功了。

```bash
http://192.168.68.11:30412
http://192.168.68.12:30412
http://192.168.68.13:30412
```

![image-20210302182217942](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210302182224.png) 

##### 2.2.10 部署Dashboard Web页面

**注意**：部署Dashboard Web页面章节涉及的所有命令均是在master节点上执行的。

1. 获取recommended.yaml文件

```shell
#可以使用如下命令进行下载
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.3/aio/deploy/recommended.yaml
```

以上地址如果访问不了的话，可以使用如下方式下载：

```shell
#下载recommended.yaml的压缩包
wget https://files.cnblogs.com/files/gongcqq/recommended.tar.gz

#解压压缩包以获取recommended.yaml文件
tar -zxvf recommended.tar.gz
```

默认Dashboard只能集群内部访问，修改Service为NodePort类型，暴露到外部：

![image-20210306004457104](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210306004502.png) 

> **注意**：如果是通过上面第二个地址中的recommended.tar.gz获取到的recommended.yaml文件的话，就不用再增加NodePort类型了，因为压缩包中的recommended.yaml文件是已经增加过的。

2. 执行recommended.yaml文件

```shell
kubectl apply -f recommended.yaml
```

执行完成后，可以通过`kubectl get pods -n kubernetes-dashboard`命令进行状态查看，下图就是正常状态：

![image-20210306004943314](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210306004947.png) 

3. 创建用户并生成token

```shell
#创建用户
kubectl create serviceaccount dashboard-admin -n kube-system

#用户授权
kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin

#生成用户token
kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk '/dashboard-admin/{print $1}')
```

下图就是用户的token信息：

![image-20210306005317155](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210306005320.png) 

4. 登录Dashboard页面

浏览器输入集群中任一主机ip加30001端口都可访问到Dashboard页面，这里以master主机为例，如下所示：

```http
https://192.168.68.11:30001/
```

进入到登录页面后，需要填写用户token，填写完成后，直接登录即可：

![image-20210306010103826](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210306010108.png) 

下面就是登录成功后的主界面：

![image-20210306010249999](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210306010255.png) 

#### 2.3 kubespray方式搭建k8s集群(多master)

在使用kubespray方式搭建k8s集群前，如果运行时我们选择的是docker的话，需要提前先把docker装好，这里就不再讲解docker的安装过程了。

##### 2.3.1 主机规划

| 角色   | 主机名 | IP            |
| ------ | ------ | ------------- |
| master | k8s-01 | 192.168.68.20 |
| master | k8s-02 | 192.168.68.21 |
| node   | k8s-03 | 192.168.68.22 |

##### 2.3.2 系统初始化

###### 2.3.2.1 修改主机名

```shell
#在192.168.68.20主机执行以下命令
hostnamectl set-hostname k8s-01

#在192.168.68.21主机执行以下命令
hostnamectl set-hostname k8s-02

#在192.168.68.22主机执行以下命令
hostnamectl set-hostname k8s-03
```

###### 2.3.2.2 关闭防火墙、selinux、swap、重置iptables

```shell
# 关闭selinux
setenforce 0 && sed -i '/SELINUX/s/enforcing/disabled/' /etc/selinux/config

# 关闭防火墙
systemctl stop firewalld && systemctl disable firewalld

# 设置iptables规则
iptables -F && iptables -X && iptables -F -t nat && iptables -X -t nat && iptables -P FORWARD ACCEPT

# 关闭swap
swapoff -a && sed -ri 's/.*swap.*/#&/' /etc/fstab && free –h

# 关闭dnsmasq(否则可能导致容器无法解析域名)
service dnsmasq stop && systemctl disable dnsmasq
```

> **说明**：以上命令需要在所有主机执行。

###### 2.3.2.3 k8s参数设置

```shell
# 制作配置文件
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

#生效命令
sysctl --system
```

> **说明**：以上命令需要在所有主机执行。

##### 2.3.3 使用kubespray部署集群

这部分操作可以在集群中的节点上进行，也可以在集群之外的节点上进行，这里为了方便，就在集群的20主机上操作了。

###### 2.3.3.1 配置免密

这里在集群的20主机上进行配置，使该节点可以免密登录到所有节点：

```shell
# 1.生成keygen(直接执行ssh-keygen，然后一路回车下去即可)
ssh-keygen

# 2.查看并复制生成的pubkey
cat /root/.ssh/id_rsa.pub

# 3.分别登陆到每个节点上，将pubkey写入到"/root/.ssh/authorized_keys"中
mkdir -p /root/.ssh && echo "<上一步骤复制的pubkey>" >> /root/.ssh/authorized_keys

# 4.在20主机使用如下命令看看是否可以免密登录到集群中的其他主机
ssh root@<集群中其他主机的IP>
```

###### 2.3.3.2 下载安装依赖软件

```shell
# 安装基础软件
yum install -y epel-release python36 python36-pip git

# 下载kubespray源码
wget https://github.com/kubernetes-sigs/kubespray/archive/v2.15.0.tar.gz

# 解压缩
tar -zxvf v2.15.0.tar.gz && cd kubespray-2.15.0

# 查看requirements中需要安装的软件及其版本
cat requirements.txt

# 安装requirements
pip3.6 install -r requirements.txt

# 如果安装过程遇到问题可以使用如下命令升级pip后再重试
# pip3.6 install --upgrade pip
```

###### 2.3.3.3 生成配置

项目中有一个目录是集群的基础配置，示例配置在目录inventory/sample中，我们复制一份出来作为自己集群的配置。

```shell
# copy一份demo配置，以便自定义
cp -rpf inventory/sample inventory/mycluster
```

由于kubespray给我们准备了py脚本，可以直接根据环境变量自动生成配置文件，所以我们现在只需要设定好环境变量就可以啦。

```shell
# 使用真实的hostname(否则会自动把我们的hostname改成node1/node2...这种)
export USE_REAL_HOSTNAME=true

# 指定配置文件位置
export CONFIG_FILE=inventory/mycluster/hosts.yaml

# 定义ip列表(我们的服务器内网ip地址列表，3台及以上，前两台默认为master节点)
declare -a IPS=(192.168.68.20 192.168.68.21 192.168.68.22)

# 生成配置文件(如果执行有问题的话，可以重复执行一遍)
python3 contrib/inventory_builder/inventory.py ${IPS[@]}
```

###### 2.3.3.4 个性化配置

配置文件都生成好了，虽然可以直接用，但并不一定能完全满足我们的个性化需求，所以我们可以根据自身需要去修改生成的配置文件。

1. 节点组织配置(这里可以调整每个节点的角色)

   ```shell
   vim inventory/mycluster/hosts.yaml
   ```

   ![image-20210725222546242](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726001233.png)

   > **说明**：如果有需要的话，可以在这一步进行节点等信息的修改，我这边就不做修改了。

2. 全局配置

   ```shell
   vim inventory/mycluster/group_vars/all/all.yml
   ```

   ![image-20210725222853635](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726001244.png) 

   > **说明**：如果有需要的话，可以进行修改，比如如果希望配置http(s)代理实现外网访问，就可以把`http_proxy`和`https_proxy`这两项被注释的地方打开，然后配置上可以访问外网的地址即可，我这边就不做修改了。

3. k8s集群配置(包括svc网段、pod网段等)

   ```shell
   vim inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml
   ```

   ![image-20210725224204461](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726001252.png) 

   ![image-20210725224934148](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726001259.png) 

4. etcd配置

   ```shell
   vim inventory/mycluster/group_vars/etcd.yml
   ```

   ![image-20210725225449542](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726001306.png) 

5. 附加组件

   ```shell
   vim inventory/mycluster/group_vars/k8s-cluster/addons.yml
   ```

   ![image-20210725225826717](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726001313.png) 

   ![image-20210725230125241](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726001320.png) 


###### 2.3.3.5 一键部署

配置文件都调整好了后，就可以开始一键部署啦，不过部署过程会非常慢。为了节省时间，这里就把相关的二进制文件和镜像先下载下来。

可以通过网盘把二进制文件下载下来，下面是网盘地址：

> 链接：https://pan.baidu.com/s/1pLGk6Hg2qWZJzgqQZSy1PQ 提取码：b0m5

以上二进制文件下载完成之后，传到集群中的所有主机上，然后通过以下命令解压到指定目录：

```shell
# 我这边是传到/root目录下的
tar -zxvf /root/kubespray-k8s-releases-v2.15.0.tar.gz -C /
```

然后可以通过以下命令提前下载好安装k8s集群所需的镜像：

```shell
#下载镜像相关的脚本文件
wget https://gitee.com/gongcqq/k8s-install/raw/master/kubespray-v2.15.0-images.sh

#执行脚本下载镜像
sh kubespray-v2.15.0-images.sh
```

> **说明**：由于有些镜像是worker node节点也会用到的，所以建议在k8s集群中的所有节点上都执行以上下载并执行镜像脚本的操作。

镜像下载完成之后，接下来就可以进行一键部署了，在20主机的`/root/kubespray-2.15.0`目录下执行如下命令：

```shell
# -vvvv会打印最详细的日志信息，建议开启
ansible-playbook -i inventory/mycluster/hosts.yaml -b cluster.yml -vvvv
```

经过较长时间的等待后，当打印如下信息，且失败数为0的时候，说明k8s集群已经部署成功了：

![image-20210726003145035](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726003149.png) 

###### 2.3.3.6 集群冒烟测试

我们可以使用`kubectl get node`命令查看集群的状态，也可以使用`kubectl get pod -A -o wide`命令查看各组件Pod的运行情况：

![image-20210726003624422](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726003636.png) 

我们也可以使用`kubectl get cs`命令查看etcd等组件的健康状态：

![image-20210726003805364](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726005154.png) 

**说明**：通过上图可知，etcd集群中的三个节点都是健康的，但是controller-manager和scheduler却是不健康的，这主要是由于它们对应的yaml文件中端口默认值为0导致的，把相应的yaml文件中对应的端口注释掉即可。

```shell
vim /etc/kubernetes/manifests/kube-controller-manager.yaml
```

![image-20210726004418162](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726005159.png) 

```shell
vim /etc/kubernetes/manifests/kube-scheduler.yaml
```

![image-20210726004556697](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726005206.png) 

最后使用`systemctl restart kubelet`命令重启一下kubelet即可，再次查看健康状态的时候，它们就健康了。

![image-20210726004814874](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726005213.png) 

> **说明**：由于这里部署的是多master集群，所以要在所有master主机上都执行以上操作，不然其他的master主机上执行`kubectl get cs`命令的时候controller-manager和scheduler还是会显示不健康。

我们也可以使用`cat /etc/hosts`命令在集群中的各个主机上查看一下hosts，发现集群中的主机IP和主机名的对应关系已经被自动写入到了集群中所有主机的hosts文件中了。

![image-20210726005855744](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726010829.png) 

接下来通过DaemonSet创建Pod来验证下集群是否可用：

```shell
# 创建一个nginx-ds.yml文件
cat > nginx-ds.yml <<EOF
apiVersion: v1
kind: Service
metadata:
  name: nginx-ds
  labels:
    app: nginx-ds
spec:
  type: NodePort
  selector:
    app: nginx-ds
  ports:
  - name: http
    port: 80
    targetPort: 80
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-ds
spec:
  selector:
    matchLabels:
      app: nginx-ds
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
      - name: my-nginx
        image: nginx:1.19
        ports:
        - containerPort: 80
EOF

# 基于文件创建对应的资源
kubectl apply -f nginx-ds.yml
```

![image-20210726010313684](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210726010837.png) 

通过上图可知，Pod都已经正常运行了，我们可以在集群内部通过`curl 10.200.173.162`命令访问Pod中的nginx，也可以在浏览器通过`NodePort`暴露的**31126**端口进行外部访问，比如下面这样：

```http
#以下IP地址可以是集群中任何一台宿主机的IP地址
http://192.168.68.20:31126/
```

> **说明**：通过以上步骤已经完成了k8s集群的安装，而且集群冒烟测试也是通过的。目前的集群中，有两台master主机和一台node主机，如果我们想要增加或者减少master主机或者node主机的话，可以参考[集群运维](https://gitee.com/gongcqq/k8s-install/raw/master/kubespray-cluster-operation.md)文档。

#### 2.4 二进制方式搭建k8s集群(多master)

**注意**：由于k8s在未来的版本中，运行时将不再支持docker，而是支持Containerd，所以为了及早适应，这里二进制方式搭建的k8s集群的运行时就使用Containerd了。

##### 2.4.1 安装要求

- 一台或多台机器，操作系统为CentOS7；

- 硬件配置：2GB或更多RAM，2个CPU或更多CPU；

- 集群中所有机器之间网络互通；

- 可以访问外网，因为需要联网拉取镜像；

- 需要禁止swap分区。

##### 2.4.2 主机规划

| 角色   | 主机名 | IP            |
| ------ | ------ | ------------- |
| master | k8s-01 | 192.168.68.20 |
| master | k8s-02 | 192.168.68.21 |
| node   | k8s-03 | 192.168.68.22 |

##### 2.4.3 系统初始化

如无特殊说明，则该部分涉及的命令均需在所有节点执行。

###### 2.4.3.1 修改主机名

```shell
#在192.168.68.20主机执行以下命令
hostnamectl set-hostname k8s-01

#在192.168.68.21主机执行以下命令
hostnamectl set-hostname k8s-02

#在192.168.68.22主机执行以下命令
hostnamectl set-hostname k8s-03
```

###### 2.4.3.2 关闭防火墙、selinux、swap、重置iptables

```shell
# 关闭selinux
setenforce 0 && sed -i '/SELINUX/s/enforcing/disabled/' /etc/selinux/config

# 关闭防火墙
systemctl stop firewalld && systemctl disable firewalld

# 设置iptables规则
iptables -F && iptables -X && iptables -F -t nat && iptables -X -t nat && iptables -P FORWARD ACCEPT

# 关闭swap
swapoff -a && sed -ri 's/.*swap.*/#&/' /etc/fstab && free –h
```

###### 2.4.3.3 k8s参数设置

```shell
# 制作配置文件
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

#生效命令
sysctl --system
```

###### 2.4.3.4 配置hosts

```shell
cat >> /etc/hosts << EOF
192.168.68.20 k8s-01
192.168.68.21 k8s-02
192.168.68.22 k8s-03
EOF
```

###### 2.4.3.5 移除docker

```shell
#如果之前安装过docker的话，可以使用如下命令进行卸载
yum remove -y docker-ce docker-ce-cli containerd.io

#卸载后，删除镜像、容器、配置文件等内容
rm -rf /var/lib/docker
```

##### 2.4.4 配置免密登录

为了节点之间传文件的方便，我这边配置一下免密登录，可以在任意节点进行配置，我这边就在k8s-01节点上配置了。

```shell
# 1.生成keygen(直接执行ssh-keygen，然后一路回车下去即可)
ssh-keygen

# 2.查看并复制生成的pubkey
cat /root/.ssh/id_rsa.pub

# 3.分别登陆到各个节点上，将pubkey写入到"/root/.ssh/authorized_keys"中
mkdir -p /root/.ssh && echo "<上一步骤复制的pubkey>" >> /root/.ssh/authorized_keys

# 4.在k8s-01节点上使用如下命令看看是否可以免密登录到集群中的其他主机
ssh root@<集群中其他主机的IP>
```

##### 2.4.5 准备k8s软件包

为了方便，我这边就先在k8s-01主机上下载好所有的软件包，然后再分发到各个节点上。

我们可以通过[github-kubernetes](https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG)下载对应版本的二进制包，直接点击下图位置的链接即可进行下载：

![image-20210801203640174](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210802083307.png) 

也可以直接通过下面的命令下载需要的二进制文件，我这边就使用这种方式了：

```shell
# 设置版本号
export VERSION=v1.20.2

# 下载master节点组件
wget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-apiserver

wget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-controller-manager

wget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-scheduler

wget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kubectl

# 下载worker节点组件
wget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-proxy

wget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kubelet

# 下载etcd组件
wget https://github.com/etcd-io/etcd/releases/download/v3.4.10/etcd-v3.4.10-linux-amd64.tar.gz

tar -xvf etcd-v3.4.10-linux-amd64.tar.gz
mv etcd-v3.4.10-linux-amd64/etcd* .
rm -rf etcd-v3.4.10-linux-amd64*

# 统一修改文件权限为可执行
chmod +x kube*
```

完成下载后，分发文件到各个节点：

```shell
IPS=(k8s-01 k8s-02 k8s-03)

for instance in ${IPS[@]}; do
  scp etcd* kube* root@${instance}:/usr/local/bin/
done
```

##### 2.4.6 生成证书

如无特殊说明，生成证书的所有操作都会在k8s-01节点上进行。

###### 2.4.6.1 安装cfssl

cfssl是非常好用的CA工具，我们用它来生成证书和秘钥文件，安装过程也很简单，如下：

```shell
# 下载
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/local/bin/cfssl
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson

# 修改为可执行权限
chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson

# 验证
cfssl version
```

###### 2.4.6.2 生成根证书

根证书是集群所有节点共享的，只需要创建一个CA证书，后续创建的所有证书都由它签名。

```shell
# 先在k8s-01节点创建一个单独的证书目录
mkdir /root/pki && cd /root/pki

# 生成根证书配置文件
cat > ca-config.json <<EOF
{
  "signing": {
    "default": {
      "expiry": "876000h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "876000h"
      }
    }
  }
}
EOF

cat > ca-csr.json <<EOF
{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "CA",
      "ST": "Oregon"
    }
  ]
}
EOF

# 生成证书和私钥
cfssl gencert -initca ca-csr.json | cfssljson -bare ca
```

###### 2.4.6.3 生成admin客户端证书

```shell
cat > admin-csr.json <<EOF
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "system:masters",
      "OU": "seven"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin
```

###### 2.4.6.4 生成kubelet客户端证书

Kubernetes使用一种称为Node Authorizer的专用授权模式来授权Kubelets发出的API请求。Kubelet使用将其标识为system:nodes组中的凭据，其用户名为system:node:nodeName，接下里就给每个节点生成证书。

```shell
WORKERS=(k8s-01 k8s-02 k8s-03)
WORKER_IPS=(192.168.68.20 192.168.68.21 192.168.68.22)

for ((i=0;i<${#WORKERS[@]};i++)); do
cat > ${WORKERS[$i]}-csr.json <<EOF
{
  "CN": "system:node:${WORKERS[$i]}",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Beijing",
      "O": "system:nodes",
      "OU": "seven",
      "ST": "Beijing"
    }
  ]
}
EOF
cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${WORKERS[$i]},${WORKER_IPS[$i]} \
  -profile=kubernetes \
  ${WORKERS[$i]}-csr.json | cfssljson -bare ${WORKERS[$i]}
done
```

###### 2.4.6.5 生成kube-controller-manager客户端证书

```shell
cat > kube-controller-manager-csr.json <<EOF
{
    "CN": "system:kube-controller-manager",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "BeiJing",
        "L": "BeiJing",
        "O": "system:kube-controller-manager",
        "OU": "seven"
      }
    ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
```

###### 2.4.6.6 生成kube-proxy客户端证书

```shell
cat > kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "seven"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-proxy-csr.json | cfssljson -bare kube-proxy
```

###### 2.4.6.7 生成kube-scheduler客户端证书

```shell
cat > kube-scheduler-csr.json <<EOF
{
    "CN": "system:kube-scheduler",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "BeiJing",
        "L": "BeiJing",
        "O": "system:kube-scheduler",
        "OU": "seven"
      }
    ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-scheduler-csr.json | cfssljson -bare kube-scheduler
```

###### 2.4.6.8 生成kube-apiserver服务端证书

**kube-apiserver服务端证书配置文件**

```shell
cat > kubernetes-csr.json <<EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "seven"
    }
  ]
}
EOF
```

**生成kube-apiserver服务端证书**

服务端证书与客户端略有不同，客户端需要通过一个名字或者一个ip去访问服务端，所以证书必须要包含客户端所访问的名字或ip，用以客户端验证。

```shell
# apiserver的service ip地址(一般是svc网段的第一个ip)
KUBERNETES_SVC_IP=10.233.0.1
# 下面这个主机列表中，worker节点也要加上
MASTER_IPS=192.168.68.20,192.168.68.21,192.168.68.22
# 生成证书
cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${KUBERNETES_SVC_IP},${MASTER_IPS},127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local \
  -profile=kubernetes \
  kubernetes-csr.json | cfssljson -bare kubernetes
```

###### 2.4.6.9 生成Service Account证书

```shell
cat > service-account-csr.json <<EOF
{
  "CN": "service-accounts",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "seven"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  service-account-csr.json | cfssljson -bare service-account
```

###### 2.4.6.10 生成proxy-client 证书

```shell
cat > proxy-client-csr.json <<EOF
{
  "CN": "aggregator",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "seven"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  proxy-client-csr.json | cfssljson -bare proxy-client
```

###### 2.4.6.11 分发客户端、服务端证书

```shell
# 分发worker节点需要的证书和私钥
for instance in ${WORKERS[@]}; do
  scp ca.pem ${instance}-key.pem ${instance}.pem root@${instance}:~/
done

# 分发master节点需要的证书和私钥
OIFS=$IFS
IFS=','
for instance in ${MASTER_IPS}; do
  scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
    service-account-key.pem service-account.pem proxy-client.pem proxy-client-key.pem root@${instance}:~/
done
IFS=$OIFS
```

##### 2.4.7 kubernetes各组件的认证配置

kubernetes的认证配置文件，也叫kubeconfigs，用于让kubernetes的客户端定位kube-apiserver并通过apiserver的安全认证。

接下来就生成各个组件的kubeconfigs，包括controller-manager，kubelet，kube-proxy，scheduler，以及admin用户。

**说明**：该章节的命令同样在k8s-01节点的`/root/pki`目录下执行。

###### 2.4.7.1 kubelet

```shell
WORKERS="k8s-01 k8s-02 k8s-03"

for instance in ${WORKERS}; do
  kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-credentials system:node:${instance} \
    --client-certificate=${instance}.pem \
    --client-key=${instance}-key.pem \
    --embed-certs=true \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:node:${instance} \
    --kubeconfig=${instance}.kubeconfig

  kubectl config use-context default --kubeconfig=${instance}.kubeconfig
done
```

###### 2.4.7.2 kube-proxy

```shell
kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials system:kube-proxy \
   --client-certificate=kube-proxy.pem \
   --client-key=kube-proxy-key.pem \
   --embed-certs=true \
   --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
   --cluster=kubernetes \
   --user=system:kube-proxy \
   --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```

###### 2.4.7.3 kube-controller-manager

```shell
kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.pem \
  --client-key=kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig
```

###### 2.4.7.4 kube-scheduler

```shell
kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig
```

###### 2.4.7.5 admin用户配置

```shell
kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=admin.kubeconfig

kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem \
  --embed-certs=true \
  --kubeconfig=admin.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes \
  --user=admin \
  --kubeconfig=admin.kubeconfig

kubectl config use-context default --kubeconfig=admin.kubeconfig
```

###### 2.4.7.6 分发配置文件

```shell
# 把kubelet和kube-proxy需要的kubeconfig配置分发到每个worker节点
WORKERS="k8s-01 k8s-02 k8s-03"
for instance in ${WORKERS}; do
    scp ${instance}.kubeconfig kube-proxy.kubeconfig ${instance}:~/
done

# 把kube-controller-manager和kube-scheduler需要的kubeconfig配置分发到master节点
MASTERS="k8s-01 k8s-02"
for instance in ${MASTERS}; do
    scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig ${instance}:~/
done
```

##### 2.4.8 部署ETCD集群

Kubernetes组件是无状态的，并在etcd中存储集群状态。在本小节中，我们将部署三个节点的etcd群集，并对其进行配置以实现高可用性和安全的远程访问。

###### 2.4.8.1 配置etcd

```shell
# 在所有节点上依次执行以下命令
mkdir -p /etc/etcd /var/lib/etcd
chmod 700 /var/lib/etcd
cp /root/ca.pem /root/kubernetes-key.pem /root/kubernetes.pem /etc/etcd/
```

```shell
# 在k8s-01主机上依次执行以下命令
ETCD_NAMES=(k8s-01 k8s-02 k8s-03)
ETCD_IPS=(192.168.68.20 192.168.68.21 192.168.68.22)

for ((i=0;i<${#ETCD_NAMES[@]};i++)); do
cat <<EOF > /root/etcd-${ETCD_NAMES[$i]}.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos
[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAMES[$i]} \\
  --cert-file=/etc/etcd/kubernetes.pem \\
  --key-file=/etc/etcd/kubernetes-key.pem \\
  --peer-cert-file=/etc/etcd/kubernetes.pem \\
  --peer-key-file=/etc/etcd/kubernetes-key.pem \\
  --trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${ETCD_IPS[$i]}:2380 \\
  --listen-peer-urls https://${ETCD_IPS[$i]}:2380 \\
  --listen-client-urls https://${ETCD_IPS[$i]}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${ETCD_IPS[$i]}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster ${ETCD_NAMES[0]}=https://${ETCD_IPS[0]}:2380,${ETCD_NAMES[1]}=https://${ETCD_IPS[1]}:2380,${ETCD_NAMES[2]}=https://${ETCD_IPS[2]}:2380 \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
EOF
done
```

```shell
# 在k8s-01主机上将etcd的service文件分发到各个节点上
for ((i=0;i<${#ETCD_NAMES[@]};i++)); do
scp /root/etcd-${ETCD_NAMES[$i]}.service ${ETCD_IPS[$i]}:/etc/systemd/system/etcd.service
done
```

###### 2.4.8.2 启动etcd集群

所有etcd节点都配置好etcd.service后，开始启动etcd集群：

```shell
# 在所有节点上执行以下命令
systemctl daemon-reload && systemctl enable etcd && systemctl restart etcd
```

###### 2.4.8.3 验证etcd集群

```shell
# 在任一节点上执行以下命令进行验证
ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca.pem \
  --cert=/etc/etcd/kubernetes.pem \
  --key=/etc/etcd/kubernetes-key.pem

# 或者也可以在k8s-01节点执行以下命令进行验证
ETCDCTL_API=3 etcdctl \
--cacert=/etc/etcd/ca.pem \
--cert=/etc/etcd/kubernetes.pem \
--key=/etc/etcd/kubernetes-key.pem \
--endpoints="https://${ETCD_IPS[0]}:2379,https://${ETCD_IPS[1]}:2379,https://${ETCD_IPS[2]}:2379" endpoint health
```

![image-20210801234646283](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210802083325.png) 

##### 2.4.9 部署kubernetes控制平面

**注意**：该章节涉及到的所有命令，都需要在所有master节点上执行，即k8s-01节点和k8s-02节点。

###### 2.4.9.1 配置 API Server

```shell
# 创建kubernetes必要目录
mkdir -p /etc/kubernetes/ssl
# 准备证书文件
mv /root/ca.pem /root/ca-key.pem /root/kubernetes-key.pem /root/kubernetes.pem \
    /root/service-account-key.pem /root/service-account.pem \
    /root/proxy-client.pem /root/proxy-client-key.pem \
    /etc/kubernetes/ssl

# 配置kube-apiserver.service
# 本机内网ip，20主机执行的话就是"IP=192.168.68.20"，21主机执行的话就是"IP=192.168.68.21"
IP=<主机的IP地址>
# apiserver实例数
APISERVER_COUNT=2
# etcd节点
ETCD_ENDPOINTS=(192.168.68.20 192.168.68.21 192.168.68.22)
# 创建apiserver的service
cat <<EOF > /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=${IP} \\
  --allow-privileged=true \\
  --apiserver-count=${APISERVER_COUNT} \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\
  --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \\
  --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \\
  --etcd-servers=https://${ETCD_ENDPOINTS[0]}:2379,https://${ETCD_ENDPOINTS[1]}:2379,https://${ETCD_ENDPOINTS[2]}:2379 \\
  --event-ttl=1h \\
  --kubelet-certificate-authority=/etc/kubernetes/ssl/ca.pem \\
  --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \\
  --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \\
  --service-account-issuer=api \\
  --service-account-key-file=/etc/kubernetes/ssl/service-account.pem \\
  --service-account-signing-key-file=/etc/kubernetes/ssl/service-account-key.pem \\
  --api-audiences=api,vault,factors \\
  --service-cluster-ip-range=10.233.0.0/16 \\
  --service-node-port-range=30000-32767 \\
  --proxy-client-cert-file=/etc/kubernetes/ssl/proxy-client.pem \\
  --proxy-client-key-file=/etc/kubernetes/ssl/proxy-client-key.pem \\
  --runtime-config=api/all=true \\
  --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --requestheader-allowed-names=aggregator \\
  --requestheader-extra-headers-prefix=X-Remote-Extra- \\
  --requestheader-group-headers=X-Remote-Group \\
  --requestheader-username-headers=X-Remote-User \\
  --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\
  --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\
  --v=1
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
EOF
```

###### 2.4.9.2 配置kube-controller-manager

```shell
# 准备kubeconfig配置文件
mv /root/kube-controller-manager.kubeconfig /etc/kubernetes/

# 创建kube-controller-manager.service
cat <<EOF > /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes
[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --bind-address=0.0.0.0 \\
  --cluster-cidr=10.200.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\
  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\
  --cluster-signing-duration=876000h0m0s \\
  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --service-account-private-key-file=/etc/kubernetes/ssl/service-account-key.pem \\
  --service-cluster-ip-range=10.233.0.0/16 \\
  --use-service-account-credentials=true \\
  --v=1
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
EOF
```

###### 2.4.9.3 配置kube-scheduler

```shell
# 准备kubeconfig配置文件
mv /root/kube-scheduler.kubeconfig /etc/kubernetes/

# 创建scheduler的service 文件
cat <<EOF > /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes
[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
  --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
  --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
  --leader-elect=true \\
  --bind-address=0.0.0.0 \\
  --port=0 \\
  --v=1
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
EOF
```

###### 2.4.9.4 启动服务

```shell
systemctl daemon-reload && systemctl enable kube-apiserver
systemctl enable kube-controller-manager && systemctl enable kube-scheduler
systemctl restart kube-apiserver
systemctl restart kube-controller-manager
systemctl restart kube-scheduler

# 服务启动后，可以通过以下命令查看各个组件的监听端口
netstat -ntlp
```

###### 2.4.9.5 配置kubectl

kubectl是用来管理kubernetes集群的客户端工具，前面我们已经下载到了所有的master节点，下面我们来配置这个工具，让它可以使用。如无特殊说明，以下命令要在所有的master节点上执行。

```shell
# 创建kubectl的配置目录
mkdir ~/.kube/

# 把管理员的配置文件移动到kubectl的默认目录
mv ~/admin.kubeconfig ~/.kube/config

# 测试，只要没报错就行，显示没有资源的话，是正常的
kubectl get nodes
```

在执行kubectl exec、run、logs等命令时，apiserver会转发到kubelet。这里定义RBAC规则，授权apiserver调用kubelet API。

```shell
# 随便在一个master节点上执行以下命令即可
kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes
```

##### 2.4.10 部署kubernetes工作节点

**注意**：该章节的所有命令，如无特殊说明，均需在所有节点上执行。这里会在master节点上也部署k8s的kubelet插件以及kube-proxy插件，不然到时候使用`kubectl get node`命令查看节点信息的时候，不会显示master节点的信息。

###### 2.4.10.1 下载容器运行时-Containerd

1. 软件包下载

   ```shell
   # 设定containerd的版本号
   VERSION=1.4.3
   # 下载压缩包
   wget https://github.com/containerd/containerd/releases/download/v${VERSION}/cri-containerd-cni-${VERSION}-linux-amd64.tar.gz
   ```

2. 整理压缩文件

   下载后的文件是一个tar.gz，是一个allinone的包，包括了runc、circtl、ctr、containerd等容器运行时以及cni相关的文件，下面将它们解压缩到一个独立的目录中。

   ```shell
   # 解压缩
   tar -xvf cri-containerd-cni-${VERSION}-linux-amd64.tar.gz
   # 复制需要的文件
   cp etc/crictl.yaml /etc/
   cp etc/systemd/system/containerd.service /etc/systemd/system/
   cp -r usr /
   ```

3. containerd配置文件

   ```shell
   # 创建需要的目录
   mkdir -p /etc/containerd
   # 默认配置生成配置文件
   containerd config default > /etc/containerd/config.toml
   # 定制化配置(可选)
   vim /etc/containerd/config.toml
   ```

4. 启动containerd

   ```shell
   # 启动并设置开机启动
   systemctl enable containerd && systemctl restart containerd
   # 检查状态
   systemctl status containerd
   ```

###### 2.4.10.2 配置kubelet

1. 准备kubelet配置

   ```shell
   mkdir -p /etc/kubernetes/ssl/
   
   # 如果是master节点的话，下面的ca.pem和ca-key.pem就不用移了，因为之前已经移到指定目录了
   mv /root/${HOSTNAME}-key.pem /root/${HOSTNAME}.pem /root/ca.pem /root/ca-key.pem /etc/kubernetes/ssl/
   
   mv /root/${HOSTNAME}.kubeconfig /etc/kubernetes/kubeconfig
   
   # 下面的命令要替换成所在主机的IP地址
   IP=<主机的IP地址>
   
   # 写入kubelet配置文件
   cat <<EOF > /etc/kubernetes/kubelet-config.yaml
   kind: KubeletConfiguration
   apiVersion: kubelet.config.k8s.io/v1beta1
   authentication:
     anonymous:
       enabled: false
     webhook:
       enabled: true
     x509:
       clientCAFile: "/etc/kubernetes/ssl/ca.pem"
   authorization:
     mode: Webhook
   clusterDomain: "cluster.local"
   clusterDNS:
     - "169.254.25.10"
   podCIDR: "10.200.0.0/16"
   address: ${IP}
   readOnlyPort: 0
   staticPodPath: /etc/kubernetes/manifests
   healthzPort: 10248
   healthzBindAddress: 127.0.0.1
   kubeletCgroups: /systemd/system.slice
   resolvConf: "/etc/resolv.conf"
   runtimeRequestTimeout: "15m"
   kubeReserved:
     cpu: 200m
     memory: 512M
   tlsCertFile: "/etc/kubernetes/ssl/${HOSTNAME}.pem"
   tlsPrivateKeyFile: "/etc/kubernetes/ssl/${HOSTNAME}-key.pem"
   EOF
   ```

2. 配置kubelet服务

   ```shell
   cat <<EOF > /etc/systemd/system/kubelet.service
   [Unit]
   Description=Kubernetes Kubelet
   Documentation=https://github.com/kubernetes/kubernetes
   After=containerd.service
   Requires=containerd.service
   [Service]
   ExecStart=/usr/local/bin/kubelet \\
     --config=/etc/kubernetes/kubelet-config.yaml \\
     --container-runtime=remote \\
     --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
     --image-pull-progress-deadline=2m \\
     --kubeconfig=/etc/kubernetes/kubeconfig \\
     --network-plugin=cni \\
     --node-ip=${IP} \\
     --register-node=true \\
     --v=2
   Restart=on-failure
   RestartSec=5
   [Install]
   WantedBy=multi-user.target
   EOF
   ```

###### 2.4.10.3 配置nginx-proxy

nginx-proxy是一个用于worker节点访问apiserver的一个代理，是apiserver一个优雅的高可用方案，它使用kubelet的staticpod方式启动，让每个节点都可以均衡的访问到每个apiserver服务，优雅的替代了通过虚拟ip访问apiserver的方式。

> **注意**：nginx-proxy只需要在没有apiserver的节点部署，即部署在k8s-03节点上即可，所以这部分涉及到的命令只需要在k8s-03节点上执行即可。

```shell
# 创建所需的目录
mkdir -p /etc/nginx && mkdir -p /etc/kubernetes/manifests/

# master ip列表
MASTER_IPS=(192.168.68.20 192.168.68.21)

# 创建配置文件
cat <<EOF > /etc/nginx/nginx.conf
error_log stderr notice;

worker_processes 2;
worker_rlimit_nofile 130048;
worker_shutdown_timeout 10s;

events {
  multi_accept on;
  use epoll;
  worker_connections 16384;
}

stream {
  upstream kube_apiserver {
    least_conn;
    server ${MASTER_IPS[0]}:6443;
    server ${MASTER_IPS[1]}:6443;
  }

  server {
    listen        127.0.0.1:6443;
    proxy_pass    kube_apiserver;
    proxy_timeout 10m;
    proxy_connect_timeout 1s;
  }
}

http {
  aio threads;
  aio_write on;
  tcp_nopush on;
  tcp_nodelay on;

  keepalive_timeout 5m;
  keepalive_requests 100;
  reset_timedout_connection on;
  server_tokens off;
  autoindex off;

  server {
    listen 8081;
    location /healthz {
      access_log off;
      return 200;
    }
    location /stub_status {
      stub_status on;
      access_log off;
    }
  }
}
EOF

# 创建yaml文件
cat <<EOF > /etc/kubernetes/manifests/nginx-proxy.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-proxy
  namespace: kube-system
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: kube-nginx
spec:
  hostNetwork: true
  dnsPolicy: ClusterFirstWithHostNet
  nodeSelector:
    kubernetes.io/os: linux
  priorityClassName: system-node-critical
  containers:
  - name: nginx-proxy
    image: docker.io/library/nginx:1.19
    imagePullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 25m
        memory: 32M
    securityContext:
      privileged: true
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8081
    readinessProbe:
      httpGet:
        path: /healthz
        port: 8081
    volumeMounts:
    - mountPath: /etc/nginx
      name: etc-nginx
      readOnly: true
  volumes:
  - name: etc-nginx
    hostPath:
      path: /etc/nginx
EOF
```

###### 2.4.10.4 配置kube-proxy

```shell
mv /root/kube-proxy.kubeconfig /etc/kubernetes/

# 创建kube-proxy-config.yaml
cat <<EOF > /etc/kubernetes/kube-proxy-config.yaml
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
bindAddress: 0.0.0.0
clientConnection:
  kubeconfig: "/etc/kubernetes/kube-proxy.kubeconfig"
clusterCIDR: "10.200.0.0/16"
mode: ipvs
EOF

#创建kube-proxy.service
cat <<EOF > /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes
[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/etc/kubernetes/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
EOF
```

###### 2.4.10.5 启动服务

```shell
systemctl daemon-reload && systemctl enable kubelet kube-proxy
systemctl restart kubelet kube-proxy

# 查看日志
journalctl -f -u kubelet
journalctl -f -u kube-proxy
```

###### 2.4.10.6 手动下载镜像

```shell
# 在所有节点下载pause镜像
crictl pull registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause:3.2
ctr -n k8s.io i tag registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause:3.2 k8s.gcr.io/pause:3.2
```

##### 2.4.11 部署网络插件-Calico

在k8s-01节点上下载网络插件并部署：

```shell
# 下载
wget https://gitee.com/gongcqq/k8s-install/raw/master/calico.yaml
# 部署
kubectl apply -f calico.yaml
```

##### 2.4.12 部署DNS插件-CoreDNS

在k8s-01节点上下载DNS插件并部署。

###### 2.4.12.1 部署CoreDNS

```shell
# 设置 coredns 的 cluster-ip
COREDNS_CLUSTER_IP=10.233.0.10
# 下载coredns配置
wget https://gitee.com/gongcqq/k8s-install/raw/master/coredns.yaml
# 替换cluster-ip
sed -i "s/\${COREDNS_CLUSTER_IP}/${COREDNS_CLUSTER_IP}/g" coredns.yaml
# 创建coredns
kubectl apply -f coredns.yaml
```

###### 2.4.12.2 部署NodeLocal DNSCache

```shell
# 设置 coredns 的 cluster-ip
COREDNS_CLUSTER_IP=10.233.0.10
# 下载nodelocaldns配置
wget https://gitee.com/gongcqq/k8s-install/raw/master/nodelocaldns.yaml
# 替换cluster-ip
sed -i "s/\${COREDNS_CLUSTER_IP}/${COREDNS_CLUSTER_IP}/g" nodelocaldns.yaml
# 创建 nodelocaldns
kubectl apply -f nodelocaldns.yaml
```

![image-20210802081019992](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210802083337.png) 

**附：官方文档地址**

coredns官方文档：https://coredns.io/plugins/kubernetes/
NodeLocal DNSCache：https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/

##### 2.4.13 集群冒烟测试

1. 创建pod

   ```shell
   # 写入配置
   cat > pod-nginx.yaml <<EOF
   apiVersion: v1
   kind: Service
   metadata:
     name: nginx
     labels:
       app: nginx
   spec:
     type: NodePort
     selector:
       app: nginx
     ports:
     - name: http
       port: 80
       targetPort: 80
   ---
   apiVersion: v1
   kind: Pod
   metadata:
     name: nginx
     labels:
       app: nginx
   spec:
     containers:
     - name: nginx
       image: docker.io/library/nginx:1.19
       ports:
       - containerPort: 80
   EOF
   
   # 创建pod
   kubectl apply -f pod-nginx.yaml
   ```

   ![image-20210802082545155](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210802083343.png) 

   > **说明**：我们可以使用`curl 10.200.61.194`命令看是否能curl通，上面也通过NodePort对外暴露了一个可以访问的端口，我们可以在浏览器通过`主机IP:端口`的方式进行访问，比如：`http://192.168.68.21:31694/`。

2. 检查dns可用性

   ```shell
   # 进入pod中
   kubectl exec nginx -it -- bash
   
   # 查看dns配置
   cat /etc/resolv.conf
   
   # 查看名字是否可以解析
   curl nginx
   ```

##### 2.4.14 其他相关说明

使用二进制安装的k8s集群，当我们使用`kubectl get node`命令列出主机信息的时候，`ROLES`字段是没有展示对应主机的角色信息的。如果有需要，我们可以通过给节点打标签的方式展示出来：

```shell
# 给两台master节点主机打标签
kubectl label node k8s-01 node-role.kubernetes.io/master=
kubectl label node k8s-02 node-role.kubernetes.io/master=

# 给worker节点主机打标签
kubectl label node k8s-03 node-role.kubernetes.io/worker=
```

![image-20210802112718462](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210802114551.png) 

另外，如果我们是使用kubeadm安装的k8s集群的话，安装成功后，master节点默认会被打上污点，这样业务相关的Pod就不会被调度到master节点上了，从而减轻master节点的压力。使用二进制方式安装k8s的话默认是不会给master节点打污点的，如果我们有需要，也可以通过以下命令给master节点打上污点：

```shell
# 分别给两台master节点打污点
kubectl taint node k8s-01 node-role.kubernetes.io/master:NoSchedule
kubectl taint node k8s-02 node-role.kubernetes.io/master:NoSchedule
```

![image-20210802114527350](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210802114558.png) 

### 3.k8s的命令行工具kubectl

kubectl是Kubernetes集群的命令行工具，通过kubectl能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。

#### 3.1 基本语法

```shell
kubectl [command] [TYPE] [NAME] [flags]
```

- `command`：指定要对一个或多个资源执行的操作，例如 ==create==、==get==、==describe==、==delete==。

- `TYPE`：指定资源类型，资源类型不区分大小写，可以指定单数、复数或缩写形式。例如，以下命令输出的结果相同:

```shell
kubectl get pod pod1
kubectl get pods pod1
kubectl get po pod1
```

- `NAME`：指定资源的名称，名称区分大小写。如果省略名称，可以使用`kubectl get pods`显示所有资源的详细信息。
- `flags`：指定可选的参数。例如，可以使用`-s`或`-server`参数指定Kubernetes API服务器的地址和端口。

#### 3.2 kubectl子命令使用分类

##### 基础命令：

![image-20210314173053517](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210314173506.png) 

##### 部署和集群管理命令：

![image-20210314173233468](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210314173515.png) 

##### 故障诊断和调试命令：

![image-20210314173354442](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210314173524.png) 

##### 其他命令：

![image-20210314173434052](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210314173542.png) 

### 4.资源清单详解

k8s集群中对资源管理和资源对象编排部署都可以通过声明样式(YAML)文件来解决，也就是可以把需要对资源对象的操作编辑到YAML格式文件中，我们把这种文件叫做资源清单文件，通过kubectl命令直接操作资源清单文件就可以实现对大量的资源对象进行编排部署。

在k8s中，一般使用YAML格式的文件来创建符合我们预期期望的pod,这样的YAML文件称为资源清单。

#### 4.1 资源清单中的常用字段

##### 4.1.1 必须存在的属性

| 参数名                  | 字段类型 | 说明                                                         |
| ----------------------- | -------- | ------------------------------------------------------------ |
| version                 | String   | K8S API的版本，目前基本是v1，可以使用`kubectl api-versions`命令进行查询 |
| kind                    | String   | 这里指的是yaml文件定义的资源类型和角色，比如：Deployment     |
| metadata                | Object   | 元数据对象，固定值，不可更改                                 |
| metadata.name           | String   | 元数据对象的名称，由我们自定义                               |
| metadata.namespace      | String   | 元数据对象的命名空间，由我们自定义，默认值是default          |
| spec                    | Object   | 详细定义对象，固定值，不可更改                               |
| spec.containers[]       | list     | 这里是spec对象的容器列表定义，是个列表                       |
| spec.containers[].name  | String   | 这里定义容器的名称                                           |
| spec.containers[].image | String   | 这里定义要用到的镜像的名称                                   |

##### 4.1.2 spec的主要属性

| 参数名                                      | 字段类型 | 说明                                                         |
| :------------------------------------------ | :------- | :----------------------------------------------------------- |
| spec.containers[].name                      | String   | 用于定义容器的名称                                           |
| spec.containers[].image                     | String   | 用于定义要用到的镜像的名称                                   |
| spec.containers[].imagePullPolicy           | String   | 定义镜像的拉取策略，有`Always`，`Never`，`IfNotPresent`三种策略可选。Always意思是每次都尝试重新拉取镜像；Never指的是仅使用本地镜像；IfNotPresent指的是如果本地有镜像就使用本地镜像，没有就拉取在线镜像。如果没有设置策略的话，默认使用的是Always。 |
| spec.containers[].command[]                 | List     | 指定容器的启动命令，可以指定多个。不指定则使用镜像打包时使用的启动命令。 |
| spec.containers[].args[]                    | List     | 指定容器的启动命令参数，可以指定多个                         |
| spec.containers[].workingDir                | String   | 指定容器的工作目录                                           |
| spec.containers[].volumeMounts[]            | List     | 指定容器内部的存储卷配置                                     |
| spec.containers[].volumeMounts[].name       | String   | 指定可以被容器挂载的存储卷的名称                             |
| spec.containers[].volumeMounts[].mountPath  | String   | 指定可以被容器挂载的存储卷的路径                             |
| spec.containers[].volumeMounts[].readOnly   | String   | 设置存储卷路径读写的模式，true或者false，默认为读写模式。    |
| spec.containers[].ports[]                   | List     | 指定容器需要用到的端口列表                                   |
| spec.containers[].ports[].name              | String   | 指定端口名称                                                 |
| spec.containers[].ports[].containerPort     | String   | 指定容器需要监听的端口号                                     |
| spec.containers[].ports[].hostPost          | String   | 指定容器所在主机需要监听的端口号，默认跟上面containerPort相同。需要注意，设置了hostPost同一台主机无法启动该容器的相同副本(因为主机的端口号不能相同，这样会冲突)。 |
| spec.containers[].ports[].protocol          | String   | 指定端口协议，支持TCP和UDP，默认值为TCP                      |
| spec.containers[].env[]                     | List     | 指定容器运行时需要设置的环境变量列表                         |
| spec.containers[].env[].name                | String   | 指定环境变量名称                                             |
| spec.containers[].env[].value               | String   | 指定环境变量值                                               |
| spec.containers[].resources                 | Object   | 指定资源限制和资源请求的值                                   |
| spec.containers[].resources.limits          | Object   | 指定设置容器运行时资源的运行上限                             |
| spec.containers[].resources.limits.cpu      | String   | 指定CPU的限制                                                |
| spec.containers[].resources.limits.memory   | String   | 指定MEM内存的限制                                            |
| spec.containers[].resources.requests        | Object   | 指定容器启动和调度室的限制设置                               |
| spec.containers[].resources.requests.cpu    | String   | CPU请求，容器启动时初始化的可用数量                          |
| spec.containers[].resources.requests.memory | String   | 内存请求，容器启动时初始化的可用数量                         |
| spec.restartPolicy                          | String   | 定义Pod重启策略，可以选择值为`Always`、`OnFailure`、`Never`，默认值为Always。使用Always表示Pod一旦终止运行，无论容器是如何终止的，kubelet服务都将重启它；OnFailure表示只有Pod以非零退出码终止时，kubelet才会重启该容器，如果容器是正常结束(退出码为0)，则kubelet将不会重启它；Never表示Pod终止后，kubelet会将退出码报告给Master，但不会重启该Pod。 |
| spec.nodeSelector                           | Object   | 定义Node的Label过滤标签，以key:value格式指定。               |
| spec.imagePullSecrets                       | Object   | 定义pull镜像时使用secret名称，以name:secretkey格式指定。     |

##### 4.1.3 kubectl explain命令

上面只是列举了资源清单中的部分属性，如果我们想要获取更全的属性的话，可以使用`kubectl explain`命令。

以pod为例，我们想要知道pod对应的yaml文件中包含哪些属性的话，可以使用`kubectl explain pod`命令，如下所示：

![image-20210507195359748](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210507195415.png) 

上图展示的属性都是第一级别的根属性，比如我们想要知道上面spec属性下的containers属性下的volumeMounts属性下包含哪些属性，使用`kubectl explain pod.spec.containers.volumeMounts`命令即可，如下所示：

![image-20210507200700400](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210507200705.png) 

如果我们还想查看其他属性，也都可以使用类似方式。如果不想查看pod下的属性，比如想查看deployment下有哪些属性，那就使用`kubectl explain deployment`命令即可。

#### 4.2 举例说明

##### 4.2.1 查看已有资源的资源详情

以Deployment的资源清单为例，命令如下：

```shell
#查看有哪些Deployment资源
kubectl get deploy -o wide

#根据名称获取资源清单详情，下面的"harbor-test"就是Deployment的名称
kubectl get deploy harbor-test -o yaml
```

获取到的资源清单详情如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2020-09-29T08:57:26Z"
  generation: 1
  name: harbor-test
  namespace: default
  resourceVersion: "56211880"
  selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/harbor-test
  uid: 38711c5c-b616-4cb8-906f-24b8cb7a5729
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: harbor
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: harbor
      name: harbor-test
    spec:
      containers:
      - image: ai/pushgateway:v0.8.0
        imagePullPolicy: IfNotPresent
        name: harbor-test
        ports:
        - containerPort: 9090
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2020-09-29T08:57:26Z"
    lastUpdateTime: "2020-09-29T08:57:31Z"
    message: ReplicaSet "harbor-test-7864497b8f" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  - lastTransitionTime: "2021-04-08T02:00:10Z"
    lastUpdateTime: "2021-04-08T02:00:10Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
```

##### 4.2.2 查看新建资源的资源详情

如果之前并没有已经存在的资源，我们也可以新建一个资源，并查看其资源清单详情。

以Deployment的资源清单为例，命令如下：

```shell
#该命令意思就是新建一个名为"gongsl"的Deployment资源
kubectl create deploy gongsl --image=nginx:1.17 -o yaml --dry-run=client
```

获取到的资源清单详情如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: gongsl
  name: gongsl
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gongsl
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: gongsl
    spec:
      containers:
      - image: nginx:1.17
        name: nginx
        resources: {}
status: {}
```

**注意事项：**

1. 上面新建资源使用的命令后面加上的`--dry-run=client`表示尝试运行，也就是说，加上这个后，并不会真的新建一个Deployment资源，只是模拟一下这个效果，如果想要真的新建一个Deployment资源的话，去掉它即可；
2. 低版本的k8s中，想要尝试运行的话，命令后面加的并不是`--dry-run=client`，而是`--dry-run`。

### 5.k8s核心技术-Pod

#### 5.1 Pod的基本概念

1. Pod是k8s系统中可以创建和管理的最小单元；
2. 每个Pod都是应用的一个实例，有独立的IP地址；
3. k8s直接处理或调度的是Pod而不是容器，Pod是由一个或多个容器组成；
4. 一个Pod中的多个容器彼此间共享网络和存储资源；
5. 每个Pod中都有一个被称为"根容器"的Pause容器，Pause容器对应的镜像属于k8s平台的一部分。

#### 5.2 Pod存在的意义

1. 我们可以使用docker创建容器，但是docker是单进程设计，而pod是多进程设计，可以运行多个应用程序；
2. 彼此间频繁调用的多个应用，可以放在同一个pod中，这样可以通过localhost或者socket方式调用，更加方便。

#### 5.3 Pod的生命周期

![Pod生命周期](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210508130811.jpg)  

**生命周期图解：**

1. Pod一旦被创建，首先会创建`Pause`根容器，根容器一旦被创建，即使Pod中的应用容器创建失败，根容器也会继续运行，只有Pod被删除后，根容器才会随之消失；
2. 根容器创建完成后，如果存在Init容器，这时就会启动Init容器，上图中的`Init C`就是指Init容器；
3. `Init C`可以有一个，可以有多个，也可以没有。`Init C`在初始化完成之后就会死亡，如果有多个`Init C`的话，必须等前一个初始化完成后才能执行下一个。只有当所有的`Init C`全部执行完成后，才会去运行主容器；
4. 初始化操作完成之后就会进行主容器的操作，上面的`container`就是指主容器，主容器也可以有多个；
5. 在主容器的运行过程中，在开始和结束的时候可以分别进行一个`START`和`STOP`的操作。也就是说，我们可以在主容器运行前通过`START`操作来执行一些指令，也可以在主容器退出的时候通过`STOP`操作来执行一些指令；
6. 上面的`readiness`指的是就绪检测，`Liveness`指的是生存检测；
7. 我们并不需要在容器刚运行的时候就进行就绪检测，我们可以根据自身需要决定什么时候进行就绪检测，比如我们可以在容器运行五秒后再进行就绪检测等。同理，生存检测也是一样的，我们可以自定义什么时候进行生存检测；
8. 在Pod被拉起后，Pod中的容器里面的服务有可能还没有加载好，还不能被外网访问，这时候我们可以通过命令、TCP连接、HTTP协议获取状态等就绪检测方式进行检测，检测到服务已经加载好后再把Pod的状态改为运行状态，这样就可以保证外网的正常访问；
9. 如果存在就绪检测，那么在就绪检测没有执行成功前，Pod的状态是不会变成运行状态的；
10. 在Pod的运行过程中，有可能会存在进程还在运行，但是已经无法正常对外提供服务的情况。由于进程还在，所以主容器就会继续运行，主容器运行，Pod就还是运行状态，这种情况肯定是有问题的。这时候就可以通过生存检测的方式对Pod中的容器进行检测，当检测到容器内部已经不能正常对外提供访问的时候，我们可以通过重启或者重建Pod等操作使其恢复正常。

#### 5.4 关于Init容器

##### 5.4.1 Init容器的特点

1. Init容器总是运行到完成，在Pod的生命周期中，Init容器不会一直处于运行状态；
2. 如果有多个Init容器，这些Init容器会按照顺序逐个运行，前一个Init容器必须运行成功，下一个才能运行；
3. 只有Init容器都运行完成之后，才会运行我们自己的应用容器；
4. 如果Pod重启，Pod中的所有Init容器都会重新执行；
5. 更改Init容器的`image`字段，等同于重启该Pod；
6. 在Pod中的每个应用容器和Init容器的名称必须唯一，与任何其它容器重名，都会在校验时抛出错误。

##### 5.4.2 Init容器使用案例

下面例子定义的Pod中具有2个Init容器。第一个等待名为`myservice`的Service的启动，第二个则是等待名为`mydb`的Service的启动。只有这两个Init容器都启动完成，Pod才会启动应用容器。

**通过myapp.yaml文件创建Pod：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]
```

**创建Pod的命令：**

```bash
kubectl apply -f myapp.yaml
```

**检查Pod的运行状态：**

```shell
kubectl get -f myapp.yaml -o wide
```

![image-20210508153927669](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210508154015.png) 

由上图状态可知，两个Init容器都没有启动成功。

**查看名为init-myservice的Init容器的日志：**

```shell
kubectl logs myapp-pod -c init-myservice
```

![image-20210508154628311](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210508155201.png) 

由上图可知，首先启动的名为`init-myservice`的Init容器一直在等待名为`myservice`的Service的启动。

**查看名为init-mydb的Init容器的日志：**

```shell
kubectl logs myapp-pod -c init-mydb
```

![image-20210508154920923](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210508154923.png) 

由上图可知，前一个Init容器还没启动成功，所以名为`init-mydb`的Init容器一直处于等待中。

**通过services.yam文件创建Service：**

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
---
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9377
```

**创建Service的命令：**

```shell
kubectl create -f services.yaml
```

**查看Service是否创建成功：**

```shell
kubectl get svc -o wide
```

![image-20210508160211945](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210508160216.png) 

当所需的Service创建完成后，再次查看Pod的状态，这时Pod就已经变成运行状态了，如下所示：

![image-20210508160416890](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210508160445.png) 

#### 5.5 Pod的健康检查-探针

##### 5.5.1 探针的类型

探针是由kubelet对容器进行的定期诊断。要执行诊断，需要kubelet调用由容器实现的Handler(处理程序)。有三种类型的处理程序：

- `ExecAction`：在容器内执行指定命令。如果命令退出时返回码为0则认为诊断成功；
- `TCPSocketAction`：对容器的IP地址上的指定端口执行TCP检查。如果端口打开，则诊断被认为是成功的；
- `HTTPGetAction`：对容器的IP地址上指定端口和路径执行HTTP Get请求。如果响应的状态码大于等于200且小于 400，则诊断被认为是成功的。

每次探测都将获得以下三种结果之一：

- `Success`：容器通过了诊断；
- `Failure`：容器未通过诊断；
- `Unknown`：诊断失败，因此不会采取任何行动。

针对运行中的容器，`kubelet`可以选择是否执行以下三种探针，以及如何针对探测结果作出反应：

- `livenessProbe`：指示容器是否正在运行。如果存活态探测失败，则kubelet会杀死容器，并且容器将根据其重启策略决定下一步操作。如果容器不提供存活探针，则默认状态为`Success`；
- `readinessProbe`：指示容器是否准备好为请求提供服务。如果就绪态探测失败，端点控制器将从与Pod匹配的所有服务的端点列表中删除该Pod的IP地址。初始延迟之前的就绪态的状态值默认为`Failure`。如果容器不提供就绪态探针，则默认状态为`Success`；
- `startupProbe`: 指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被 禁用，直到此探针成功为止。如果启动探测失败，`kubelet`将杀死容器，并且容器将根据其重启策略决定下一步操作。 如果容器没有提供启动探测，则默认状态为`Success`。

##### 5.5.2 探针的使用场景

###### 5.5.2.1 何时使用livenessProbe(存活探针)：

- 如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃，则不一定需要存活态探针；`kubelet`将根据Pod的重启策略自动执行修复操作；
- 如果你希望容器在探测失败时被杀死并重新启动，那么需要指定一个存活态探针，并指定重启策略`restartPolicy`为`Always`或`OnFailure`。

###### 5.5.2.2 何时使用readinessProbe(就绪探针)：

- 如果要仅在探测成功时才开始向Pod发送请求流量，请指定就绪态探针。在这种情况下，就绪态探针可能与存活态探针相同，但是规约中的就绪态探针的存在意味着Pod将在启动阶段不接收任何数据，并且只有在探针探测成功后才开始接收数据；
- 如果我们的容器需要加载大规模的数据、配置文件或者在启动期间执行迁移操作，可以添加一个就绪态探针；
- 如果我们希望容器能够自行进入维护状态，也可以指定一个就绪态探针，它检查与存活探测不同的特定于就绪的端点。

###### 5.5.2.3 何时使用startupProbe(启动探针)：

- 对于所包含的容器需要较长时间才能启动就绪的Pod而言，启动探针是有用的。你不再需要配置一个较长的存活态探测时间间隔，只需要设置另一个单独的配置，对启动期间的容器执行探测，从而允许使用远远超出存活态时间间隔所允许的时长；
- 如果我们的容器启动时间超出 `initialDelaySeconds + failureThreshold × periodSeconds` 总值，我们应该设置一个启动探测，对存活态探针所使用的同一端点执行检查。 `periodSeconds` 的默认值是 10 秒。你应该将其 `failureThreshold` 设置得足够高，以便容器有充足的时间完成启动，并且避免更改存活态探针所使用的默认值。这一设置有助于减少死锁状况的发生。

##### 5.5.3 就绪探针的使用

上面提到可以通过==exec==、==httpGet==、==tcpSocket==这三种方式对容器进行诊断，而无论是就绪探针还是存活探针，都是支持这三种方式的，下面就针对这三种方式演示就绪探针的使用。

###### 5.5.3.1 使用exec进行就绪检测

新增一个exec.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    readinessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 1
      periodSeconds: 3
```

通过yaml文件创建Pod:

```shell
kubectl apply -f exec.yaml
```

查看Pod的状态：

```shell
kubectl get pod -o wide
```

![image-20210509175823128](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509175825.png) 

使用`kubectl describe`命令查看Pod的详细信息：

![image-20210509180730845](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509180733.png) 

进入容器中新增就绪检测需要的文件：

```shell
kubectl exec pod-nginx -it -- bash
或
kubectl exec pod-nginx -it -- sh
```

![image-20210509181959874](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509182005.png) 

**使用探针时，有一些常用的属性，下面介绍一下它们各自的含义：**

- `initialDelaySeconds`：指容器启动后要等待多少秒后就绪探针和存活探针才被初始化，默认值和最小值都是0；
- `periodSeconds`：指的是执行探测的时间间隔(单位是秒)，默认是10秒，最小值是1；
- `timeoutSeconds`：指探测超时后等待多少秒，默认值是1秒，最小值是1；
- `successThreshold`：指探测器在失败后，被视为成功的最小连续成功数，默认值是1。存活探测和启动探测的这个值必须是1，最小值是1；
- `failureThreshold`：指当探测失败后，Kubernetes的重试次数。存活探测情况下的放弃意味着重新启动容器，就绪探测情况下的放弃，Pod会被打上未就绪的标签，默认值是3，最小值是1。

###### 5.5.3.2 使用httpGet进行就绪检测

新增一个httpGet.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    readinessProbe:
      httpGet:
        port: 80
        path: /test.html
      initialDelaySeconds: 1
      periodSeconds: 3
```

通过yaml文件创建Pod:

```shell
kubectl apply -f httpGet.yaml
```

查看Pod的状态：

```shell
kubectl get pod -o wide
```

![image-20210509183945663](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509183948.png) 

使用`kubectl logs`命令查看Pod日志：

![image-20210509184450785](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509184554.png) 

进入容器中新增就绪检测需要的文件：

```shell
kubectl exec pod-nginx -it -- bash
```

![image-20210509190005862](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509190014.png) 

###### 5.5.3.3 使用tcpSocket进行就绪检测

新增一个tcpSocket.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    readinessProbe:
      tcpSocket:
        port: 81
      initialDelaySeconds: 1
      periodSeconds: 3
```

通过yaml文件创建Pod:

```shell
kubectl apply -f tcpSocket.yaml
```

查看Pod的状态：

```shell
kubectl get pod -o wide
```

![image-20210509190647617](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509190825.png) 

使用`kubectl describe`命令查看Pod的详细信息：

![image-20210509191049164](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509191058.png) 

默认情况下nginx是使用80端口运行的，nginx容器中并没有使用81端口运行的应用程序，所以就出现了上图中的错误。

使用`kubectl delete pod pod-nginx`命令删除该Pod，然后将yaml文件改成下面的样子后重新创建Pod：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    readinessProbe:
      tcpSocket:
        port: 80  #只是将这里的端口改成了80，其他地方都没有变动
      initialDelaySeconds: 1
      periodSeconds: 3
```

![image-20210509192218822](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509192222.png) 

##### 5.5.4 存活探针的使用

存活探针和就绪探针一样，同样是有==exec==、==httpGet==、==tcpSocket==这三种探测方式，下面就依次介绍一下。

###### 5.5.4.1 使用exec进行存活检测

新增一个exec-liveness.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: exec-liveness
spec:
  containers:
  - name: liveness
    image: busybox
    imagePullPolicy: IfNotPresent
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
```

**注意**：如果通过以上配置文件创建了Pod，那么kubelet会在容器内执行命令`cat /tmp/healthy`来进行探测，如果命令执行成功并且返回值为0，kubelet就会认为这个容器是健康存活的；如果这个命令返回非0值，kubelet会杀死这个容器并重新启动它。

通过yaml文件创建Pod:

```shell
kubectl apply -f exec-liveness.yaml
```

Pod创建后，当容器启动时，会执行如下的命令：

 ```shell
 /bin/sh -c "touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600"
 ```

以上命令表明这个容器生命的前30秒，`/tmp/healthy`文件是存在的。所以在这最开始的30秒内，存活探针在进行检测的时候，执行命令`cat /tmp/healthy`会返回成功代码，30秒之后再执行该命令就会返回失败代码了。

在Pod创建后的30秒内查看Pod的状态以及详细信息：

```shell
kubectl get pod -o wide
kubectl describe pod exec-liveness|tail -n 10
```

![image-20210509215420137](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509215423.png)  

由上图可知，Pod创建的30秒内，Pod的运行是正常的，存活检测也是没有任何问题的。

在Pod创建30秒后查看Pod的状态以及详细信息：

![image-20210509215752404](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509215756.png) 

由上图可知，由于30秒后`/tmp/healthy`文件已经被删除了，所以存活检测失败了。

再过一段时间后我们会发现，容器被杀死并且被重启了，重启完成后，再使用`kubectl get pod -o wide`命令查看Pod的状态时，输出结果中的RESTARTS的值变成了1，表示已经进行了一次重启。

![image-20210509220358522](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509220720.png) 

###### 5.5.4.2 使用httpGet进行存活检测

新增一个httpGet-liveness.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: http-liveness
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    livenessProbe:
      httpGet:
        port: 80
        path: /index.html
      initialDelaySeconds: 1
      periodSeconds: 3
```

默认情况下，nginx容器中是包含上面的index.html的，所以创建容器后，存活检测肯定是成功的。

通过yaml文件创建Pod:

```shell
kubectl apply -f httpGet-liveness.yaml
```

查看Pod的状态、详细信息以及日志信息：

![image-20210509230027051](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509230040.png) 

由上图可知，存活检测是检测成功的，下面通过手动删除容器中index.html文件的方式，看看存活检测是否能够检测出。

![image-20210509232519403](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509232522.png) 

由上图可知，删除index.html文件后，存活检测已经检测出问题了，并报了404错误，过一段时候后，容器就会被重启。

![image-20210509233318300](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509234429.png) 

由于index.html文件是nginx容器自带的，所以当容器重启完成之后，该文件就又会出现，这时候存活检测就又会检测成功了，可以通过`kubectl logs http-liveness`命令查看日志情况。

###### 5.5.4.3 使用tcpSocket进行存活检测

新增一个tcpSocket-liveness.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tcp-liveness
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    livenessProbe:
      tcpSocket:
        port: 80
      initialDelaySeconds: 1
      periodSeconds: 3
```

通过yaml文件创建Pod:

```shell
kubectl apply -f tcpSocket-liveness.yaml
```

查看Pod的状态、详细信息以及日志信息：

![image-20210509234423456](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210509234434.png) 

由上图可知，通过tcpSocket方式进行的存活检测是检测成功的。

#### 5.6 Pod的生命周期事件

这里的事件主要是指`postStart`和`preStop`，当一个容器启动后，k8s会发送postStart事件，在容器被终结时，k8s会发送一个preStop事件。所以我们可以通过这两个事件，对容器启动时和终止时分别进行一些自定义的操作。

定义一个lifecycle-events.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-demo
spec:
  containers:
  - name: lifecycle-demo-container
    image: nginx
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh", "-c", "echo Hello postStart handler > /usr/share/message"]
      preStop:
        exec:
          command: ["/bin/sh", "-c", "echo Hello preStop handler > /usr/share/message"]
```

通过yaml创建Pod：

```shell
kubectl apply -f lifecycle-events.yaml
```

查看postStart是否生效：

```shell
kubectl exec lifecycle-demo -it -- cat /usr/share/message
```

![image-20210510143809792](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210510143821.png) 

**注意**：k8s只有在Pod被终止时才会发送preStop事件，这个时候我们已经无法进入容器了，所以这里就没法演示效果了。

### 6.名称空间和标签

#### 6.1 名称空间-Namespace

Namespace在很多情况下用于实现多用户的资源隔离，通过将集群内部的资源对象分配到不同的名称空间中，形成逻辑上的分组，便于不同的分组在共享使用整个集群资源的同时还能被分别管理。k8s集群在启动后，会创建一个名为`default`的默认名称空间，如果不特别指明名称空间，那么用户创建的Pod等资源都会被系统默认分配到这个默认的名称空间中。

> **说明：**我们应避免使用前缀`kube-`来创建名称空间，因为它是为Kubernetes系统名称空间保留的。

并非所有对象都在名称空间中。k8s的大多数资源(比如Pod，service，副本控制器等)都会位于某些名称空间中，但是名称空间资源本身并不在名称空间中，而且底层资源(比如node，persistentvolume等)也都不属于任何名称空间。我们可以通过以下命令查看哪些Kubernetes资源在名称空间中，哪些不在名称空间中：

```shell
#查询位于名称空间中的资源
kubectl api-resources --namespaced=true

#查询不在名称空间中的资源
kubectl api-resources --namespaced=false
```

##### 6.1.1 查看名称空间

```shell
#使用以下三个命令中的任何一个都可以，效果是一样的
kubectl get namespaces
kubectl get namespace
kubectl get ns
```

![image-20210531170507346](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210531170518.png) 

初始状态下，Kubernetes默认会具有三个名称空间：

- `default` 无名称空间对象的默认名称空间；
- `kube-system` 由Kubernetes系统创建的对象的名称空间；
- `kube-public` 自动创建且被所有用户可读的名称空间(包括未经身份认证的)。此名称空间通常在某些资源在整个集群中可见且可公开读取时被集群使用，此名称空间的公共方面只是一个约定，而不是一个必要条件。

##### 6.1.2 创建名称空间

- 新建一个名为`my-namespace.yaml`的yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ns-test
```

- 通过yaml文件方式进行创建：

```shell
kubectl create -f my-namespace.yaml
```

或者我们也可以直接通过命令的方式创建名称空间：

```shell
kubectl create ns ns-abc
```

查看上面创建的两个名称空间：

![image-20210531172310025](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210531172313.png) 

##### 6.1.3 删除名称空间

比如我们想要删除上面创建的名为`ns-abc`的名称空间，命令如下：

```shell
kubectl delete ns ns-abc
```

> **注意**：以上命令会删除对应名称空间下的所有内容。

##### 6.1.4 指定名称空间创建资源

这里以创建Pod为例，新增一个`Pod.yaml`文件，文件内容如下：

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: sys-test
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-name
  namespace: sys-test
spec:
  containers:
  - name: c-name
    image: nginx:1.20
    imagePullPolicy: IfNotPresent
  restartPolicy: OnFailure
```

> 由于`sys-test`这个名称空间之前并不存在，所以以上yaml文件中也加上了创建名称空间的内容。

通过`kubectl apply -f Pod.yaml`命令创建Pod，然后通过`kubectl get pod -n sys-test -o wide`命令进行查看：

![image-20210531203330730](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210531203333.png) 

#### 6.2 标签-Label

Label是Kubernetes系统中一个核心概念。一个Label就是一个key=value的键值对，其中key与value由用户自己指 定。Label可以附加到各种资源对象上，如Node、Pod、Service等，一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上，Label通常在资源对象定义时确定，也可以在对象创建后动态添加或删除。

##### 6.2.1 标签的基本用法

Label最常见的用法是使用`metadata.labels`字段来为对象添加Label，通过`spec.selector`来引用对象。示例：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-name
  namespace: sys-test
  labels:
    app: deploy-label
spec:
  replicas: 3
  selector:
    matchLabels:
      app: test-label
  template:
    metadata:
      labels:
        app: test-label
    spec:
      containers:
      - name: c-name
        image: nginx:1.20
        imagePullPolicy: IfNotPresent
```

![image-20210531212203425](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210531212211.png) 


> **注意**：在以上yaml文件中，`spec.selector.matchLabels`下的标签要和`spec.template.metadata.labels`下的标签保持一致，可以不和`metadata.labels`下的标签(即**app: deploy-label**)保持一致。

##### 6.2.2 根据标签选择资源

假设同一个名称空间下有很多Pod或者其他别的资源，我们也可以通过标签来进行资源的选择，比如我们想要展示指定标签的Pod，命令如下：

```shell
#查看Pod的标签情况
kubectl get pod -n sys-test --show-labels

#根据指定标签(标签key值为app，value值为pod-label)展示相应的Pod
kubectl get pod -l app=pod-label -n sys-test
```

![image-20210531214854544](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210531214858.png) 

##### 6.2.3 多标签选择的使用

如果想通过多个标签作为条件来筛选的话，标签之间使用逗号隔开即可，如下所示：

```shell
#查看Pod的标签情况
kubectl get pod -n sys-test --show-labels

#给某一个Pod加个其他的标签(env=dev)，方便下面多标签筛选用
kubectl label pod <pod-name> env=dev -n sys-test

#使用多标签进行筛选
kubectl get pod -l app=test-label,env=dev -n sys-test
```

![image-20210601094856214](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601094859.png) 

##### 6.2.4 基于集合的用法

提前准备一批Pod，标签情况如下：

![image-20210601103623492](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601103626.png) 

- 筛选包含app标签，并且type标签值不等于pig的所有Pod：

```shell
#由于写该文档使用的字体的缘故，所有不等于都会被自动转换成!=，特此说明
kubectl get pod -l 'app,type!=pig' -n sys-test --show-labels
```

![image-20210601113056491](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601113059.png) 

- 筛选app标签值等于test-label，type标签值不等于pig的所有Pod：

```shell
kubectl get pod -l 'app=test-label,type!=pig' -n sys-test --show-labels
```

![image-20210601113320829](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601113323.png) 

- 筛选包含app标签，并且type标签值包含pig和dog的所有Pod：

```shell
kubectl get pod -l 'app,type in (pig,dog)' -n sys-test --show-labels
```

![image-20210601113718595](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601113721.png) 

- 筛选不包含type标签的所有Pod：

```shell
kubectl get pod -l '!type' -n sys-test --show-labels
```

![image-20210601114005064](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601114007.png) 

- 筛选包含type标签，但是标签值不包含dog和pig的所有Pod：

```shell
kubectl get pod -l 'type,type notin (dog,pig)' -n sys-test --show-labels
```

![image-20210601114201274](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601114204.png) 

##### 6.2.5 selector字段的用法

前面有提到过，Label最常见的用法是使用`metadata.labels`字段来为对象添加Label，通过`spec.selector`来引用对象，而selector下又包含了==matchLabels==和==matchExpressions==两个字段，比如Job、Deployment等，如下图：

![image-20210601115404447](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601115407.png) 

下面是这两个字段的一个基本的使用格式：

```yaml
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - {key: tier, operator: In, values: [cache]}
    - {key: environment, operator: NotIn, values: [dev]}
```

> **注意**：如果同时使用了`matchLabels`和`matchExpressions`，那么必须在`matchLabels`和`matchExpressions`中给出的所有条件都满足才能匹配。

##### 6.2.6 标签的新增和删除

```shell
#新增标签(标签的key值为app，value值为test)
kubectl label pod pod-name app=test -n sys-test

#根据key值删除标签(标签的key值为app)
kubectl label pod pod-name app- -n sys-test
```

![image-20210601142329645](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601142333.png) 

### 7.k8s核心技术-控制器

k8s提供了很多内置的controller(控制器)，或者叫工作负载资源，通过这些控制器可以控制Pod的具体状态和行为。

控制器的主要分类：

- `Deployment`和`ReplicaSet`(替换原来的`ReplicationController`)。Deployment很适合用来管理我们在集群上的无状态应用，Deployment中的所有Pod都是相互等价的，并且可以在需要的时候被换掉；
- `StatefulSet`。让我们能够运行一个或者多个以某种方式跟踪应用状态的Pods，很适合管理集群上的有状态应用。例如，假设我们需要将数据作持久存储，那我们就可以运行一个StatefulSet，将每个Pod与某个PersistentVolume对应起来。我们在StatefulSet中各个Pod内运行的代码可以将数据复制到同一StatefulSet中的其它Pod中以提高整体的服务可靠性；

- `DaemonSet`。定义提供节点本地支撑设施的Pods。这些Pods可能对于我们集群的运维是非常重要的，例如作为网络链接的辅助工具或者作为网络[插件](https://kubernetes.io/zh/docs/concepts/cluster-administration/addons/)的一部分等等。每次我们向集群中添加一个新节点时，如果该节点与某DaemonSet的规约匹配，则控制面会为该DaemonSet调度一个Pod到该新节点上运行；
- `Job`和`CronJob`。定义一些一直运行到结束并停止的任务。Job用来表达的是一次性的任务，而CronJob会通过cron表达式定时创建Job，然后通过Job运行一些运行到结束并停止的任务。

> 由于`ReplicationController`的一些缺陷，已经不再推荐使用了，所以下文就不再进行介绍了。而`Deployment`是可以管理`ReplicaSet`的，并且拥有更多的功能，所以我们一般不会直接创建`ReplicaSet`，而是创建`Deployment`，而它会自动创建`ReplicaSet`，再通过`ReplicaSet`去创建Pod。kubernetes官网建议我们使用`Deployment`而不是直接使用`ReplicaSet`，所以下文中也不会再介绍`ReplicaSet`了。

#### 7.1 Deployment

Deployment为Pod和ReplicaSet提供了一个声明式定义方法，用来替代以前的ReplicationController来方便地管理应用。需要注意的是，Deployment是先创建ReplicaSet，然后再通过ReplicaSet去创建Pod。

典型的应用场景包括：

- 创建Deployment以将ReplicaSet上线，ReplicaSet在后台创建Pod；
- 滚动升级和应用回滚；
- 应用扩缩容；
- 暂停和恢复Deployment。

##### 7.1.1 创建Deployment

下面是Deployment的一个简单案例，创建一个名为nginx-deployment.yaml的文件：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
        ports:
        - containerPort: 80 
```

在以上案例中：

- 创建了一个名为**nginx-deployment**(由`metadata.name`字段标明)的Deployment；
- 该Deployment创建了3个(由`spec.replicas`字段标明)Pod副本；
- 以上案例中的`spec.selector`字段定义Deployment如何查找要管理的Pods。在这里，我们选择在Pod模板中定义的标签(`app: nginx`)。当然，也可以存在更复杂的选择规则，只要Pod模板本身满足所给规则即可。

通过`kubectl apply -f nginx-deployment.yaml`命令创建Deployment，通过`kubectl get all -o wide`命令查看是否创建成功：

![image-20210601165710858](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601173137.png) 

> **注意**：我们也可以通过`--record`标志将所执行的命令写入资源注解`kubernetes.io/change-cause`中。这对于以后的检查是有用的。例如，要查看针对每个Deployment修订版本所执行过的命令。

假设我们是使用`kubectl apply -f nginx-deployment.yaml --record`命令创建Deployment的，那么可以发现这条命令已经保存到资源注解的`kubernetes.io/change-cause`中了，如下所示：

```shell
kubectl get deploy nginx-deployment -o yaml|grep -10 " annotations"
```

![image-20210601174215278](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601174219.png) 

**注**：默认情况下应该是只保存最后一次操作记录，而且查询的记录不会保存，我尝试过扩缩容操作，扩缩容命令记录也能保存下来，所以猜测应该是只会保存最后一次修改的记录。

在Deployment创建成功后，可以使用以下任一命令进行查看，三个命令查询的效果都是一样的，如果想查看更多信息，只要在命令后面加上`-o wode`即可，比如`kubectl get deploy -o wide`。

```shell
kubectl get deploy
kubectl get deployment
kubectl get deployments
```

比如执行`kubectl get deploy`命令后，则会输出类似下面的内容：

```shell
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           16m
```

- `NAME`：列出了集群中Deployment的名称；
- `READY`：显示应用程序的可用的**副本数**。显示的模式是"就绪个数/期望个数"；
- `UP-TO-DATE`：显示为了达到期望状态已经更新的副本数；
- `AVAILABLE`：显示应用可供用户使用的副本数；
- `AGE`：显示应用程序运行的时间。

> **注意**：期望副本数是根据yaml中的`spec.replicas`字段设置的。

如果我们要查看每个Pod的标签，可以执行`kubectl get pods --show-labels`命令，输出结果如下：

![image-20210601180144901](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601180147.png) 

从输出结果可以发现，每个Pod都有一个**pod-template-hash**标签，这个标签我们不要去修改它，Deployment控制器会将该标签添加到Deployment所创建或收留的每个ReplicaSet中，此标签可确保Deployment的子ReplicaSets不重叠。标签通过对ReplicaSet的`PodTemplate`进行哈希处理会生成一个哈希值，该标签会存在于ReplicaSet可能拥有的任何现有Pod中。

##### 7.1.2 更新Deployment

我们可以通过`kubectl set image`命令对Deployment进行更新操作，具体操作如下：

```shell
#查看Deployment目前的基本信息
kubectl get deploy -o wide

#将名为nginx-deployment的Deployment中nginx的镜像版本更新至1.20.1
kubectl set image deployment/nginx-deployment nginx=nginx:1.20.1

#实时状态查看
kubectl rollout status deployment/nginx-deployment
```

![image-20210601211503658](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601211506.png) 

当Deployment更新完成之后，可以使用`kubectl get rs -o wide`命令查看ReplicaSet的情况，如下图所示，可以发现，虽然Deployment已经更新完了，且创建了新的ReplicaSet，但是老的ReplicaSet并不会被删除，不过其副本数已经为零了。

![image-20210601212756188](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210601212800.png) 

我们也可以使用`kubectl edit deploy nginx-deployment`命令来修改Deployment中containers字段下的image属性值，来达到更新Deployment的目的。

> **注意**：假如我们要创建一个Deployment以生成`nginx:1.14.2`的5个副本，但紧接着就更新这个Deployment以创建5个`nginx:1.16.1`的副本，而此时假设只有3个`nginx:1.14.2`副本已创建。在这种情况下，Deployment会立即开始杀死那3个`nginx:1.14.2`的Pods，并开始创建`nginx:1.16.1`的Pods。它不会等待`nginx:1.14.2`的5个副本都创建完成后才开始执行Deployment的变更动作。

##### 7.1.3 回滚Deployment

新建一个名为deployment.yaml的文件，内容如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-name
  labels:
    app: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-label
  template:
    metadata:
      labels:
        app: nginx-label
    spec:
      containers:
      - name: c-nginx
        image: nginx:1.16
        ports:
        - containerPort: 80
```

首先通过`kubectl apply -f deployment.yaml --record`命令执行文件。这里需要先升级几个版本，然后才能演示回滚到之前的版本，命令如下：

```shell
#依次执行以下命令，多升级几个版本
kubectl set image deploy deploy-name c-nginx=nginx:1.17 --record
kubectl set image deploy deploy-name c-nginx=nginx:1.18 --record
kubectl set image deploy deploy-name c-nginx=nginx:1.19 --record
kubectl set image deploy deploy-name c-nginx=nginx:1.20 --record

#以上几个命令都执行完成之后，可以通过如下命令查看修订历史(以上命令后面没加"--record"的话，查出来的就是空)
kubectl rollout history deploy deploy-name
```

![image-20210602100842626](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210602100845.png) 

我们也可以通过`--revision`查看某一个版本的详情，命令如下：

```shell
#查看版本3的详情
kubectl rollout history deploy deploy-name --revision=3
```

![image-20210602101231944](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210602101234.png) 

接下来进行回滚操作，假设我们要回滚到上一版本(即版本4)，可以使用如下命令：

```shell
kubectl rollout undo deploy deploy-name
```

![image-20210602104556192](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210602104558.png) 

由上图可知，回滚到版本4成功后，在修订历史中，之前的版本4就变成了现在的版本6了。如果我们再次执行回滚到上一版本的命令，就会回滚到版本5，这样一来，nginx的版本就又会变成1.20了。如果再次查看修订历史，之前的版本5则会变成版本7，如下图所示：

![image-20210602105554704](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210602105557.png) 

如果我们不想只回滚到上一版本，也可以通过使用`--to-revision`来回滚到特定的修订版本，案例如下：

```shell
#回滚到版本2
kubectl rollout undo deploy deploy-name --to-revision=2
```

![image-20210602110838731](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210602110841.png) 

##### 7.1.4 缩放Deployment

###### 7.1.4.1 kubectl scale命令

我们可以使用`kubectl scale`命令来增加或者减少Deployment的副本数，案例如下：

```shell
#假设现在已经运行了一个副本数为3个的Deployment，可以通过如下命令将副本数扩容到5个
kubectl scale deploy deploy-name --replicas=5

#也可以使用如下命令将副本数缩容到2个
kubectl scale deploy deploy-name --replicas=2
```

![image-20210602114939473](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210602114942.png) 

###### 7.1.4.2 HPA实现水平扩缩容

我们还可以使用[Horizontal Pod Autoscaler](https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/)(简称HPA)进行[水平自动缩放](https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)，HPA 可以基于 CPU 利用率自动扩缩ReplicationController、Deployment、ReplicaSet和StatefulSet中的Pod数量。除了CPU利用率，也可以基于其他应程序提供的自定义度量指标来执行自动扩缩。Pod自动扩缩不适用于无法扩缩的对象，比如DaemonSet等。

**部署metrics-server服务：**

使用HPA需要metrics-server环境，所以这里先部署metrics-server环境。具体步骤如下：

- 下载components.yaml文件：

  ```shell
  #最好从以下地址获取components.yaml文件，这个是修改过的，直接从官方github获取的可能无法使用
  wget https://files.cnblogs.com/files/gongcqq/components.tar.gz
  
  #解压获取components.yaml文件
  tar -zxvf components.tar.gz
  ```

- 执行components.yaml文件：

  ```shell
  #components.yaml文件中会用到一个镜像，网速不好的可以使用如下命令在worker节点提前下载
  docker pull gongcqq/metrics-server:v0.4.2
  
  #使用如下命令执行解压出来的名为components-0.4.2.yaml的yaml文件
  kubectl apply -f components-0.4.2.yaml
  
  #然后使用如下命令查看metrics-server相关的Pod是否正常运行，并已处于READY状态
  kubectl get pod -n kube-system -o wide
  ```

  ![image-20210603143058580](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603143102.png) 

- 检查metrics-server是否可以正常使用：

  ```shell
  #检查相关apiservice是否可用
  kubectl get apiservice |grep "v1beta1.metrics.k8s.io"
  
  #使用kubectl top命令检查是否有数据
  kubectl top nodes
  ```

  ![image-20210603143829060](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603143831.png) 

- 修改kube-apiserver.yaml文件：

  ```shell
  #先使用vim编辑器打开文件
  vim /etc/kubernetes/manifests/kube-apiserver.yaml
  
  #然后在spec.containers.command字段中添加如下内容，并保存退出，见下图
  --enable-aggregator-routing=true
  ```

  ![image-20210603150653553](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603150658.png) 

  ```shell
  #修改kube-apiserver.yaml文件后API Server会自动重启生效，稍等一会儿后再次使用如下命令查看
  kubectl top nodes
  kubectl get apiservice |grep "v1beta1.metrics.k8s.io"
  ```

  ![image-20210603155539667](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603155541.png) 

  由上图可知，metrics-server已经部署成功了。

  > **注意**：我的kubernetes集群是使用kubeadm方式安装的，如果是使用二进制方式安装的Kubernetes集群，是否还可以通过修改kube-apiserver.yaml文件的方式来解决metrics-server不可用的问题，暂未可知。

**关于水平自动扩缩容的案例如下：**

metrics-server部署成功后，就可以测试HPA的水平扩缩容了，下面是一个简单的案例。

- 新建一个名为hpa-example.yaml的yaml文件，内容如下：

  ```yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: hpa-deploy
  spec:
    selector:
      matchLabels:
        run: hpa-label
    replicas: 1
    template:
      metadata:
        labels:
          run: hpa-label
      spec:
        containers:
        - name: hpa-example
          image: gongcqq/hpa-example:1.0
          ports:
          - containerPort: 80
          resources:
            limits:
              cpu: 500m
            requests:
              cpu: 200m
  ---
  apiVersion: v1
  kind: Service
  metadata:
    name: hpa-service
    labels:
      run: hpa-label
  spec:
    ports:
    - port: 80
    selector:
      run: hpa-label
  ```

  > 需要注意的是，以上yaml文件中，一定要有`spec.resources`属性，并要设置其下的limits字段以及requests字段中cpu的相关的值，不然下面没法根据CPU进行自动扩缩容。

- 使用`kubectl apply -f hpa-example.yaml`命令进行执行，然后使用`kubectl get deploy,pod,svc -o wide`命令进行查看：

  ![image-20210603160242740](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603160245.png)  

- 使用如下命令创建一个HPA，实现自动扩缩容：

  ```shell
  #下面命令的意思是，当现有Deployment中Pod的平均CPU利用率达到50%的时候，就进行自动扩缩容，k8s中会有相关算法决定最终的副本数的个数。"--max"参数表示副本数最大扩到10个，"--min"参数表示最小缩到1个。
  kubectl autoscale deploy hpa-deploy --cpu-percent=50 --min=1 --max=10
  ```

- 创建好HPA后，可以通过`kubectl get hpa`命令进行查看：

  ![image-20210603160539464](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603160541.png)  

  > **注意**：当前的CPU利用率为`0%`或者`<unknown>`，这是由于我们尚未发送任何请求到服务器，当负载增加后，这里的利用率才会出现具体百分比。

- 我们可以通过如下命令启动一个容器，并通过一个循环向**hpa-service**服务器发送无限的查询请求来增加CPU负载：

  ```shell
  #重新开一个终端窗口执行以下命令
  kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while true; do wget -q -O- http://hpa-service; done"
  ```

  ![image-20210602194909060](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210602194913.png) 

- 一段时间后，我们再使用`kubectl get hpa`命令进行查看，结果如下图所示：

  ![image-20210603160959992](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210605115800.png) 

- 使用`kubectl get deploy,pod -o wide`命令查看Pod情况，看看是否进行了自动扩容，结果如下图所示：

  ![image-20210603161601362](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603161605.png)

> 阿里巴巴还有一个CronHPA，用于实现定时扩缩容的，由于不是k8s官网的东西，所以这里就不进行介绍了。

##### 7.1.5 暂停、恢复Deployment

创建一个名为deployment.yaml的yaml文件，内容如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-name
  labels:
    app: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-label
  template:
    metadata:
      labels:
        app: nginx-label
    spec:
      containers:
      - name: c-nginx
        image: nginx:1.16
        ports:
        - containerPort: 80
```

然后执行以下命令：

```shell
#执行deployment.yaml文件
kubectl apply -f deployment.yaml --record

#为了方便下面演示暂停、恢复操作时作对比，这里先升级一个版本
kubectl set image deploy deploy-name c-nginx=nginx:1.17 --record
```

最后查看一下最终的效果：

```shell
#运行状态查看
kubectl get deploy,pod,rs -o wide

#修订历史查看
kubectl rollout history deploy deploy-name
```

![image-20210603164719118](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603164721.png) 

###### 7.1.5.1 暂停Deployment

上面已经进行了一些前期准备，现在开始演示暂停效果：

- 使用如下命令暂停Deployment：

  ```shell
  kubectl rollout pause deploy deploy-name
  ```

- 暂停后，查看Deployment的详情：

  ```shell
  kubectl describe deploy deploy-name
  ```

  ![image-20210603165712357](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210605115820.png) 

- 对Deployment执行一个更新操作，看看是否触发上线动作：

  ```shell
  #将镜像版本更新至nginx:1.18
  kubectl set image deploy deploy-name c-nginx=nginx:1.18 --record
  
  #镜像更新后，使用如下命令查看资源详情以及修订历史
  kubectl get deploy,pod -o wide
  kubectl get rs -o wide
  kubectl rollout history deploy deploy-name
  ```

  ![image-20210603171357161](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603171400.png) 

###### 7.1.5.2 恢复Deployment

上面暂停Deployment后，对Deployment的操作并不会触发上线动作，即不会影响到rs，现在恢复Deployment，再看看效果，具体操作如下：

- 使用如下命令恢复Deployment：

  ```shell
  kubectl rollout resume deploy deploy-name
  ```

- 恢复Deployment后，再次使用如下命令查看资源详情以及修订历史：

  ```shell
  kubectl get deploy,pod -o wide
  kubectl get rs -o wide
  kubectl rollout history deploy deploy-name
  ```

  ![image-20210603173332972](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603173335.png)

  > **注意**：我们不可以回滚处于暂停状态的Deployment，除非先恢复其执行状态。

在实际生产环境上，我们的项目都会以Pod的形式部署在生产主机上，如果我们使用Deployment的方式管理Pod的话，假设某个阶段，我们对项目修改的动作比较多，但是又不想每次修改都触发上线动作，那就可以使用暂停操作。等多次修改并确认无误后，再恢复之前暂停的Deployment，就会触发上线动作，并将之前的修改动作一次性应用到生产环境上。

##### 7.1.6 策略

###### 7.1.6.1 清理策略

我们可以在Deployment中通过`spec.revisionHistoryLimit`字段以指定保留此Deployment的多少个旧的RS。其余的ReplicaSet将在后台被垃圾回收。默认情况下，此值为10。

```shell
kubectl explain Deployment.spec.revisionHistoryLimit
```

![image-20210603180531371](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210603180533.png) 

> **注意**：将此字段设置为0将导致Deployment的所有历史记录被清空，因此Deployment将无法回滚。

###### 7.1.6.2 替换策略

我们可以在Deployment中通过`spec.strategy.type`字段来指定用新Pods替换旧Pods的策略。`spec.strategy.type`字段的值可以是==Recreate==或者==RollingUpdate==，RollingUpdate是默认值。

值为Recreate是指在创建新Pods之前，所有现有的Pods都会被杀死；值为RollingUpdate是指采取滚动更新的方式更新Pods。我们可以指定 `spec.strategy.rollingUpdate`属性下的`maxUnavailable`字段和`maxSurge`字段来控制滚动更新过程。

##### 7.1.7 金丝雀部署

多标签的场景是用来区分同一组件的不同版本或者不同配置的多个部署。常见的做法是部署一个使用**金丝雀发布**来部署新应用版本(在Pod模板中通过镜像标签指定)，保持新旧版本应用同时运行。这样的话，新版本在完全发布之前也可以接收实时的生产流量。

我们可以使用`track`标签来区分不同的版本，案例如下：

假设稳定的发行版有一个`track`标签，其值为`stable`：

```yaml
    name: frontend
    replicas: 3
    ...
    labels:
    app: guestbook
    tier: frontend
    track: stable
    ...
    image: gb-frontend:v3
```

然后，我们可以创建guestbook前端的新版本，让这些版本的`track`标签的值为`canary`，以便两组Pod不会重叠：

```yaml
    name: frontend-canary
    replicas: 1
    ...
    labels:
    app: guestbook
    tier: frontend
    track: canary
    ...
    image: gb-frontend:v4
```

前端服务通过选择标签的公共子集(即忽略`track`标签)来覆盖两组副本，以便流量可以转发到两个应用：

```yaml
  selector:
     app: guestbook
     tier: frontend
```

我们可以调整`stable`和`canary`版本的副本数量，以确定每个版本将接收实时生产流量的比例(在本例中为3:1)。一旦有信心，我们就可以将新版本应用的`track`标签的值从`canary`替换为`stable`，并且将老版本应用删除。

#### 7.2 StatefulSet

StatefulSet是用来管理有状态应用的工作负载API对象，它可以用来管理一组Pod集合的部署和扩缩，并保证这些Pod的顺序和唯一性。

和Deployment类似，StatefulSet管理基于相同容器规约的一组Pod。但和Deployment不同的是，StatefulSet为它们的每个Pod维护了一个有粘性的ID。这些Pod是基于相同的规约来创建的，但是不能相互替换，无论怎么调度，每个Pod都有一个永久不变的ID。

如果希望使用存储卷为工作负载提供持久存储，可以使用StatefulSet作为解决方案的一部分。尽管StatefulSet中的单个Pod仍可能出现故障，但持久的Pod标识符使得将现有卷与替换已失败Pod的新Pod相匹配变得更加容易。

对于需要满足以下一个或多个需求的应用很适合使用StatefulSet：

- 稳定的、唯一的网络标识符；
- 稳定的、持久的存储；
- 有序的、优雅的部署和缩放；
- 有序的、自动的滚动更新。

> **说明**：在上面的几点描述中，"稳定的"意味着Pod调度或重调度的整个过程是有持久性的。如果应用程序不需要任何稳定的标识符或有序的部署、删除或伸缩，则应该使用由一组无状态的副本控制器提供的工作负载来部署应用程序，比如Deployment或许可能更适用于我们的无状态应用部署需要。

相关限制的一些解释说明：

- 给定Pod的存储必须由[PersistentVolume 驱动](https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md)基于所请求的`storage class`来提供，或者由管理员预先提供；
- 当我们在删除或者缩放StatefulSet时并不会删除它关联的存储卷，这样做是为了保证数据安全，它通常比自动清除StatefulSet所有相关的资源更有价值；
- StatefulSet需要[无头服务(Headless Service)](https://kubernetes.io/zh/docs/concepts/services-networking/service/#headless-services)来负责Pod的网络标识，需要我们负责创建此服务；
- 当删除StatefulSet时，StatefulSet不提供任何终止Pod的保证，为了实现StatefulSet中的Pod可以有序地且体面地终止，可以在删除之前将StatefulSet缩放为0；
- 在默认[Pod管理策略](https://kubernetes.io/zh/docs/concepts/workloads/controllers/statefulset/#pod-management-policies)(`OrderedReady`)时使用滚动更新，可能进入需要[人工干预](https://kubernetes.io/zh/docs/concepts/workloads/controllers/statefulset/#forced-rollback)才能修复的损坏状态。

##### 7.2.1 使用StatefulSet

下面创建一个简单案例，案例如下：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None  #这里设置为None就表示创建的是一个无头服务
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx  #需要去匹配spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 3  #默认值是1
  template:
    metadata:
      labels:
        app: nginx  #需要去匹配spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: nginx:1.16
        ports:
        - containerPort: 80
          name: web
```

上述案例中：

- 名为`nginx`的无头服务(Headless Service)是用来控制网络域名的；
- 名为`web`的StatefulSet的spec属性下的replicas字段为3，它表明将在独立的3个Pod副本中启动nginx容器。

通过以上yaml文件创建StatefulSet，然后使用`kubectl get sts,pod,svc -o wide`命令进行查看：

![image-20210604155039985](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210604155042.png) 

###### 7.2.1.1 稳定的网络ID

StatefulSet(下面简称sts)中的每个Pod根据StatefulSet的名称和Pod的序号派生出它的主机名。组合主机名的格式为`$(sts 名称)-$(序号)`。上例将会创建三个名称分别为`web-0、web-1、web-2`的Pod。sts可以使用[无头服务](https://kubernetes.io/zh/docs/concepts/services-networking/service/#headless-services)控制它的Pod的网络域。管理域的这个服务的格式为：`$(服务名称).$(命名空间).svc.cluster.local`，其中`cluster.local`是集群域。一旦每个Pod创建成功，就会得到一个匹配的DNS子域，格式为：`$(pod 名称).$(所属服务的 DNS 域名)`，其中所属服务由sts的`serviceName`域来设定。

下面给出一些案例，告诉我们集群域、服务名以及StatefulSet名称是怎样影响StatefulSet的Pod上的DNS名称的：

| cluster域名   | Service       | StatefulSet(ns/name) | StatefulSet域名                 | Pod DNS                                       | Pod主机名     |
| ------------- | ------------- | -------------------- | ------------------------------- | --------------------------------------------- | ------------- |
| cluster.local | default/nginx | default/web          | nginx.default.svc.cluster.local | web-{0...N-1}.nginx.default.svc.cluster.local | web-{0...N-1} |
| cluster.local | foo/nginx     | foo/web              | nginx.foo.svc.cluster.local     | web-{0...N-1}.nginx.foo.svc.cluster.local     | web-{0...N-1} |
| kube.local    | foo/nginx     | foo/web              | nginx.foo.svc.kube.local        | web-{0...N-1}.nginx.foo.svc.kube.local        | web-{0...N-1} |

###### 7.2.1.2 部署和扩缩保证

- 对于包含N个副本的StatefulSet，当部署Pod时，这些Pod是根据序号的从小到大依次创建的，而且前面的Pod必须是Running和Ready状态才会部署下一个Pod；
- 当删除Pod时，它们是逆序终止的，也就是根据序号的从大到小依次终止；
- 在将缩放操作应用到Pod之前，它前面的所有Pod必须是Running和Ready状态；
- 在Pod终止之前，所有的继任者必须完全关闭。

在上面的nginx示例被创建后，会按照web-0、web-1、web-2的顺序部署三个Pod。在web-0进入Running和Ready状态前不会部署web-1。在web-1进入Running和Ready状态前不会部署 web-2。如果 web-1 已经处于Running和Ready状态，而 web-2 尚未部署，在此期间发生了 web-0 运行失败，那么 web-2 将不会被部署，要等到 web-0 部署完成并进入Running和Ready状态后，才会部署web-2。

如果用户想将示例中的StatefulSet收缩为`replicas=1`，首先被终止的是 web-2。 在 web-2 没有被完全停止和删除前，web-1不会被终止。当web-2已被终止和删除、web-1尚未被终止，如果在此期间发生web-0运行失败，那么就不会终止web-1，必须等到web-0进入Running和Ready状态后才会终止web-1。

###### 7.2.1.3 滚动更新

使用`RollingUpdate`更新策略可以对StatefulSet中的Pod执行自动的滚动更新。在没有声明`spec.updateStrategy`时，`RollingUpdate`是默认配置。当StatefulSet的 `spec.updateStrategy.type` 被设置为 `RollingUpdate` 时，StatefulSet控制器会删除和重建StatefulSet中的每个Pod。它将按照与Pod终止相同的顺序(按照序号从大到小)进行，每次更新一个Pod。它会等到被更新的Pod进入Running和Ready状态，然后才会再更新其前身。

###### 7.2.1.4 关于分区

通过声明`spec.updateStrategy.rollingUpdate.partition`的方式，`RollingUpdate`更新策略可以实现分区。如果声明了一个分区，当StatefulSet的`spec.template`被更新时，所有序号大于等于该分区序号的Pod都会被更新。所有序号小于该分区序号的 Pod 都不会被更新，并且，即使他们被删除也会依据之前的版本进行重建。 如果 StatefulSet 的`spec.updateStrategy.rollingUpdate.partition`的值大于它的`spec.replicas`，那么对它的`spec.template`的更新将不会传递给它的Pod。在大多数情况下，我们不需要使用分区，但如果我们希望进行阶段更新、执行金丝雀或执行分阶段上线，则这些分区会非常有用。

##### 7.2.2 创建StatefulSet

这里还是使用上面案例中用到的yaml文件，将其命名为web.yaml，内容如下：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx
  serviceName: "nginx"
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: nginx:1.16
        ports:
        - containerPort: 80
          name: web
```

使用如下命令来创建定义在`web.yaml`中的Headless Service和StatefulSet：

```shell
kubectl apply -f web.yaml
```

在另一个终端中，使用如下命令来查看StatefulSet的Pod创建情况：

```shell
kubectl get pods -w -l app=nginx
```

![image-20210604211505810](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210604213220.png)  

StatefulSet中的Pod是拥有一个唯一的顺序索引和稳定的网络身份标识的。

###### Pod的顺序索引

```shell
[root@master ~]# kubectl get pods -l app=nginx
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          21m
web-1   1/1     Running   0          21m
web-2   1/1     Running   0          21m
```

如同前面所提到的，StatefulSet中的Pod拥有一个具有黏性的、独一无二的身份标志。这个标志是基于StatefulSet控制器分配给每个Pod的唯一顺序索引。Pod名称的形式为`<statefulset名称>-<顺序索引序号>`。以上StatefulSet拥有三个副本，所以它创建了三个Pod`web-0`、`web-1`和`web-2`。

###### 稳定的网络身份标识

每个Pod都拥有一个基于其顺序索引的稳定的主机名，可以使用`kubectl exec`命令在每个Pod中执行`hostname`查看：

```shell
for i in 0 1 2; do kubectl exec "web-$i" -- sh -c 'hostname'; done
```

![image-20210604213151112](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210604213203.png) 

使用`kubectl run`命令运行一个提供`nslookup`命令的容器(这里用的是busybox)，该命令来自于`dnsutils`包。然后在容器中通过对Pod的主机名执行`nslookup`命令，我们可以检查他们在集群内部的DNS地址。

```shell
kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm
```

以上命令会进入到容器中，然后在容器中再执行以下命令：

```shell
#这边就只对主机名为web-1的Pod进行测试，别的Pod效果也是一样的
nslookup web-1.nginx
```

![image-20210605110540178](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210605115740.png) 

![image-20210605110948000](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210605115737.png) 

下面我们删除这个Pod：

```shell
kubectl delete pod web-1
```

使用`kubectl exec`命令查看名为web-1的Pod的主机名，可以发现，主机名是没变的：

```shell
[root@master ~]# kubectl exec web-1 -- sh -c "hostname"
web-1
```

然后查看一下名为web-1的Pod的情况：

```shell
kubectl get pod web-1 -o wide
```

![image-20210605112306660](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210605115732.png) 

再次在之前启起来的名为dns-test的Pod的容器中执行以下命令：

```shell
nslookup web-1.nginx
```

![image-20210605113158879](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210605115716.png) 

通过以上案例可以发现，Pod的序号、主机名、SRV条目和记录名称都没有改变，但和Pod相关联的IP地址发生了改变，这就是为什么不要在其他应用中使用StatefulSet中的Pod的IP地址进行连接，这点很重要。

如果我们的应用已经实现了用于测试liveness以及readiness的连接逻辑，我们可以考虑使用Pod的SRV记录(比如`web-1.nginx.default.svc.cluster.local`)。因为他们是稳定的，并且当我们的Pod的状态变为Running和Ready时，我们的应用就能够发现它们的地址。

##### 7.2.3 扩缩StatefulSet

扩缩StatefulSet和扩缩Deployment是类似的，都可以使用`kubectl scale`命令。目前用的例子中StatefulSet有三个副本，如果想要扩容到五个，可以使用如下命令：

```shell
kubectl scale sts web --replicas=5
```

![image-20210605144727745](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210606172135.png) 

我们也可以使用`kubectl scale`来缩容，副本数设置小于5即可。除此之外，还可以使用`kubectl patch`命令进行扩缩容操作，或者修改镜像等。

比如使用`kubectl patch`命令进行扩缩容操作，上一步已经使用`kubectl scale`命令扩容，现在就来演示缩容：

```shell
#将副本数从之前的5个缩容到现在的3个
kubectl patch sts web -p '{"spec":{"replicas":3}}'
```

![image-20210605150730044](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210606172126.png) 

我们还可以使用`kubectl patch`命令对StatefulSet进行升级操作，下面的更新StatefulSet章节会讲到。

##### 7.2.4 更新StatefulSet

###### 7.2.4.1 kubectl patch

现在演示一下如何使用`kubectl patch`命令更新StatefulSet，具体命令如下：

```shell
#将nginx的版本升级到1.18
kubectl patch sts web --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"nginx:1.18"}]'
```

![image-20210605151916767](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210606172119.png) 

在使用`kubectl patch`命令对StatefulSet进行升级操作时，我把升级的过程也截图贴出来了，通过上图正好也印证了一点，StatefulSet的升级是从后往前，从序号大的到序号小的进行的。而且，只有当上一个处于Running和Ready状态时，才会对下一个Pod进行升级操作。

如果我们想在更新StatefulSet中的某个Pod的镜像时，查看前面的镜像是否确实更新成功了的话，可以在更新时使用下面的命令获取目前3个Pod的镜像情况：

```shell
for p in 0 1 2; do echo -n Pod名称：web-$p, 镜像：;kubectl get pod "web-$p" --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done
```

![image-20210605163542209](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210606172109.png) 

###### 7.2.4.2 灰度发布

上文有提到过分区的概念，通过声明`spec.updateStrategy.rollingUpdate.partition`的方式，`RollingUpdate`更新策略可以实现分区。我们可以通过分区的方式进行灰度发布，下面进行一个案例的演示：

```shell
#为了方便演示，我们先将StatefulSet的副本数扩到5个
kubectl patch sts web -p '{"spec":{"replicas":5}}'

#通过以下命令查看扩容后各个Pod的镜像
for p in 0 1 2 3 4; do echo -n Pod名称：web-$p, 镜像：;kubectl get pod "web-$p" --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done

#然后通过kubectl patch命令设置partition字段的值为2
kubectl patch sts web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":2}}}}'

#最后通过如下命令查看partition字段的值是否设置成功
kubectl get sts web -o yaml|grep -2 " partition"
```

![image-20210605171421961](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210606172102.png) 

以上操作完成之后，开始更新StatefulSet：

```shell
#将StatefulSet的镜像更新至nginx:1.20
kubectl patch sts web --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"nginx:1.20"}]'
```

![image-20210605173718968](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210606172053.png) 

前面已经提到过，StatefulSet下的Pod的名称是由StatefulSet的名称和序号组成的，上面将`partition`字段的值设置为2的意思就是，当StatefulSet的`spec.template`字段下的内容被更新时，所有名称中的序号大于等于2的Pod都会被更新。升级镜像会修改`image`字段，该字段就是`spec.template`字段下的，所以StatefulSet下的五个副本中，只有名为web-2、web-3以及web-4的Pod的镜像更新了。名为web-0以及web-1的Pod的名称中的序号都小于`partition`字段的值(也就是小于2)，所以对应Pod的镜像都没有进行更新，都还是nginx:1.18。即便我们使用`kubectl delete`命令删除名为web-0和web-1的Pod，待Pod重新拉起后，镜像依然会是nginx:1.18。

如果我们希望StatefulSet下的所有Pod的镜像都更新至nginx:1.20的话，将`partition`字段的值设置为0即可：

```shell
#执行如下命令，将partition的值设置为0
kubectl patch sts web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":0}}}}'
```

![image-20210605175928508](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210606172043.png) 

> **说明**：在生产环境上上线新版本代码时，灰度发布将变得非常有用。因为在上线新版本时，我们并不确定新版本的稳定性，所以贸然将生产上的所有流量都导向新版本的话，一旦出现问题，将可能出现无法挽回的损失。这时候就可以通过灰度发布的方式，进行流量的控制。
>
> 假设我们老版本项目启了5个Pod，通过负载将用户流量分发到了各个Pod上，这时我们要上线新版本了，可以像上面案例中演示的那样，通过设置`partition`字段值的方式，只让部分Pod更新至新版本，待新版本稳定后，再让所有的Pod都更新至新版本，以最低的风险完成新老版本的交替。

##### 7.2.5 删除StatefulSet

我们直接使用`kubectl delete`命令执行删除操作的话，删除StatefulSet后，该StatefulSet管理的Pod也都会被一起删除掉，这种属于级联删除，我们还可以只删除StatefulSet，保留StatefulSet下的Pod，这种属于非级联删除。我们可以使用`--cascade=false`参数实现非级联删除，演示如下：

```shell
kubectl delete statefulset web --cascade=false
```

![image-20210608144749724](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210608155311.png) 

当我们删除statefulset后，如果我们再删除其下的Pod，那么对应Pod删除后将不会再被重新拉起了。

![image-20210608145215536](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210608155357.png) 

##### 7.2.6 Pod的管理策略

对于某些分布式系统来说，StatefulSet的顺序性保证是不必要或者不应该的。这些系统仅仅要求唯一性和身份标志。为了解决这个问题，在Kubernetes 1.7中，针对StatefulSet API对象引入了`spec.podManagementPolicy`属性。此选项仅影响扩缩操作的行为，更新不受影响。`spec.podManagementPolicy`的默认策略是`OrderedReady`，这种策略表明StatefulSet控制器遵循上文提到的顺序性保证，它的值还可以是`Parallel`，该策略告诉StatefulSet控制器可以并行地终止所有Pod，在启动或终止另一个Pod前，不必等待这些Pod变成Running和Ready或者完全终止状态。案例如下：

- 创建一个名为web-parallel.yaml的yaml文件，内容如下：

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: nginx-svc
    labels:
      app: nginx-parallel
  spec:
    ports:
    - port: 80
      name: web-parallel
    clusterIP: None
    selector:
      app: nginx-parallel
  ---
  apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    name: web-parallel
  spec:
    serviceName: "nginx-svc"
    podManagementPolicy: "Parallel"  #这里使用的是Parallel策略
    replicas: 5
    selector:
      matchLabels:
        app: nginx-parallel
    template:
      metadata:
        labels:
          app: nginx-parallel
      spec:
        containers:
        - name: c-nginx
          image: nginx:1.18
          ports:
          - containerPort: 80
            name: web-parallel
  ```

- 使用`kubectl apply -f web-parallel.yaml`命令根据指定文件创建对应资源；

- 使用`kubectl get all -o wide`命令查询Pod、StatefulSet等资源的运行情况：

  ![image-20210608152149438](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210608155408.png) 

- 我们也可以使用`kubectl delete -f web-parallel.yaml`命令删除创建的资源，观察Pod终止的情况：

  ![image-20210608152620519](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210608155427.png) 

- 如果我们使用如下命令执行更新操作的话，可以发现，和创建、删除不一样，更新依然是顺序性的：

  ```shell
  kubectl patch sts web-parallel --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"nginx:1.20"}]'
  ```

  ![image-20210608153921415](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210608155449.png) 

#### 7.3 DaemonSet

DaemonSet确保全部(或者某些)节点上运行一个Pod的副本。当有节点加入集群时，也会为他们新增一个Pod。当有节点从集群移除时，这些Pod也会被回收，删除DaemonSet将会删除它创建的所有Pod。

DaemonSet 的一些典型用法：

- 在每个节点上运行集群守护进程；
- 在每个节点上运行日志收集守护进程；
- 在每个节点上运行监控守护进程。

一种简单的用法是为每种类型的守护进程在所有的节点上都启动一个DaemonSet。一个稍微复杂的用法是为同一种守护进程部署多个DaemonSet，每个具有不同的标志，并且对不同硬件类型具有不同的内存、CPU要求。

##### 7.3.1 使用DaemonSet

下面进行一个简单的案例演示，首先创建一个名为daemonset.yaml的文件，内容如下：

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-es
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: c-fluentd
        image: gongcqq/fluentd:v1.9
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

> **说明**：master节点由于加了污点，所以默认情况下是不调度Pod的，为了演示DaemonSet会在所有节点新增一个Pod，所以以上的yaml文件使用`tolerations`字段设置了容忍，如果不想在master节点上创建Pod，去掉文件中关于容忍的内容即可。

然后使用`kubectl apply -f daemonset.yaml`命令基于yaml文件创建DaemonSet：

![image-20210608173729536](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210609104912.png)

> **注意**：DaemonSet没有副本数的概念，创建DaemonSet后，具体会根据该DaemonSet创建多少个Pod，取决于集群中可以调度Pod的节点数。不过DaemonSet也有亲和性，如果使用亲和性了的话，情况就会不一样了，下面会介绍到。

##### 7.3.2 DaemonSet的亲和性问题

之所以单独把DaemonSet的亲和性拿出来说一下，是因为DaemonSet默认会在可以调度的节点上都创建一个Pod，但是如果DaemonSet配置了亲和性的话，就有点不一样了。

###### 7.3.2.1 nodeSelector属性演示

- 新建一个名为ds-nodeSelector.yaml的文件，内容如下：

  ```yaml
  apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    name: fluentd-es
    labels:
      k8s-app: fluentd-logging
  spec:
    selector:
      matchLabels:
        name: fluentd-elasticsearch
    template:
      metadata:
        labels:
          name: fluentd-elasticsearch
      spec:
        tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
        containers:
        - name: c-fluentd
          image: gongcqq/fluentd:v1.9
        nodeSelector:
          disktype: ssd
  ```

- 使用`kubectl apply -f ds-nodeSelector.yaml`命令基于yaml文件创建DaemonSet，然后查看Pod运行情况：

  ![image-20210608193246006](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210609104919.png)

  > **说明**：可以发现，此时虽然已经有一个正在运行的DaemonSet了，但是并没有运行中的Pod。这是因为我们使用nodeSelector通过标签匹配的方式进行固定节点调度了，但是集群中的节点都没有nodeSelector属性下配置的标签(即key值为disktype，value值为ssd的标签)，所以最终导致没有节点可供DaemonSet进行调度，所以就出现没有正在运行的Pod的情况了。

- 下面通过`kubectl label nodes node1 disktype=ssd`命令给node1节点打标签，然后观察效果：

  ![image-20210608194230336](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210609104922.png)

  > **说明**：通过上图可以发现，虽然刚创建DaemonSet的时候由于配置了nodeSelector的原因导致没有节点可以进行Pod的调度，但是只要我们在节点上打上对应的标签，那么打过标签的节点就可以进行Pod的调度了。如果我们把打过标签的节点上对应的标签删除掉的话，那么之前被调度在这个节点上的Pod就会被删除。

**结论**：使用`nodeSelector`属性进行固定节点调度的效果已经通过上面的例子看到了，虽然DaemonSet默认会在集群中的每个能够调度Pod的节点上创建一个Pod副本，但是如果配置了固定节点调度，那么最终只会在满足nodeSelector属性指定条件的节点上创建Pod，如果所有节点都不满足，那就都不会创建Pod。关于节点亲和性中的硬策略和软策略我也进行了测试，这里就只说下结论。如果是使用节点亲和性中的硬策略，那效果和使用nodeSelector类似，只在满足硬策略的节点上创建Pod，如果是使用了节点亲和性的软策略，那么不管权重多大，DaemonSet都还是会在所有能调度的节点上创建一个Pod副本。

###### 7.3.2.2 Pod间亲和性演示

- 新建一个名为ds-podRequired.yaml的文件，内容如下：

  ```yaml
  apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    name: fluentd-es
    labels:
      k8s-app: fluentd-logging
  spec:
    selector:
      matchLabels:
        name: fluentd-elasticsearch
    template:
      metadata:
        labels:
          name: fluentd-elasticsearch
      spec:
        affinity:
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: security
                  operator: In
                  values:
                  - S1
              topologyKey: "kubernetes.io/hostname"
        tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
        containers:
        - name: c-fluentd
          image: gongcqq/fluentd:v1.9
  ```

  > **说明**：以上yaml文件中使用的是Pod间亲和性的硬策略。

- 新建一个名为ds-pod.yaml的文件，内容如下：

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: test-pod
    labels:
      security: S1
  spec:
    containers:
    - name: c-pod
      image: nginx:1.18
      imagePullPolicy: IfNotPresent
    restartPolicy: OnFailure
  ```

  > **说明**：以上`ds-pod.yaml`文件中Pod的标签要和`ds-podRequired.yaml`文件中设置Pod亲和性时所匹配的标签一致，不然就达不到测试的效果了。

- 使用`kubectl apply -f ds-podRequired.yaml`命令基于yaml文件创建DaemonSet：

  ![image-20210608204425537](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210609104930.png) 

- 目前上图中的所有Pod都处于挂起状态，现在使用`kubectl apply -f ds-pod.yaml`命令创建一个满足DaemonSet中亲和性硬策略的Pod：

  ![image-20210608205233983](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210609104937.png) 

- 我们也可以使用`kubectl delete pod test-pod`命令删除手动创建的那个Pod，删除后之前DaemonSet创建的由挂起状态转为运行状态的Pod仍然处于运行状态：

  ![image-20210608205518920](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210609104945.png) 

**结论**：如果使用了硬策略，那么只有当DaemonSet创建的Pod和满足DaemonSet中硬策略规则的Pod处于同一节点(准确地说应该是拓扑域，这里拓扑域用的是"kubernetes.io/hostname"，所以此时拓扑域等价于节点)上时，DaemonSet创建的Pod才会运行，否则都会处于挂起(Pending)状态。关于软策略我也进行了测试，这里就只说下结论。如果是使用了Pod间亲和性的软策略，那么不管权重多大，DaemonSet都还是会在所有能调度的节点上创建一个Pod副本。

##### 7.3.3 DaemonSet Pod的容忍

控制器会自动将以下容忍度添加到DaemonSet的Pod中：

| Toleration Key                         | Effect     | Version | Description                                                  |
| -------------------------------------- | ---------- | ------- | ------------------------------------------------------------ |
| node.kubernetes.io/not-ready           | NoExecute  | 1.13+   | 当出现类似网络断开的情况导致节点问题时，DaemonSet Pod不会被逐出。 |
| node.kubernetes.io/unreachable         | NoExecute  | 1.13+   | 当出现类似于网络断开的情况导致节点问题时，DaemonSet Pod不会被逐出。 |
| node.kubernetes.io/disk-pressure       | NoSchedule | 1.8+    | DaemonSet Pod被默认调度器调度时能够容忍磁盘压力属性。        |
| node.kubernetes.io/memory-pressure     | NoSchedule | 1.8+    | DaemonSet Pod被默认调度器调度时能够容忍内存压力属性。        |
| node.kubernetes.io/unschedulable       | NoSchedule | 1.12+   | DaemonSet Pod能够容忍默认调度器所设置的`unschedulable`属性。 |
| node.kubernetes.io/network-unavailable | NoSchedule | 1.12+   | DaemonSet在使用宿主网络时，能够容忍默认调度器所设置的`network-unavailable`属性。 |

##### 7.3.4 与DaemonSet中Pod的通信

与DaemonSet中的Pod进行通信的几种可能模式如下：

- **Push**：配置DaemonSet中的Pod，将更新发送到另一个服务，例如统计数据库，这些服务没有客户端；
- **NodeIP and Known Port**：DaemonSet中的Pod可以使用`hostPort`，从而可以通过节点IP访问到Pod。客户端能通过某种方法获取节点IP列表，并且基于此也可以获取到相应的端口；
- **DNS**：创建具有相同Pod selector的[无头服务(headless service)](https://kubernetes.io/zh/docs/concepts/services-networking/service/#headless-services)，通过使用`endpoints`资源或从DNS中检索到多个A记录来发现DaemonSet；
- **Service**：创建具有相同Pod selector的服务，并使用该服务随机访问到某个节点上的守护进程(没有办法访问到特定节点)。

##### 7.3.5 DaemonSet的更新和回滚

DaemonSet的更新和回滚与Deployment的更新和回滚的方式是一样的。回滚这里就不再介绍了，具体可以参考无状态的回滚方式，就大致演示一下如何使用`kubectl set image`命令和`kubectl patch`命令对DaemonSet进行更新操作。

- 新增一个ds-roll.yaml文件，内容如下：

  ```yaml
  apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    name: nginx-ds
    labels:
      app: nginx-label
  spec:
    selector:
      matchLabels:
        name: nginx-label
    template:
      metadata:
        labels:
          name: nginx-label
      spec:
        tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
        containers:
        - name: c-nginx
          image: nginx:1.18
          imagePullPolicy: IfNotPresent
  ```

- 使用`kubectl apply -f ds-roll.yaml `命令基于yaml文件创建DaemonSet：

  ![image-20210608215814561](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210609104955.png) 

- 使用以下命令进行滚动更新操作：

  ```shell
  #将镜像版本更新至nginx:1.19
  kubectl set image ds nginx-ds c-nginx=nginx:1.19
  
  #也可以使用如下命令查看实时更新状态
  kubectl rollout status ds nginx-ds
  ```

  ![image-20210608221446407](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210609105000.png) 

- 除了使用`kubectl set image`命令外，还可以使用如下命令进行更新操作：

  ```shell
  #将镜像版本更新至nginx:1.20
  kubectl patch ds nginx-ds --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"nginx:1.20"}]'
  
  #也可以使用如下命令查看实时更新状态
  kubectl rollout status ds nginx-ds
  ```

  ![image-20210608221820380](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210609105018.png) 

#### 7.4 Job

Job会创建一个或者多个Pods，并将继续重试Pods的执行，直到指定数量的Pods成功终止。 随着Pods成功结束，Job跟踪记录成功完成的Pods个数。当数量达到指定的成功个数阈值时，任务(即Job)结束。删除Job的操作会清除所创建的全部Pods。挂起Job的操作会删除Job的所有活跃Pod，直到Job被再次恢复执行。

一个简单的使用场景就是，我们可以创建一个Job对象以便以一种可靠的方式运行某Pod直到完成。当第一个Pod失败或者被删除(比如因为节点硬件失效或者重启)时，Job对象会启动一个新的Pod。

我们也可以使用Job以并行的方式运行多个Pod。

下面进行一个案例演示，该案例主要负责计算π到小数点后2000位。新建一个job.yaml文件，内容如下：

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl:5.32
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 3
```

使用`kubectl apply -f job.yaml`命令基于yaml文件创建Job，然后使用`kubectl get job,pod -o wide`命令查看：

![image-20210609195202694](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154353.png) 

我们可以使用`kubectl logs`命令查看计算出的π到小数点后2000位的结果：

![image-20210609195432971](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154403.png) 

##### 7.4.1 Job的并行性

适合以Job形式来运行的任务主要有三种类型：

1. **非并行的Job：**
   - 通常只启动一个Pod，除非该Pod失败；
   - 当Pod成功终止时，立即视Job为完成状态。

2. **具有确定完成计数的并行Job：**
   - `spec.completions`字段设置为非0的正数值；
   - Job用来代表整个任务，当成功的Pod个数达到`spec.completions`时，Job被视为完成；
   - 当`spec.completionMode="Indexed"`时，每个Pod都会获得一个不同的索引值，介于0和`spec.completions-1`之间。

3. **带工作队列的并行Job：**
   - 不设置`spec.completions`，默认值和`spec.parallelism`相等；
   - 多个Pod之间必须相互协调，或者借助外部服务确定每个Pod要处理哪个工作条目。例如，任一Pod都可以从工作队列中取走最多N个工作条目；
   - 每个Pod都可以独立确定是否其它Pod都已完成，进而确定Job是否完成；
   - 当Job中任何Pod成功终止，不再创建新Pod；
   - 一旦至少1个Pod成功完成，并且所有Pod都已终止，即可宣告Job成功完成；
   - 一旦Pod成功退出，任何其它Pod都不应再对此任务执行任何操作或生成任何输出。所有Pod都应启动退出过程。

**关于以上三种类型Job的说明**：

- 对于非并行的Job，我们可以不设置`completions`和`parallelism`。这两个属性都不设置时，均取默认值1；
- 对于确定完成计数类型的Job，我们应该设置`completions`为所需要的完成个数。我们可以设置`parallelism`，也可以不设置，其默认值为1；
- 对于一个工作队列Job，我们不可以设置`completions`，但要将`parallelism`设置为一个非负整数。

> **注意**：并行性请求(`parallelism`)可以设置为任何非负整数，如果未设置，则默认为1。如果设置为0，则Job相当于启动之后便被暂停，直到此值被增加。

##### 7.4.2 Pod和容器失效的处理

Pod中的容器可能因为多种不同原因失效，例如因为其中的进程退出时返回值非零，或者容器因为超出内存约束而被杀死等等。如果发生这类事件，并且`spec.template.spec.restartPolicy = "OnFailure"`，Pod则继续留在当前节点，但容器会被重新运行。因此，我们的程序需要能够处理在本地被重启的情况，或者设置`restartPolicy = "Never"`。

整个Pod也可能会失败，且原因各不相同。例如，当Pod启动时，节点失效(被升级、被重启、被删除等)或者其中的容器失败而`restartPolicy = "Never"`。 当Pod失败时，Job控制器会启动一个新的Pod。这意味着，我们的应用需要处理在一个新Pod中被重启的情况。尤其是应用需要处理之前运行所产生的临时文件、锁、不完整的输出等问题。

注意，即使我们将`parallelism`设置为1，且将`completions`设置为1，并且设置`restartPolicy = "Never"`，同一程序仍然有可能被启动两次。

如果我们确实将`parallelism`和`completions`都设置为比1大的值，那就有可能同时出现多个Pod运行的情况。为此，我们的Pod也必须能够处理并发性问题。

###### Pod回退失效策略

在有些情形下，我们可能希望Job在经历若干次重试之后直接进入失败状态，因为这很可能意味着遇到了配置错误。我们可以通过设置`spec.backoffLimit`的值来改变Job失败之前的重试次数。失效回退的限制值默认为6。与Job相关的失效的Pod会被Job控制器重建，回退重试时间将会按指数增长(从10秒、20秒到40秒)，最多至6分钟。当Job的Pod被删除时，或者Pod成功时没有其它Pod处于失败状态，失效回退的次数也会被重置为0。

> **说明**：如果我们的Job的`restartPolicy`被设置为"OnFailure"，就要注意运行该Job的容器会在Job到达失效回退次数上限时自动被终止。这会使得调试Job中可执行文件的工作变得非常棘手。建议在调试Job时将`restartPolicy`设置为"Never"，或者使用日志系统来确保失效Jobs的输出不会意外遗失。

##### 7.4.3 Job的终止与清理

Job完成时不会再创建新的Pod，不过已有的Pod也不会被删除。保留这些Pod使得我们可以查看已完成的Pod的日志输出，以便检查错误、警告或者其它诊断性输出。Job完成时Job对象也一样被保留了下来，这样我们就可以查看它的状态。在查看了Job状态之后删除老的Job的操作留给了用户自己。我们可以使用`kubectl delete`命令来删除Job。 当删除Job时，该Job所创建的Pods也会被删除。

默认情况下，Job会持续运行(Running)直到完成(Completed)，除非某个Pod失败(`restartPolicy=Never`)或者某个容器出错退出(`restartPolicy=OnFailure`)。这时，Job基于前述的`spec.backoffLimit`来决定是否以及如何重试。一旦重试次数到达`spec.backoffLimit`所设的上限，Job就会被标记为失败，其中运行的Pods都会被终止。

终止Job的另一种方式是设置一个活跃期限。我们可以为Job的`spec.activeDeadlineSeconds`设置一个秒数值。该值适用于Job的整个生命期，无论Job创建了多少个Pod。一旦Job运行时间达到`activeDeadlineSeconds`的秒值，其所有运行中的Pod都会被终止，并且Job的状态更新为`type: Failed`及`reason: DeadlineExceeded`。

注意，Job的`spec.activeDeadlineSeconds`优先级高于其`spec.backoffLimit`设置。因此，如果一个Job正在重试一个或多个失效的Pod，该Job一旦到达`activeDeadlineSeconds`所设的时限即不再部署额外的Pod，即使其重试次数还未 达到`backoffLimit`所设的限制。

> 还要注意的是，`restartPolicy`对应的是Pod，而不是Job本身。所以一旦Job状态变为`type: Failed`，就不会再发生Job重启的动作。换言之，由`spec.activeDeadlineSeconds`和`spec.backoffLimit`所触发的Job终结机制都会导致Job出现永久性的失败。

##### 7.4.4 backoffLimit和completions

上文提到过backoffLimit指的是失败的重试次数，意思就是，如果Job下的Pod运行失败的话，Job会创建新的Pod继续执行任务(并不是重启之前已经失败的Pod)。另外，还有一个Job的完成数(`completions`)的概念，这个完成数是指Job下成功的Pod的个数，并不是Job下Pod的个数，失败的Pod并不属于完成数。所以如果完成数设置为1，那么Job下执行成功的Pod就只会有一个，但是因为Job有重试次数，所以如果Pod执行失败了，那么失败的Pod是会有多个的，具体有多少个失败的Pod，取决于`backoffLimit`的值，如果不配置`backoffLimit`属性，那么该属性默认值为6。

###### 7.4.4.1 backoffLimit用法演示

下面演示一下`backoffLimit`属性的用法：

- 新建一个yaml文件，内容如下：

  ```yaml
  apiVersion: batch/v1
  kind: Job
  metadata:
    name: test-job
  spec:
    backoffLimit: 5
    template:
      spec:
        containers:
        - name: c-busybox
          image: busybox:1.30
          imagePullPolicy: IfNotPresent
          command: ["/bin/sh", "-c", "cat aaa"]
        restartPolicy: Never
  ```

  > **说明**：以上yaml文件中，由于容器中根本没有`aaa`这个文件，所以`cat aaa`的命令肯定会执行失败，那么Pod就会执行失败，然后yaml文件中`backoffLimit`属性值设置为5，表示Job最终会新建5个重试的Pod。

- 基于yaml文件创建Job，然后观察Pod的个数变化：

  ```shell
  kubectl get job,pod -o wide
  kubectl describe job test-job|grep -A20 "Events"
  ```

  ![image-20210610180312068](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154413.png)

  > **说明**：可以发现，Job每过一段时间就会创建一个Pod去重试，最终创建了5个重试的Pod，每个都重试失败。通过`kubectl describe`命令来查看Job的详细信息，从查询结果的最后一行可以发现，Job已经达到了重试次数，所以不会再重试了，最终Job就以失败告终了。

上文也有提到关于Job的活跃期限(`activeDeadlineSeconds`)，这里也进行下演示。主要演示到了活跃期限后，即便没有达到重试次数，Job也不会继续重试了，也会以失败告终。

- 新建一个yaml文件，内容如下：

  ```yaml
  apiVersion: batch/v1
  kind: Job
  metadata:
    name: test-job
  spec:
    activeDeadlineSeconds: 30
    backoffLimit: 5
    template:
      spec:
        containers:
        - name: c-busybox
          image: busybox:1.30
          imagePullPolicy: IfNotPresent
          command: ["/bin/sh", "-c", "cat aaa"]
        restartPolicy: Never
  ```

  > **说明**：这里将Job的活跃期限(`activeDeadlineSeconds`)设置为了30秒，也就是说，不管成功与否，30秒后该Job就结束了。

- 基于yaml文件创建Job后，观察Pod个数以及Job详情情况：

  ```shell
  kubectl get job,pod -o wide
  kubectl describe job test-job|grep -A20 "Events"
  ```

  ![image-20210610190751212](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154423.png)

  > **说明**：可以发现，虽然重试次数设置为5次，但是由于已经过了Job的活跃期限(`activeDeadlineSeconds`)，所以还没有重试5次Job就已经结束了。

如果设置了活跃期限，而Job创建的Pod在这个期限内也执行完成(状态为`Completed`)了，那就不会有任何问题，这个活跃期限对Job也就没什么影响了，可是如果Pod执行时间较长，达到Job的活跃期限后，Pod还在执行(状态为`Running`)中，那么就会通过删除Job下Pod的方式来结束Pod的运行。演示案例如下：

- 新建一个yaml文件，内容如下：

  ```yaml
  apiVersion: batch/v1
  kind: Job
  metadata:
    name: test-job
  spec:
    activeDeadlineSeconds: 30
    backoffLimit: 5
    template:
      spec:
        containers:
        - name: c-busybox
          image: busybox:1.30
          imagePullPolicy: IfNotPresent
          command: ["/bin/sh", "-c", "sleep 60"]
        restartPolicy: Never
  ```

  > **说明**：在以上yaml文件中，Job的活跃期限设置为30秒，然后通过`sleep 60`命令让Pod执行60秒后再结束，Pod执行过程中是会一直处于`Running`状态的，创建Job后，可以观察一下，看看是不是超过30秒后Pod就被删除了。

- 基于yaml文件创建Job后，观察Pod个数以及Job详情情况：

  ```shell
  kubectl get job,pod -o wide
  kubectl describe job test-job|grep -A20 "Events"
  ```

  ![image-20210610194600412](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154430.png) 

###### 7.4.4.2 completions用法演示

上文也有提到过，可以使用`completions`属性设置Job下成功Pod的个数，如果该属性值大于1的话，默认情况下，Pod会一个个地执行，执行成功一个Pod后，才会去执行下一个，直到成功数达到`completions`属性设置的成功Pod的个数。如果我们想要Pod并发执行的话(也就是多个Pod同时执行)，可以通过设置`parallelism`属性来完成。演示如下：

- 新建一个yaml文件，内容如下：

  ```yaml
  apiVersion: batch/v1
  kind: Job
  metadata:
    name: test-job
  spec:
    completions: 6
    parallelism: 2
    template:
      spec:
        containers:
        - name: c-busybox
          image: busybox:1.30
          imagePullPolicy: IfNotPresent
          command: ["/bin/sh", "-c", "sleep 10"]
        restartPolicy: Never
  ```

  > **说明**：在以上yaml文件中，完成数(`completions`)设置为6，并发数(`parallelism`)设置为2。也就是说，我们要求最终要有6个成功的Pod数，并且最多允许两个Pod同时执行。

- 基于yaml文件创建Job后，观察Pod的完成情况：

  ![image-20210610201800774](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154437.png)

##### 7.4.5 关于job-name标签

当我们在k8s中运行Job后，该Job及其下的所有Pod默认都会被k8s加上一个`job-name`标签。如果同一个namespace下运行多个Job的话，而我们又想筛选出某个Job下的所有Pod，这时候使用`job-name`标签进行筛选就会变得非常方便。下面进行一个案例演示：

- 假设我们目前运行了两个job，一个包含3个成功的Pod，另一个包含2个成功的Pod：

  ![image-20210610205325745](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154446.png) 

- 使用`kubectl get job,pod --show-labels`命令查看相应的标签：

  ![image-20210610205804644](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154459.png) 

- 假设这时候我们想要筛选出名为`test-job`的Job下的所有Pod，可以使用如下命令：

  ```shell
  kubectl get pod -l job-name=test-job -o wide
  ```

  ![image-20210610210056012](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154506.png) 

#### 7.5 CronJob

所谓CronJob，其实就是使用符合Cron表达式格式的方式，周期性地在给定的调度时间执行Job。所有CronJob的定时任务中`schedule`里Cron表达式的时间都是基于kube-controller-manager的时区。

CronJob对于创建周期性的、反复重复的任务很有用，例如执行数据备份或者发送邮件。CronJob也可以用来计划在指定时间来执行的独立任务，例如计划当集群看起来很空闲时执行某个Job。

##### 7.5.1 使用CronJob

下面进行一个简单的案例演示：

- 新建一个名为cronjob.yaml的yaml文件，用于每分钟打印一次当前时间和一个问好信息，内容如下：

  ```yaml
  apiVersion: batch/v1beta1
  kind: CronJob
  metadata:
    name: hello
  spec:
    schedule: "*/1 * * * *"  #这个表达式的意思是每分钟执行一次
    jobTemplate:
      spec:
        template:
          spec:
            containers:
            - name: c-hello
              image: busybox:1.30
              imagePullPolicy: IfNotPresent
              command:
              - /bin/sh
              - -c
              - date; echo Hello from the Kubernetes cluster
            restartPolicy: OnFailure
  ```

  > **注意**：我这边安装的k8s的版本是1.18，该版本中CronJob对应的apiVersion是`batch/v1beta1`，如果k8s的版本是1.21及其以上的话，CronJob对应的apiVersion版本就是`batch/v1`。

- 基于yaml文件创建CronJob，并查看：

  ```shell
  #基于yaml创建CronJob
  kubectl apply -f cronjob.yaml
  
  #cj是CronJob的缩写
  kubectl get cj,job,pod -o wide
  ```

  ![image-20210610214016018](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154512.png) 

- 我们也可以使用`kubectl logs`命令查看某一个Pod的日志情况，看看是否打印当前时间和问好信息：

  ![image-20210610214335258](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154518.png) 

关于Cron表达式的语法，下面简单介绍一下：

```java
┌───────────── 分钟 (0 - 59)
│ ┌───────────── 小时 (0 - 23)
│ │ ┌───────────── 月的某天 (1 - 31)
│ │ │ ┌───────────── 月份 (1 - 12)
│ │ │ │ ┌───────────── 周的某天 (0 - 6) （周日到周一；在某些系统上，7也是星期日）
│ │ │ │ │                                   
│ │ │ │ │
│ │ │ │ │
* * * * *
```

| 条目                   | 描述                         | 相当于    |
| ---------------------- | ---------------------------- | --------- |
| @yearly (or @annually) | 每年 1 月 1 日的午夜运行一次 | 0 0 1 1 * |
| @monthly               | 每月第一天的午夜运行一次     | 0 0 1 * * |
| @weekly                | 每周的周日午夜运行一次       | 0 0 * * 0 |
| @daily (or @midnight)  | 每天午夜运行一次             | 0 0 * * * |
| @hourly                | 每小时的开始一次             | 0 * * * * |

例如，下面这行指出必须在每个星期五的午夜以及每个月 13 号的午夜开始任务：

```shell
0 0 13 * 5
```

要生成CronJob的表达式，我们还可以使用[crontab.guru](https://crontab.guru/)之类的Web工具。

##### 7.5.2 CronJob的限制

CronJob根据其计划编排，在每次该执行任务的时候大概会创建一个Job。之所以说"大概"，是因为在某些情况下，可能会创建两个Job，或者不会创建任何Job。虽然官方也有在试图使这些情况尽量少发生，但也不能完全杜绝。因此，Job应该是*幂等的*。

如果`startingDeadlineSeconds`设置为很大的数值或未设置(默认)，并且`concurrencyPolicy`设置为`Allow`，则作业将始终至少运行一次。

> **注意**：如果`startingDeadlineSeconds`的值低于10秒，CronJob可能无法被调度。这是因为CronJob控制器每10秒钟执行一次检查。

对于每个CronJob，CronJob[控制器(Controller)](https://kubernetes.io/zh/docs/concepts/architecture/controller/)会检查从上一次调度的时间点到现在所错过了的调度次数。如果错过的调度次数超过100次，那么它就不会启动这个Job，并记录这个错误:

```
Cannot determine if job needs to be started. Too many missed start time (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
```

需要注意的是，如果`startingDeadlineSeconds`字段非空，则控制器会统计从`startingDeadlineSeconds`设置的值到现在而不是从上一个计划时间到现在错过了多少次Job。例如，如果`startingDeadlineSeconds`是`200`，则控制器会统计在过去200秒中错过了多少次Job。

如果未能在调度时间内创建CronJob，则计为错过。例如，如果`concurrencyPolicy`被设置为`Forbid`，并且当前有一个调度仍在运行的情况下，试图调度的CronJob将被计算为错过。

假设一个CronJob被设置为从`08:30:00`开始每隔一分钟创建一个新的Job，并且它的`startingDeadlineSeconds`字段未被设置值。那么当错过的调度次数超过100次后，它就不会启动这个Job了。假设`startingDeadlineSeconds`字段值被设置为200秒，比如到`10:11:00`的时候，错过的调度次数已经超过100次了，但Job仍将从`10:12:00`开始。造成这种情况的原因是控制器此时会检查在最近200秒内发生了多少次错过的Job调度(按照每隔一分钟调度一次计算，就是3个错过的调度)，而不是从到现在为止的最后一个调度时间开始。

> **说明**：CronJob仅负责创建与其调度时间相匹配的Job，而Job负责管理其代表的Pod。

##### 7.5.3 并发性策略

`spec.concurrencyPolicy`属性是可选的。它声明了CronJob创建的任务执行时发生重叠如何处理。spec仅能声明下列规则中的一种：

- `Allow`(默认)：CronJob允许并发任务执行；
- `Forbid`：CronJob不允许并发任务执行，如果新任务的执行时间到了而老任务没有执行完，CronJob会忽略新任务的执行；
- `Replace`：如果新任务的执行时间到了而老任务没有执行完，CronJob会用新任务替换当前正在运行的任务。

> **注意**：并发性规则仅适用于同一个CronJob创建的Job。如果存在多个CronJob，它们相应的Job是互不影响的，所以肯定是允许并发执行的。

##### 7.5.4 挂起和历史记录

`spec.suspend`属性用来表示挂起，它也是可选的，默认值是false，如果设置为true，对已经执行过的任务是没有什么影响的，不过会让后续发生的执行都会被挂起。比如本来设置的是每五分钟创建一个Job，一旦CronJob被挂起，就算到了五分钟，也不会再创建Job了。

所谓历史记录，主要是`spec.successfulJobsHistoryLimit`和`spec.failedJobsHistoryLimit`这两个属性。这两个字段用于指定应保留多少已完成和失败的Job，默认设置为3和1。

下面进行一个综合案例演示：

- 新建一个名为test-cronjob.yaml的yaml文件，内容如下：

  ```yaml
  apiVersion: batch/v1beta1
  kind: CronJob
  metadata:
    name: cronjob-123
    labels:
      app: "123"
  spec:
    schedule: "*/1 * * * *"     #表达式 每分钟执行一次
    concurrencyPolicy: Replace   #并发策略
    failedJobsHistoryLimit: 2     #失败job历史显示个数 
    successfulJobsHistoryLimit: 3  #成功job历史显示个数
    suspend: false               #true 挂起 不运行  false 运行
    jobTemplate:
      metadata:
        labels:
          app: "123"
      spec:
        backoffLimit: 2
        completions: 4
        parallelism: 2
        template:
          metadata:
            labels:
              app: "123"
          spec:
            containers:
            - name: c-name
              image: busybox:1.30
              imagePullPolicy: IfNotPresent
              command: ["/bin/sh", "-c", "date"]
            restartPolicy: Never
  ```

- 基于yaml文件创建CronJob并查看：

  ```shell
  kubectl apply -f test-cronjob.yaml
  
  kubectl get cj,job,pod -o wide
  ```

  ![image-20210611131421944](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154534.png)  

- 由于在yaml中设置的是保留3个已完成的Job，所以三分钟后就会展示3个已完成的Job了，为了证明只会展示3个，我们可以再过几分钟后再次查看，会发现最终只会有三个：

  ![image-20210611132526208](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154541.png) 

- 为了演示`failedJobsHistoryLimit`属性是生效的，使用`kubectl edit cj cronjob-123`命令去编辑CronJob中容器的内容，将之前的`command: ["/bin/sh", "-c", "date"]`改为`command: ["/bin/sh", "-c", "cat abc"]`，使得Pod创建失败，从而导致Job失败，然后观察保留失败Job的个数：

  ![image-20210611140412312](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154555.png) 

- 我们也可以使用`kubectl patch cj cronjob-123 -p '{"spec":{"suspend":true}}'`命令将`suspend`的值修改为true，之后再观察Job的创建情况：

  ![image-20210611141100316](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210611154605.png) 

### 8.k8s中的集群调度

#### 8.1 调度的基本概述

1. kube-scheduler是k8s集群的默认调度器，该调度器会将Pod放到合适的Node上，然后对应Node上的Kubelet才能够运行这些Pod；
2. 对每一个新建的Pod或者是未被调度的Pod，kube-scheduler会选择一个最优的Node去运行这个Pod。然而，Pod内的每一个容器对资源都有不同的需求，而且Pod本身也有不同的资源需求。因此，Pod在被调度到Node上之前，根据这些特定的资源调度需求，需要对集群中的Node进行一次过滤；
3. 集群中，满足一个Pod调度请求的所有Node称之为**可调度节点**。如果没有任何一个Node能满足Pod的资源请求，那么这个Pod将一直停留在未调度状态直到调度器能够找到合适的Node；
4. 调度器会先在集群中找到一个Pod的所有可调度节点，然后根据一系列函数对这些可调度节点进行打分，最终选出其中得分最高的Node来运行Pod。之后，调度器将这个调度决定通知给kube-apiserver，这个过程叫做**绑定**；
5. 在做调度决定时需要考虑的因素包括：单独和整体的资源请求、硬件/软件/策略限制、亲和以及反亲和要求、数据局域性、负载间的干扰等等。

#### 8.2 kube-scheduler的调度流程

kube-scheduler给一个Pod做调度选择包含两个步骤：

- 过滤；
- 打分。

过滤阶段会将所有满足Pod调度需求的Node选出来。例如，PodFitsResources过滤函数会检查候选Node的可用资源能否满足Pod的资源请求。在过滤之后，得出一个Node列表，里面包含了所有可调度节点；通常情况下，这个Node列表包含不止一个Node。如果这个列表是空的，代表这个Pod不可调度。

在打分阶段，调度器会为Pod从所有可调度节点中选取一个最合适的Node。根据当前启用的打分规则，调度器会给每一个可调度节点进行打分。

最后，kube-scheduler会将Pod调度到得分最高的Node上。如果存在多个得分最高的Node，kube-scheduler会从中随机选取一个。

#### 8.3 固定节点调度

某些场景下，我们会希望Pod只在固定节点上运行，这时候就需要用到固定节点调度了，下面会介绍两种调度的方式。

##### 8.3.1 通过指定节点名称的方式

通过`kubectl get node`命令查看一下集群中的节点名称，我这边worker node有**node1**和**node2**两个。

```shell
[root@master ~]# kubectl get node
NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   59d   v1.18.0
node1    Ready    <none>   59d   v1.18.0
node2    Ready    <none>   59d   v1.18.0
```

创建一个nodeName.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  nodeName: node1
```

上面创建Pod的文件中，spec属性下的`nodeName`属性就是用来进行固定节点调度的，该属性的值配置的是node1，表示该Pod只会运行在node1节点上。

通过yaml创建Pod：

```shell
kubectl apply -f nodeName.yaml
```

查看Pod的运行状态：

```shell
kubectl get pod -o wide
```

![image-20210510201406729](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210510201745.png) 

由上图可知，该Pod确实是运行在node1节点上了的。不过这里只有一个Pod，具有随机性，并不能说明就是`nodeName`属性起了作用。所以下面通过创建Deployment的方式来进行验证，并把副本数设置成10，如果10个Pod都在node1节点上的话，肯定就是`nodeName`属性起了作用。对应的yaml文件内容如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-nginx
  labels:
    app: deploy-label
spec:
  selector:
    matchLabels:
      app: deploy-label
  replicas: 10
  template:
    metadata:
      labels:
        app: deploy-label
    spec:
      nodeName: node1
      containers:
      - name: nginx
        image: nginx
```

通过yaml文件创建Pod后，使用`kubectl get pod -o wide`命令查看Pod的运行状态：

![image-20210510203719164](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210510203732.png) 

由上图可知，通过Deployment方式创建的10个Pod都运行在了node1节点上，说明确实是`nodeName`属性起了作用。

##### 8.3.2 通过标签选择的方式

这种方式需要用到`nodeSelector`属性。首先需要在指定的节点上打标签，然后再在创建Pod的时候使用`nodeSelector`属性选择这个标签，通过这种方式即可完成固定节点调度。

给node1节点打标签：

```shell
#下面的disktype是自定义的标签的key值，ssd是自定义的标签的value值
kubectl label nodes node1 disktype=ssd
```

通过如下两种方式查看节点标签是否新增成功：

```shell
kubectl get nodes node1 --show-labels
kubectl describe node node1|grep -10 Labels
```

![image-20210510210257990](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210510210300.png) 

节点标签新增成功后，新增nodeSelector.yaml文件，内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd
```

上面创建Pod的文件中，spec属性下的`nodeSelector`属性就是用来进行固定节点调度的，该属性下的disktype就是节点标签的key值，ssd就是节点标签的value值。

通过yaml文件创建Pod：

```shell
kubectl apply -f nodeSelector.yaml
```

查看Pod的运行状态：

```shell
kubectl get pod -o wide
```

![image-20210510211312836](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210510211330.png) 

由上图可知，该Pod确实已经运行在node1节点上了，为了防止是由于偶然性，这里仍然通过创建Deployment的方式来进行验证，副本数设置成5个，对应的yaml文件内容如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-nginx
  labels:
    app: deploy-label
spec:
  selector:
    matchLabels:
      app: deploy-label
  replicas: 5
  template:
    metadata:
      labels:
        app: deploy-label
    spec:
      containers:
      - name: nginx
        image: nginx
      nodeSelector:
        disktype: ssd
```

通过yaml文件创建Pod后，使用`kubectl get pod -o wide`命令查看Pod的运行状态：

![image-20210510212314782](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210510212317.png) 

由上图可知，通过Deployment方式创建的5个Pod都运行在了node1节点上，说明确实是`nodeSelector`属性起了作用。

#### 8.4 亲和性与反亲和性

使用`nodeSelector`属性可以提供一种非常简单的方法来将Pod约束到具有特定标签的节点上。而亲和性与反亲和性功能则极大地扩展了我们可以表达约束的类型，关键的增强点包括：

- 语言更具表现力，提供了更多的匹配规则；
- 规则是具有偏好性的，并不是硬性要求，所以当调度器不能满足要求的时候，Pod仍然可以被调度；
- 亲和性功能包含两种类型的亲和性，即节点的亲和性与反亲和性以及Pod间的亲和性与反亲和性；
- 我们可以使用节点上的Pod的标签来进行约束，而不是使用节点本身的标签。亲和性与反亲和性允许哪些Pod可以或者不可以被放置在一起。

##### 8.4.1 节点的亲和性与反亲和性

节点的亲和性概念上类似于`nodeSelector`，它使我们可以根据节点上的标签来约束Pod可以被调度到哪些节点。

目前有如下两种类型的节点亲和性：

- `requiredDuringSchedulingIgnoredDuringExecution` ：**硬策略**。该策略指定了将Pod调度到一个节点上必须满足的规则(就像`nodeSelector`一样，但使用了更具表现力的语法)；
- `preferredDuringSchedulingIgnoredDuringExecution`：**软策略**。该策略会指定调度器尝试将Pod调度到指定节点上，是具有偏好性的，并不是说Pod就一定要运行到指定的节点上。

> 如果使用硬策略将Pod调度到指定节点上后，节点的相关标签在运行时发生了变更，从而不再满足Pod上的亲和性规则的话，Pod将仍然会继续在该节点上运行。`requiredDuringSchedulingRequiredDuringExecution`这种策略可能会在未来的k8s中出现，使用这种策略会将Pod从不再满足Pod的节点亲和性要求的节点上驱逐。

节点的亲和性是通过Pod.spec的`affinity`字段下的`nodeAffinity`字段进行指定的。

下面是一个使用节点亲和性的Pod案例：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: nginx
```

以上节点亲和性的规则表示，Pod只能放置在具有标签键`e2e-az-name`且标签值为`e2e-az1`或`e2e-az2`的节点上。在满足这些条件的节点中，具有标签键为`another-node-label-key`且标签值为`another-node-label-value`的节点将会被优先使用。

我们可以在上面的例子中看到operator属性值为`In`的操作符的使用。节点亲和性语法支持下面这些操作符：

- `In`：标签值在某个列表中；
- `NotIn`：标签值不在某个列表中；
- `Exists`：某个标签值存在；
- `DoesNotExist`：某个标签值不存在；
- `Gt`：标签值大于某个值(字符串比较)；
- `Lt`：标签值小于某个值(字符串比较)。

> 在这些操作符中，我们可以使用`NotIn`和`DoesNotExist`来实现节点的反亲和性行为。

**注意事项：**

1. 如果同时指定了`nodeSelector`和`nodeAffinity`，两者必须都要满足，才能将Pod调度到候选节点上；
2. 如果指定了多个与`nodeAffinity`类型关联的`nodeSelectorTerms`，只要其中有一个`nodeSelectorTerms`满足的话，Pod就可以调度到指定节点上；
3. 如果指定了多个与`nodeSelectorTerms`关联的`matchExpressions`，则只有当所有的`matchExpressions`满足要求，Pod才会调度到指定节点上；
4. 如果我们修改或删除了Pod所调度到的节点的标签，Pod不会被删除；
5. 软策略中的`weight`属性是权重的意思，取值范围是1-100。如果存在多个软策略的话，对于亲和性规则而言，权重越大越亲和；对于反亲和性规则而言，权重越大越不亲和。

##### 8.4.2 Pod间的亲和性与反亲和性

**基本概念和注意事项：**

- Pod间的亲和性与反亲和性同样有两种策略，即硬策略`requiredDuringSchedulingIgnoredDuringExecution`和软策略`preferredDuringSchedulingIgnoredDuringExecution`；

- Pod间亲和性与反亲和性使我们可以基于已经在节点上运行的Pod的标签来约束Pod可以调度到的节点，而不是基于节点上的标签；
- Pod间亲和性与反亲和性需要大量的处理，这可能会显著减慢大规模集群中的调度，所以不建议在超过数百个节点的集群中使用它们；
- Pod反亲和性需要对节点进行一致的标记，即集群中的每个节点必须具有适当的标签能够匹配`topologyKey`。如果某些或所有节点缺少指定的`topologyKey`标签，可能会导致意外行为；
- Pod间的亲和性通过spec属性中`affinity`字段下的`podAffinity`字段进行指定；而Pod间的反亲和性是通过spec中`affinity`字段下的`podAntiAffinity`字段进行指定；
- Pod间的亲和性与反亲和性的合法操作符有`In`，`NotIn`，`Exists`，`DoesNotExist`。

**Pod间亲和性与反亲和性的基本案例：**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: "kubernetes.io/hostname"
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: "kubernetes.io/hostname"
  containers:
  - name: with-pod-affinity
    image: nginx
```

在上面的案例中，分别定义了一条Pod亲和性规则和一条Pod反亲和性规则。

在这个案例中，`podAffinity`配置的是硬策略，而`podAntiAffinity`配置的是软策略。案例中`podAffinity`属性下配置的亲和性规则是指将该Pod调度到`topologyKey`属性表示的拓扑域上，这个区域上至少需要存在一个正在运行的包含键为security值为S1的标签的Pod。案例中`podAntiAffinity`属性下配置的反亲和性规则是指，如果`topologyKey`属性表示的拓扑域上存在正在运行的包含键为security值为S2的标签的Pod，那么该Pod将不会被调度到这个区域上的节点中。

**关于topologyKey属性的解释说明：**

- `topologyKey`属性是用于表示一个拓扑域的，并不是表示一个节点，一个拓扑域可以包含多个节点；
- `topologyKey`后跟的是节点标签的key值，假设一个k8s集群中有A、B、C三个节点，它们都拥有名为**env**这个相同key值的节点标签，但是标签的value值分别是**dev**、**test**、**prod**，那么这时候拓扑域和集群中的节点就是一对一的关系，这时候将Pod调度到拓扑域上就是指将Pod调度到k8s集群的某一节点上。如果A、B两个节点的标签的value值都是**dev**，而C节点的标签的value值是**test**的话，拓扑域和节点就是一对多的关系了，这时候的A、B节点是处于同一拓扑域的，C节点是处于另一个拓扑域的；
- Pod间的亲和性规则并不是说两个Pod一定会处于同一节点上，只有当拓扑域和节点是一对一关系的时候才会被调度到同一节点上，如果拓扑域中包含多个节点的话，配置了Pod间亲和性规则后，k8s只会将该Pod调度到和另一个Pod相同拓扑域中的某一个节点上，并不一定就和另一个Pod是同一个节点，但一定和这个Pod处于同一拓扑域；
- 以上案例中，`topologyKey`属性对应的值是"kubernetes.io/hostname"，这个是k8s给集群中所有的节点默认加的标签的key值，value值就是每个节点的节点名，集群中节点的节点名是不会重复的，所以案例中的拓扑域和节点之间就是一对一的关系。

原则上，topologyKey可以是任何合法的标签键。然而，出于性能和安全原因，topologyKey有如下限制：

1. 对于亲和性与`requiredDuringSchedulingIgnoredDuringExecution`要求的Pod反亲和性，`topologyKey`不允许为空；
2. 对于在Pod反亲和性中使用了`requiredDuringSchedulingIgnoredDuringExecution`硬策略的情况，引入了准入控制器`LimitPodHardAntiAffinityTopology`来限制`topologyKey`不为"kubernetes.io/hostname"。如果我们想使它可用于自定义拓扑结构，则必须修改准入控制器或者禁用它；
3. 对于`preferredDuringSchedulingIgnoredDuringExecution`要求的Pod反亲和性，空的`topologyKey`的意思就是"所有拓扑结构"，这里的"所有拓扑结构"就是`kubernetes.io/hostname`、`topology.kubernetes.io/zone`和`topology.kubernetes.io/region`的组合。

**Pod间亲和性与反亲和性的进阶案例：**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  selector:
    matchLabels:
      app: store
  replicas: 2
  template:
    metadata:
      labels:
        app: store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: redis-server
        image: redis:6.0
```

以上案例是使用Pod间的反亲和性规则使创建的两个Pod不在同一节点，创建Pod后效果如下：

![image-20210511231937158](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210511232617.png) 

由上图可知，通过Deployment创建的两个副本Pod确实都不在同一节点上，我这边集群中worker节点只有两个，所以这里副本数才设置成2，上述案例设置了反亲和性规则的硬策略，如果副本数大于worker节点的个数的话，就会出现由于节点不够导致多出的Pod一直处于`Pending`状态的情况。假设我把副本数设置为5的话，运行结果就是下面这样：

![image-20210511233328432](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210511233331.png)  

**Pod间亲和性与反亲和性的高级案例：**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  selector:
    matchLabels:
      app: web-store
  replicas: 2
  template:
    metadata:
      labels:
        app: web-store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web-store
            topologyKey: "kubernetes.io/hostname"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: web-app
        image: nginx:1.20
```

这个案例是要结合上面那个进阶案例进行使用的，所以在使用这个yaml文件创建Deployment之前，要保证前面那个进阶案例对应的yaml文件已经创建成功，下面大致说一下这个yaml文件创建后达到的效果。

前面进阶案例的yaml文件中使用反亲和性规则使得创建的两个Pod分配在了不同的节点上，现在这个yaml文件中也使用反亲和性规则使得创建的两个Pod分配在不同的节点上，并且还对上面进阶案例中创建的Pod使用了亲和性规则，所以最终会出现两个节点上都分别有一个进阶案例中创建的Pod和这个案例中创建的Pod，具体截图如下：

![image-20210511235556468](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210511235559.png) 

#### 8.5 污点和容忍

节点亲和性是Pod的一种属性，它可以使Pod被吸引到一类特定的节点上。这可能是出于一种偏好，也可能是硬性要求。污点(Taint)则相反，它使节点能够排斥一类特定的Pod。

容忍(Tolerations)是应用于Pod上的，允许(但并不要求)Pod调度到带有与之匹配的污点的节点上。

污点和容忍(Tolerations)相互配合，可以用来避免Pod被分配到不合适的节点上。每个节点上都可以应用一个或多个污点，这表示对于那些不能容忍这些污点的Pod，是不会被该节点接受的。

##### 8.5.1 污点(Taint)

可以使用[kubectl taint](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#taint)命令给节点增加污点，节点被添加上污点之后就和Pod之间存在了一种相斥的关系，可以让节点拒绝Pod的调度执行，甚至将该节点上已经存在的Pod驱逐出去。

每个污点的组成如下：

```shell
key=value:effect
```

上面的key是污点的标签的键，value是污点的标签的值，其中value值可以为空，effect用来描述污点的作用，它支持如下三个选项：

- `NoSchedule`：表示k8s将不会把Pod调度到具有该污点的节点上；
- `PreferNoSchedule`：表示k8s将尽量避免把Pod调度到具有该污点的节点上；
- `NoExecute`：表示k8s将不会把Pod调度到具有该污点的节点上，同时会将该节点上已经存在的Pod驱逐出去。

 污点的新增、查看和删除：

```shell
# 新增污点。其中key值为check，value值为gongsl，effect为NoExecute
kubectl taint nodes node1 check=gongsl:NoExecute
# 查看污点
kubectl describe nodes node1|grep Taint
# 删除污点。其中key值为check
kubectl taint nodes node1 check:NoExecute-
```

下面进行一个案例演示：

![image-20210512143544365](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210512143551.png) 

当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括：

1. `node.kubernetes.io/not-ready`：节点未准备好。这相当于节点状态`Ready`的值为`False`；
2. `node.kubernetes.io/unreachable`：节点控制器访问不到节点。这相当于节点状态`Ready`的值为`Unknown`；
3. `node.kubernetes.io/out-of-disk`：节点磁盘耗尽；
4. `node.kubernetes.io/memory-pressure`：节点存在内存压力；
5. `node.kubernetes.io/disk-pressure`：节点存在磁盘压力；
6. `node.kubernetes.io/network-unavailable`：节点网络不可用；
7. `node.kubernetes.io/unschedulable`: 节点不可调度；
8. `node.cloudprovider.kubernetes.io/uninitialized`：如果kubelet启动时指定了一个"外部"云平台驱动，它将给当前节点添加一个污点将其标志为不可用。在cloud-controller-manager的一个控制器初始化这个节点后，kubelet将删除这个污点。

##### 8.5.2 容忍(Tolerations)

设置了污点的节点将根据taint的effect(NoSchedule、PreferNoSchedule、NoExecute)和Pod之间产生互斥的关系，Pod将在一定程度上不会被调度到该节点上。但我们可以在Pod上设置容忍(Tolerations)，设置了容忍的Pod将可以容忍污点的存在，可以被调度到存在污点的的节点上。

在k8s中，是使用Pod中spec属性下的`tolerations`属性进行Pod容忍的设置的。下面是Pod容忍可能涉及的相关设置：

```yaml
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  tolerationSeconds: 3600
- key: "key2"
  operator: "Exists"
  effect: "NoSchedule"
```

- 上面key，vaule，effect这些属性的属性值要与节点上设置的污点组成部分中的`key=value:effect`保持一致；
- 上面的operator属性的值默认为`Equal`，此时的value属性值需要和污点的value值相对应，operator属性的值还可以是`Exists`，此时将会忽略value属性，所以这时value属性是不用写的；
- 上面的tolerationSeconds属性用于描述当Pod需要被驱逐时仍可以在节点上继续保留运行的时间，单位是秒，也就是说多少秒后Pod才会被驱逐。

**特殊情况说明：**

1. 如果一个容忍度的key为空且operator为`Exists`，表示这个容忍度与任意的key、value和effect都匹配，即这个容忍度能容忍任意污点(taint)。

```yaml
tolerations:
- operator: "Exists"
```

2. 当不指定effect值时，表示容忍指定键名对应的所有污点效果选项。

```yaml
tolerations:
- key: "key1"
  operator: "Exists"
```

**关于Pod容忍的案例：**

先新建两个Pod，对应的yaml文件如下所示：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx1
spec:
  containers:
  - name: nginx
    image: nginx:1.20
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: "key1"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 120
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx2
spec:
  containers:
  - name: nginx
    image: nginx:1.20
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: "key2"
    operator: "Exists"
    effect: "NoExecute"
```

![image-20210513140837642](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210513140858.png) 

既然都运行在了node1节点上，那就给node1节点打污点，演示驱逐效果：

```shell
#执行如下命令，给node1节点打一个污点，这里用的是只有key值没有value值的污点
kubectl taint nodes node1 key1:NoExecute
```

节点打污点后，由于名为pod-nginx2的Pod没有匹配键值为key1的容忍，所以该Pod会被立即驱逐，名为pod-nginx1的Pod虽然有匹配键值为key1的容忍，但是由于将`tolerationSeconds`设置为了120，所以120秒后，该Pod也会被驱逐。

![image-20210513142840535](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210513142843.png) 

我们可以给一个节点添加多个污点，也可以给一个Pod添加多个容忍。k8s处理多个污点和容忍的过程就像一个过滤器：从一个节点的所有污点开始遍历，过滤掉那些Pod中存在与之相匹配的容忍度的污点。余下未被过滤的污点的effect值决定了Pod是否会被分配到该节点，特别是以下情况：

- 如果未被过滤的污点中存在至少一个effect值为`NoSchedule`的污点，则k8s不会将分配到该节点；
- 如果未被过滤的污点中不存在effect值为`NoSchedule`的污点，但是存在effect值为`PreferNoSchedule`的污点，则Kubernetes会尝试不将Pod分配到该节点；
- 如果未被过滤的污点中存在至少一个effect值为`NoExecute`的污点，则Kubernetes不会将Pod分配到该节点(如果 Pod还未在节点上运行)，或者将Pod从该节点驱逐(如果Pod已经在节点上运行)。

例如，假设我们给一个节点添加了如下污点：

```shell
kubectl taint nodes node1 key1=value1:NoSchedule
kubectl taint nodes node1 key1=value1:NoExecute
kubectl taint nodes node1 key2=value2:NoSchedule
```

假定有一个Pod，它有两个容忍度：

```shell
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
```

在这种情况下，上述Pod不会被分配到上述节点，因为其没有容忍度和第三个污点相匹配。但是如果在给节点添加上述污点之前，该Pod已经在上述节点运行，那么它还可以继续运行在该节点上，因为不能和上述Pod相匹配的上述节点的第三个污点的effect值并不是NoExecute，所以不会驱逐已经运行在节点上的Pod。

前文提到过污点的effect值`NoExecute`会影响已经在节点上运行的Pod：

- 如果Pod不能忍受effect值为`NoExecute`的污点，那么Pod将马上被驱逐；
- 如果Pod能够忍受effect值为`NoExecute`的污点，但是在容忍度定义中没有指定`tolerationSeconds`，则Pod还会一直在这个节点上运行；
- 如果Pod能够忍受effect值为`NoExecute`的污点，而且指定了`tolerationSeconds`，则Pod还能在这个节点上继续运行这个指定的时间长度。

**注意事项：**

- k8s会自动给Pod添加一个key为`node.kubernetes.io/not-ready`的容忍度并配置 `tolerationSeconds=300`，除非用户提供的Pod配置中已经已存在了key为`node.kubernetes.io/not-ready`的容忍度；

- k8s也会给Pod添加一个key为`node.kubernetes.io/unreachable`的容忍度并配置`tolerationSeconds=300`，除非用户提供的Pod配置中已经已存在了key为`node.kubernetes.io/unreachable`的容忍度；

- [DaemonSet](https://kubernetes.io/zh/docs/concepts/workloads/controllers/daemonset/)中的Pod被创建时，针对以下污点自动添加的`NoExecute`的容忍度将不会指定`tolerationSeconds`：

  - `node.kubernetes.io/unreachable`
  - `node.kubernetes.io/not-ready`

  这保证了出现上述问题时DaemonSet中的Pod永远不会被驱逐。

DaemonSet控制器自动为所有守护进程添加如下`NoSchedule`容忍度以防DaemonSet崩溃：

- `node.kubernetes.io/memory-pressure`
- `node.kubernetes.io/disk-pressure`
- `node.kubernetes.io/out-of-disk`(*只适合关键Pod*)
- `node.kubernetes.io/unschedulable`(1.10或更高版本)
- `node.kubernetes.io/network-unavailable`(*只适合主机网络配置*)

添加上述容忍度确保了向后兼容，我们也可以选择自由向DaemonSet添加容忍度。

### 9.k8s核心技术-Service

Service是Kubernetes最核心的概念，通过创建Service，可以为一组具有相同功能的容器应用提供一个统一的入口地 址，并且将请求负载分发到后端的各个容器应用上。通常是通过标签选择的方式来确定能够被Service访问到的一组Pod。

虽然Service能够提供负载均衡的能力，但是只支持四层负载，不支持七层负载。所谓四层负载，就是能够基于IP地址和端口的方式来实现负载均衡，而七层负载还能够通过主机名或域名的方式实现负载均衡。虽然Service默认不支持七层负载，但是可以通过ingress来实现七层负载的功能。

#### 9.1 Service的入门案例

- 新建一个名为`svc-myapp.yaml`的yaml文件，内容如下：

  ```yaml
  apiVersion: v1
  kind: Namespace
  metadata:
    name: gsl-test
  ---
  apiVersion: v1
  kind: Service
  metadata:
    namespace: gsl-test
    name: my-service
  spec:
    selector:
      app: myapp
    ports:
      - protocol: TCP
        port: 81   #这个是svc负载的端口，一般和targetPort保持一致，案例中为了进行区分才故意不一致的
        targetPort: 80   #这个是目标端口，对应的就是svc负载下Pod里面应用开放出来的端口
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    namespace: gsl-test
    name: my-deploy
  spec:
    replicas: 5
    selector:
      matchLabels:
        app: myapp
    template:
      metadata:
        labels:
          app: myapp
      spec:
        containers:
        - name: c-app
          image: gongcqq/myapp:1.0
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 80
  ```

  > **说明**：以上yaml文件中，创建Service的时候，svc的类型不写的话，默认使用的是ClusterIP。

- 使用`kubectl apply -f svc-myapp.yaml`命令基于yaml文件创建Service和Deployment，然后查看运行情况：

  ![image-20210615205804806](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210616210240.png)

- 通过上图可知，svc的ClusterIP是`10.103.94.253`，使用`curl 10.103.94.253:81/hostname`命令获取svc负载下Pod的主机名，通过主机名可以发现，svc默认是轮询策略，同一个命令每次获取到的主机名都不一样，但肯定都是上图中和svc同一标签的Pod的主机名：

  ![image-20210615210535634](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210616210247.png)

  **三点说明：**

  - 如果我们执行curl命令时，速度很慢，可以先执行`ethtool -K flannel.1 tx-checksum-ip-generic off`命令，然后再次执行curl命令，速度就快了；
  - 我们还可以删除deployment创建的Pod，然后会自动拉起新的Pod，再使用curl命令的时候，轮询获取到的主机名就会是新的Pod的主机名了；
  - 我们可以使用`kubectl describe svc my-service -n gsl-test`命令查看svc的详细信息，在详细信息里面我们可以发现一个**Endpoints**字段，该字段的值就是svc负载下所有Pod的IP和端口的集合，而且只要Pod的IP和端口有更新，Endpoints字段的值就会被更新。我们也可以使用`kubectl get`或者`kubectl describe`命令直接查询Endpoints的基本信息和详情，Endpoints的名称和svc的名称默认是一样的，本例中就是my-service，比如`kubectl get Endpoints my-service -n gsl-test`。

- 我们也可以将代理模式改为"ipvs"，然后通过ipvsadm命令查看具体的负载情况。我们可以使用如下命令查看目前是否是"ipvs"模式，如果结果为空，表示不是ipvs模式，命令如下：

  ```shell
  kubectl get configmap kube-proxy -n kube-system -o yaml|grep "mode"
  ```

  ![image-20210615213950836](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210616210254.png) 

- 如果不是，可以使用`kubectl edit configmap kube-proxy -n kube-system`命令改成"ipvs"后保存退出：

  ![image-20210615214153002](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210616210258.png) 

- 然后查看并删除现有的kube-proxy，删除后k8s会自动拉起新的kube-proxy，这时我们进行的修改就生效了：

  ```shell
  kubectl get pod -n kube-system -o wide|grep kube-proxy
  ```

  ![image-20210615164808669](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210616210306.png) 

- 接下来我们使用`yum install -y ipvsadm`命令下载ipvsadm插件，下载完成之后可以使用`ipvsadm -Ln`命令测试是否可用，出现类似下面这样的效果就说明是可用的：

  ![image-20210615170639736](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210616210312.png) 

- 使用`ipvsadm -Ln|grep -10 "10.103.94.253"`命令(**10.103.94.253**是svc的ClusterIP)查看svc具体的负载情况，如下所示：

  ![image-20210615215042433](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210616210319.png) 

> **注意**：可能是环境问题，我这边改为ipvs模式后，删除Pod后，重新拉起的Pod的IP不能及时同步到ipvs的负载列表里面，导致使用`ipvsadm -Ln`命令查出来的svc负载中的Pod的IP还是之前的，所以只能暂时先关闭ipvs模式了。

#### 9.2 没有选择算符的Service

没有标签就说明svc没法自动匹配Pod，也不会自动Endpoint对象，当我们使用`kubectl describe svc`命令查看svc的详情的时候，Endpoints字段也会为空。

- 新建一个名为`svc-test.yaml`的yaml文件，文件中主要是创建一个没有标签的Service和两个Pod：

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: svc-test
  spec:
    ports:
      - protocol: TCP
        port: 80
        targetPort: 80
  ---
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-test1
  spec:
    containers:
    - name: c-app
      image: gongcqq/myapp:1.0
      imagePullPolicy: IfNotPresent
  ---
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-test2
  spec:
    containers:
    - name: c-app
      image: gongcqq/myapp:1.0
      imagePullPolicy: IfNotPresent
  ```

- 使用`kubectl apply -f svc-test.yaml`命令基于yaml文件创建Service和Pod；

- 使用如下命令进行详情查看：

  ```shell
  kubectl get all -o wide
  kubectl describe svc svc-test
  kubectl get Endpoints svc-test
  ```

  ![image-20210616112214994](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210616210336.png) 

- 新建一个`endpoints.yaml`文件，内容如下：

  ```yaml
  apiVersion: v1
  kind: Endpoints
  metadata:
    name: svc-test  #名称要和上面svc的名称保持一致
  subsets:
    - addresses:
        - ip: 10.244.2.28  #这个IP就是上面名为pod-test1的Pod的IP，下面端口号同理
      ports:
        - port: 80
    - addresses:
        - ip: 10.244.1.26  #这个IP就是上面名为pod-test2的Pod的IP，下面端口号同理
      ports:
        - port: 80
  ```

- 使用`kubectl apply -f endpoints.yaml`命令基于yaml创建一个Endpoints对象，然后再次查看详情：

  ![image-20210616120214248](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210616210346.png) 

> **注意**：不使用选择算符的话，通过手动创建Endpoints的方式也能实现负载，不过负载下的Pod一旦被删除，svc负载下Pod的地址并不会被删除，没法达到同步更新，所以非特殊场景，这种不使用选择算符的方式还是要慎用的。

#### 9.3 Endpoints和EndpointSlice

关于Endpoints的创建，上文已经介绍到了，这里就不再赘述了。主要有一个特殊情况，就是如果某个Endpoints资源中包含的端点个数超过了 1000 个，则 Kubernetes v1.21版本(及更新的版本)的集群将会为该 Endpoints 添加注解`endpoints.kubernetes.io/over-capacity: warning`，这一注解表明所影响到的Endpoints对象已经超出容量。

另外，还有一个名为`EndpointSlice`的Endpoint切片，它是一种API资源，可以为 Endpoint 提供更加可扩展的替代方案。尽管从概念上讲它与Endpoint非常相似，但EndpointSlice允许跨多个资源分布网络端点。默认情况下，一旦到达100个Endpoint，EndpointSlice将被视为“已满”，届时将创建其他EndpointSlice来存储任何其他Endpoint。

EndpointSlice提供了附加的属性和功能，这些属性和功能在[EndpointSlices](https://kubernetes.io/zh/docs/concepts/services-networking/endpoint-slices/)中有详细描述。

#### 9.4 虚拟IP和Service代理

在Kubernetes集群中，每个Node运行一个`kube-proxy`进程。`kube-proxy`负责为Service实现了一种VIP(虚拟IP)的形式，而不是[`ExternalName`](https://kubernetes.io/zh/docs/concepts/services-networking/service/#externalname)的形式。

为什么Kubernetes要依赖代理将入站流量转发到后端，而不采用其他方法呢？例如，为什么不配置具有多个A值(或IPv6为AAAA)的DNS记录，并依靠轮询名称解析呢？

使用服务代理而不使用DNS轮询主要有以下几个原因：

- DNS实现的历史由来已久，它不遵守记录TTL，并且在名称查找结果到期后对其进行缓存；
- 有些应用程序仅执行一次DNS查找，并无限期地缓存结果；
- 即使应用和库进行了适当的重新解析，DNS记录上的TTL值低或为零也可能给DNS带来高负载，从而使管理变得困难。

##### 9.4.1 userspace代理模式

这种模式，`kube-proxy`会监控k8s控制平面对Service对象和Endpoints对象的添加和移除操作。对每个Service，它会在本地Node上打开一个端口(随机选择)。任何连接到"代理端口"的请求，都会被代理到Service的后端`Pods`中的某个上面(如`Endpoints`所报告的一样)。使用哪个后端Pod，是kube-proxy基于`SessionAffinity`来确定的。

最后，它会配置iptables防火墙规则，捕获到达该Service的`ClusterIP`(这个是虚拟IP)和`Port`的请求，并重定向到代理端口，代理端口再代理请求到后端Pod。

默认情况下，userspace模式下的kube-proxy通过轮询算法选择后端。

![image-20210616172804736](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210616210358.png) 

> **图解说明**：通过上图我们可以发现，客户端的Pod如果想要访问到服务端的Pod，不管是访问本机的还是远程的，都需要先通过防火墙然后再通过kube-proxy进行一次代理才能够访问的到，这就会导致kube-proxy的压力比较大，而且kube-apiserver也会监控kube-proxy以便进行服务的更新以及端点的维护等操作。

##### 9.4.2 iptables代理模式

这种模式，`kube-proxy`会监控k8s控制平面对Service对象和Endpoints对象的添加和移除操作。对每个Service，它会配置iptables规则，从而捕获到达该Service的`ClusterIP`和`Port`的请求，进而将请求重定向到Service的一组后端中的某个Pod上面。对于每个Endpoints对象，它也会配置iptables规则，这个规则会选择一个后端组合。

默认的策略是，kube-proxy在iptables模式下随机选择一个后端。

使用iptables处理流量具有较低的系统开销，因为流量由Linux netfilter处理，而无需在用户空间和内核空间之间切换，这种方法也可能更可靠。

如果kube-proxy在iptables模式下运行，并且所选的第一个Pod没有响应，则连接失败。这与userspace模式不同：在这种情况下，kube-proxy将检测到与第一个Pod的连接已失败，并会自动使用其他后端Pod重试。

我们可以使用Pod的 [就绪探测器](https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/#container-probes) 验证后端Pod可以正常工作，以便iptables模式下的 kube-proxy 仅看到测试正常的后端，这样做意味着我们可以避免将流量通过kube-proxy发送到已知已失败的Pod上。

![image-20210616181254242](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210616210426.png) 

> **图解说明**：由上图可以发现，客户端Pod所有的访问都是直接由防火墙去完成，而不用再通过kube-proxy，这样一来访问的速度就会变快，而且kube-proxy的稳定性也会提高，同时压力会减小，所以iptables代理模式后来慢慢取代了userspace代理模式。

##### 9.4.3 IPVS代理模式

在`ipvs`模式下，kube-proxy监控Kubernetes服务和端点，调用`netlink`接口相应地创建IPVS规则，并定期将IPVS规则与Kubernetes服务和端点同步。该控制循环可确保IPVS状态与所需状态匹配。访问服务时，IPVS将流量重定向到后端的Pod之一。

IPVS代理模式基于类似于iptables模式的netfilter挂钩函数，但是使用哈希表作为基础数据结构，并且在内核空间中工作。这意味着，与iptables模式下的kube-proxy相比，IPVS模式下的kube-proxy重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。与其他代理模式相比，IPVS模式还支持更高的网络流量吞吐量。

IPVS 提供了更多选项来平衡后端Pod的流量，这些是：

- `rr`：轮询(Round-Robin)；
- `lc`：最少链接(Least Connection)，即打开链接数量最少者优先；
- `dh`：目标地址哈希(Destination Hashing)；
- `sh`：源地址哈希(Source Hashing)；
- `sed`：最短预期延迟(Shortest Expected Delay)；
- `nq`：从不排队(Never Queue)。 

> **说明**：要在IPVS模式下运行kube-proxy，必须在启动kube-proxy之前使IPVS在节点上可用。当kube-proxy以IPVS代理模式启动时，它将验证IPVS内核模块是否可用。如果未检测到IPVS内核模块，则kube-proxy将退回到以iptables代理模式运行。

![image-20210616200910848](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210616210434.png) 

**注意**：如果要确保每次都将来自特定客户端的连接传递到同一Pod，则可以通过将`service.spec.sessionAffinity`设置为"ClientIP"(默认值是"None")的方式，来基于客户端的IP地址选择会话关联。我们还可以通过适当设置service的`service.spec.sessionAffinityConfig.clientIP.timeoutSeconds`来设置最大会话停留时间(默认值为10800秒，即3小时)。

#### 9.5 多端口的Service

对于某些服务，我们可能需要公开多个端口。Kubernetes允许我们在Service对象上配置多个端口定义。如何为服务使用了多个端口，那么必须设置所有端口的名称，以便使它们不会产生歧义。例如：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
    - name: https
      protocol: TCP
      port: 443
      targetPort: 9377
```

> **说明：**与一般的Kubernetes名称一样，端口名称只能包含小写字母、数字、字符和`-`。端口名称还必须以字母、数字、字符开头和结尾。例如，名称`123-abc`和`web`有效，但是`123_abc`和`-web`无效。

#### 9.6 自定义Service的集群IP

在`Service`创建的请求中，可以通过设置`spec.clusterIP`字段来指定自己的集群IP地址。用户自己选择的IP地址必须合法，并且这个IP地址在 `service-cluster-ip-range` CIDR 范围内，这对API服务器来说是通过一个标识来指定的。如果IP地址不合法，API服务器会返回HTTP状态码422，表示值不合法。

下面进行一个简单的案例演示：

- 新建一个yaml文件，内容如下：

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: my-service
  spec:
    clusterIP: 10.96.11.12  #这个是自定义的IP地址
    selector:
      app: myapp
    ports:
      - protocol: TCP
        port: 80
        targetPort: 80
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: my-deploy
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: myapp
    template:
      metadata:
        labels:
          app: myapp
      spec:
        containers:
        - name: c-app
          image: gongcqq/myapp:1.0
          imagePullPolicy: IfNotPresent
  ```

- 使用`kubectl apply -f`命令基于yaml文件创建Service和Deployment；

- 使用`kubectl get pod,svc -o wide`命令进行查看：

  ![image-20210617161258097](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210619175753.png) 

#### 9.7 无头服务(Headless Services)

有时不需要或不想要负载均衡以及单独的Service IP。遇到这种情况，可以通过指定`spec.clusterIP`的值为`"None"`来创建Headless Service。

我们可以使用无头服务与其他服务发现机制进行交互，而不必与Kubernetes的实现捆绑在一起。对这无头服务并不会分配Cluster IP，kube-proxy不会处理它们，而且平台也不会为它们进行负载均衡和路由。DNS如何实现自动配置，依赖于Service是否定义了选择算符。

对定义了选择算符的无头服务，Endpoint控制器在API中创建了Endpoints记录，并且修改DNS配置返回 A 记录(IP地址)，通过这个地址直接到达`Service`的后端Pod上。

对没有定义选择算符的无头服务，Endpoint控制器不会创建`Endpoints`记录，然而DNS系统也会查找和配置：

- 对于`ExternalName`类型的服务，查找其CNAME记录；
- 对所有其他类型的服务，查找与Service名称相同的任何`Endpoints`的记录。

下面针对无头服务进行一个简单的案例演示：

- 新建一个yaml文件，内容如下：

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: my-service
  spec:
    clusterIP: "None"   #这里值为None就表示创建的是无头服务
    selector:
      app: myapp
    ports:
      - port: 80
        targetPort: 80
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: my-deploy
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: myapp
    template:
      metadata:
        labels:
          app: myapp
      spec:
        containers:
        - name: c-app
          image: gongcqq/myapp:1.0
          imagePullPolicy: IfNotPresent
  ```

- 基于yaml文件创建对应资源，并查看运行情况：

  ![image-20210620114241054](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210620121730.png) 

- 当我们的svc被创建后，会被写入到coredns里面去，可以通过如下命令查看coredns的运行情况：

  ```shell
  kubectl get pod -n kube-system -o wide|grep "coredns"
  ```

  ![image-20210620114557189](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210620121738.png)

  > **说明**：写入coredns的svc的名称格式为：`$(serviceName).$(namespace).svc.cluster.local`，所以在该案例中，具体名称就是：`my-service.default.svc.cluster.local`。

- 我们可以通过coredns在集群主机中的IP来解析以上写入到coredns中的svc的域名，这里主要是使用dig命令，如果没有该命令，可以使用`yum -y install bind-utils`命令进行下载，具体的解析命令如下：

  ```shell
  #我这边集群中运行了两个coredns，所以会有两个IP，随机选一个IP进行解析即可
  dig -t A my-service.default.svc.cluster.local @10.244.0.12
  ```

  ![image-20210620120350007](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210620121751.png)

  > **说明**：通过上图可以发现，虽然无头服务的svc没有自己的IP了，但是其域名仍然映射着它下面的所有Pod，所以我们通过域名的方式，依然可以访问到其下的每个Pod。

#### 9.8 Service的类型

Kubernetes默认的Service类型是`ClusterIP`，我们也可以指定所需要的Service类型，主要有以下四种类型：

- `ClusterIP`：通过集群的内部IP暴露服务，选择该值时服务只能够在集群内部访问；
- `NodePort`：通过每个节点上的IP和静态端口暴露服务。NodePort服务会路由到自动创建的ClusterIP服务，通过请求`<节点 IP>:<节点端口>`，我们可以从集群的外部访问一个NodePort服务；
- `LoadBalancer`：该方式是使用云提供商(一般是收费的)的负载均衡器向外部暴露服务，外部负载均衡器可以将流量路由到自动创建的NodePort服务和ClusterIP服务上；
- `ExternalName`：把集群外部的服务引入到集群内部来，在集群内部直接使用，无需创建任何类型代理。我们需要使用kube-dns 1.7及以上版本或者 CoreDNS 0.0.8及以上版本才能使用ExternalName类型。

> **说明**：我们也可以使用[Ingress](https://kubernetes.io/zh/docs/concepts/services-networking/ingress/)来暴露自己的服务。Ingress不是一种服务类型，但它充当集群的入口点。它可以将路由规则整合到一个资源中，因为它可以在同一IP地址下公开多个服务。

**关于NodePort类型：**

如果我们将`type`字段设置为`NodePort`，则Kubernetes控制面将在`--service-node-port-range`标志指定的范围内分配端口(默认范围：30000-32767)，每个节点会将那个端口代理到我们的服务中。通过设置`spec.ports[*].nodePort`字段的值，也可以指定特定的端口号，不过指定的这个端口号要在有效范围内。

下面进行一个简单的案例演示：

- 新建一个yaml文件，内容如下：

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: my-service
  spec:
    type: NodePort  #注意这边的类型使用的是NodePort
    selector:
      app: myapp
    ports:
      - port: 80
        targetPort: 80
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: my-deploy
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: myapp
    template:
      metadata:
        labels:
          app: myapp
      spec:
        containers:
        - name: c-app
          image: gongcqq/myapp:1.0
          imagePullPolicy: IfNotPresent
  ```

- 基于yaml文件创建对应资源后，查看资源的运行情况：

  ![image-20210619161223384](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210619175809.png) 

- 外部浏览器中输入集群中任一主机的IP和上面的30597端口访问集群中的Pod：

  ```http
  http://192.168.68.13:30597/
  ```

  ![image-20210619161610349](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210619175817.png) 

- 我们也可以自定义对外暴露的端口，只需要在以上yaml文件中创建Service的部分加上`nodePort`字段即可，如下：

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: my-service
  spec:
    type: NodePort       #这里表明类型用的是NodePort
    selector:
      app: myapp
    ports:
      - port: 80
        targetPort: 80
        nodePort: 30007  #这里指定对外暴露的端口号为30007
  ```

- 基于yaml文件创建对应资源后，查看运行情况：

  ![image-20210619162340169](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210619175821.png) 

- 外部浏览器中输入集群中任一主机的IP和上面的30007端口访问集群中的Pod：

  ```http
  http://192.168.68.12:30007/
  ```

  ![image-20210619162545333](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210619175825.png) 

### 10.k8s核心技术-Ingress

Ingress是对集群中服务的外部访问进行管理的API对象，典型的访问方式是HTTP。

Ingress可以提供负载均衡、SSL终结和基于名称的虚拟托管。

Ingress公开了从集群外部到集群内**服务**的HTTP和HTTPS路由，流量路由由Ingress资源上定义的规则控制。

下面是一个将所有流量都发送到同一Service的简单Ingress示例：

![image-20210620231141807](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210620231221.png)  

可以将Ingress配置为服务提供外部可访问的 URL、负载均衡流量、终止 SSL/TLS，以及提供基于名称的虚拟主机等能力。[Ingress 控制器](https://kubernetes.io/zh/docs/concepts/services-networking/ingress-controllers)通常负责通过负载均衡器来实现Ingress，尽管它也可以配置边缘路由器或其他前端来帮助处理流量。Ingress不会公开任意端口或协议，如果我们想将HTTP和HTTPS以外的服务公开到Internet，通常使用[NodePort](https://kubernetes.io/zh/docs/concepts/services-networking/service/#nodeport)或[LoadBalancer](https://kubernetes.io/zh/docs/concepts/services-networking/service/#loadbalancer)类型的服务。

> **说明**：我们必须具有[Ingress 控制器](https://kubernetes.io/zh/docs/concepts/services-networking/ingress-controllers)才能满足Ingress的要求，仅创建Ingress资源本身没有任何效果。控制器不止一种，一般使用[ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/)。

#### 10.1 部署ingress-nginx

ingress-nginx是一种常用的ingress控制器，下面就演示一下如何在集群中部署ingress-nginx。

1. 下载配置文件：

   ```shell
   #下载并解压配置文件
   wget https://files.cnblogs.com/files/gongcqq/mandatory.tar.gz && tar -zxvf mandatory.tar.gz
   ```

2. 关于配置文件增加hostNetwork的说明：

   官方默认的`mandatory.yaml`文件中是没有下图这个`hostNetwork: true`的，而我提供的这个配置文件中是加上了的。使用ingress需要用到物理机的80/443端口，官方推荐的是使用NodePort的方式暴露端口，但是这种方式会导致我们前台通过域名访问后台的时候还要加个端口，就会很奇怪。而且这种方式需要集群中每个主机都暴露80/443端口，也会造成端口的浪费。

   开启hostNetwork模式后，一旦ingress-nginx运行在了集群中的某台主机上，那域名对应的主机地址就要是这台主机的才行，如果是集群中其他主机的地址，域名是无法访问的。所以在实际使用中，一般都会在`mandatory.yaml`配置文件中使用`nodeSelector`等方式将ingress-nginx调度到固定节点，以便固定域名映射的主机IP。

   官方默认是使用Deployment的方式调度ingress-nginx的，而且副本数默认是1，如果我们想要增加副本数且不想让这些ingress-nginx都调度到同一台主机上的话，也可以通过给节点打标签，然后使用`nodeSelector`进行固定节点调度。

   ![image-20210623144534723](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210626202852.png) 

3. 基于yaml文件创建资源：

   ```shell
   #创建资源
   kubectl apply -f mandatory.yaml

   #查看资源运行情况
   kubectl get pod,svc -n ingress-nginx -o wide
   ```
   
   ![image-20210623183525813](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210626202901.png) 

> **说明**：ingress-nginx官方默认使用的是Deployment来部署的，而且副本数是1，我们可以根据自身需求进行扩容操作，如果担心单点故障，也可以将部署方式改成DaemonSet。

#### 10.2 Ingress入门案例

- 新建一个名为`test-ingress.yaml`的yaml文件，内容如下：

  ```yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: nginx-deploy
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: nginx
    template:
      metadata:
        labels:
          app: nginx
      spec:
        containers:
        - name: c-nginx
          image: gongcqq/myapp:3.0
          imagePullPolicy: IfNotPresent
  ---
  apiVersion: v1
  kind: Service
  metadata:
    name: nginx-svc
  spec:
    selector:
      app: nginx
    ports:
      - port: 80
        targetPort: 80
  ---
  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata:
    name: nginx-ingress
  spec:
    rules:
    - host: "app.test.com"   #这个就是外网访问用的域名地址
      http:
        paths:
        - path: /
          backend:
            serviceName: nginx-svc   #这个值就是上面创建的Service的name
            servicePort: 80
  ```

- 基于yaml文件创建资源并查看运行情况：

  ```shell
  #基于yaml文件创建资源
  kubectl apply -f test-ingress.yaml
  
  #查看运行情况，下面的ing就是ingress的缩写
  kubectl get pod,svc,ing -o wide
  ```

  ![image-20210626164412723](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210626202910.png) 

- 查看目前ingress-nginx控制器所在主机的地址：

  ```shell
  kubectl get pod -n ingress-nginx -o wide
  ```

  ![image-20210626151622960](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210626202915.png)

  其实我们在ingress中配置的规则，都会被写入到ingress-nginx控制器中nginx的配置文件中，我们可以进入相应的Pod中进行查看：

  ```shell
  #进入ingress-nginx控制器所在的Pod中
  kubectl exec nginx-ingress-controller-6fc4ff765-kl78x -n ingress-nginx -it -- sh
  
  #查看nginx.conf配置文件中域名为"app.test.com"的相关配置
  cat nginx.conf|grep -A32 "start server app.test.com"
  ```

  ![image-20210627214635125](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210627214648.png) 

- 给本机电脑的hosts进行相关配置，以便通过域名访问k8s集群中的Pod：

  ![image-20210626160117903](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210626202919.png) 

  > **说明**：windows系统hosts的默认路径是：`C:\Windows\System32\drivers\etc`。

- 浏览器多次访问`http://app.test.com/hostname.html`，可以发现，每次返回的Pod的名称都不一样，说明是在轮询访问k8s中的Pod。

  ![image-20210626211827539](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210626212524.png) 

  我们也可以[点击此处](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210626212137.gif)查看动图演示。

  > **说明**：为了防止由于浏览器缓存问题导致无法看到轮询效果，所以上面打开控制台后，通过勾选`Disable cache`禁用了缓存。

- 上文配置hosts的时候，还配置了一个`abc.test.com`的域名，这个域名在k8s中是没有对应的ingress的，下面演示一下浏览器访问`http://abc.test.com`的效果：

  ![image-20210626232259442](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210626232304.png)

#### 10.3 Ingress资源

##### 10.3.1 Ingress规则

每个HTTP规则都包含以下信息：

- 可选主机。以上入门案例中配置的域名`app.test.com`就是一个可选主机，这个不是必填的，如果不设置域名，那么所有在hosts中配置映射关系的域名都可以访问到该ingress，或者我们直接在浏览器通过ingress-nginx控制器所在主机的IP地址进行访问也是可以的；
- 路径列表(比如`/test`)。每个路径都有一个由`serviceName`和`servicePort`定义的关联后端。在负载均衡器将流量定向到引用的服务之前，主机和路径都必须匹配传入请求的内容。比如我们将请求的根路径设置为`/test`，那么只有域名后加上该路径才能访问到对应的ingress，比如`http://app.test.com/test`。当然，后端也要有与该路径相匹配的请求地址才行，不然访问就该报404了；
- 后端是Service文档中所述的服务和端口名称的组合。与规则的主机和路径匹配的对Ingress的HTTP(和HTTPS)请求将发送到列出的后端。

> **说明**：通常在Ingress控制器中会配置默认后端(比如上文提到的`default-http-backend`)，以服务任何不符合规范中路径的请求。如果主机或路径都没有与Ingress对象中的HTTP请求匹配，则流量将路由到默认后端。

##### 10.3.2 路径的类型

Ingress中的每个路径都有对应的路径类型，当前支持的路径类型有以下三种：

- `ImplementationSpecific`：这个是默认类型，对于这种类型，匹配取决于IngressClass。具体实现可以将其作为单独的`pathType`处理或者与`Prefix`或`Exact`类型作相同处理；
- `Exact`：精确匹配 URL 路径，且对大小写敏感；
- `Prefix`：基于以 `/` 分隔的 URL 路径前缀匹配。匹配对大小写敏感，并且对路径中的元素逐个完成。路径元素指的是由 `/` 分隔符分隔的路径中的标签列表。如果每个 *p* 都是请求路径 *p* 的元素前缀，则请求与路径 *p* 匹配。

下面列举一个带路径的ingress资源：

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: nginx-ingress
spec:
  rules:
  - host: "app.test.com"
    http:
      paths:
      - path: /test
        pathType: Prefix
        backend:
          serviceName: nginx-svc
          servicePort: 80
```

> **说明**：以上yaml资源中，`/test`是路径，路径类型用的是`Prefix`。

路径匹配示例：

| 类型   | 路径                            | 请求路径        | 匹配与否？               |
| ------ | ------------------------------- | --------------- | ------------------------ |
| Prefix | `/`                             | （所有路径）    | 是                       |
| Exact  | `/foo`                          | `/foo`          | 是                       |
| Exact  | `/foo`                          | `/bar`          | 否                       |
| Exact  | `/foo`                          | `/foo/`         | 否                       |
| Exact  | `/foo/`                         | `/foo`          | 否                       |
| Prefix | `/foo`                          | `/foo`, `/foo/` | 是                       |
| Prefix | `/foo/`                         | `/foo`, `/foo/` | 是                       |
| Prefix | `/aaa/bb`                       | `/aaa/bbb`      | 否                       |
| Prefix | `/aaa/bbb`                      | `/aaa/bbb`      | 是                       |
| Prefix | `/aaa/bbb/`                     | `/aaa/bbb`      | 是，忽略尾部斜线         |
| Prefix | `/aaa/bbb`                      | `/aaa/bbb/`     | 是，匹配尾部斜线         |
| Prefix | `/aaa/bbb`                      | `/aaa/bbb/ccc`  | 是，匹配子路径           |
| Prefix | `/aaa/bbb`                      | `/aaa/bbbxyz`   | 否，字符串前缀不匹配     |
| Prefix | `/`, `/aaa`                     | `/aaa/ccc`      | 是，匹配 `/aaa` 前缀     |
| Prefix | `/`, `/aaa`, `/aaa/bbb`         | `/aaa/bbb`      | 是，匹配 `/aaa/bbb` 前缀 |
| Prefix | `/`, `/aaa`, `/aaa/bbb`         | `/ccc`          | 是，匹配 `/` 前缀        |
| Prefix | `/aaa`                          | `/ccc`          | 否                       |
| 混合   | `/foo` (Prefix), `/foo` (Exact) | `/foo`          | 是，优选 Exact 类型      |

##### 10.3.3 主机名通配符

主机名可以是精确匹配(例如`foo.bar.com`)也可以使用通配符来匹配(例如`*.foo.com`)。精确匹配要求HTTP `host` 头部字段与 `host` 字段值完全匹配。 通配符匹配则要求HTTP `host` 头部字段与通配符规则中的后缀部分相同。

| 主机        | host 头部         | 匹配与否？                          |
| ----------- | ----------------- | ----------------------------------- |
| `*.foo.com` | `bar.foo.com`     | 基于相同的后缀匹配                  |
| `*.foo.com` | `baz.bar.foo.com` | 不匹配，通配符仅覆盖了一个 DNS 标签 |
| `*.foo.com` | `foo.com`         | 不匹配，通配符仅覆盖了一个 DNS 标签 |

下面进行一个关于主机名通配的案例演示：

- 新建一个名为`ingress-wildcard-host.yaml`的yaml文件：

  ```yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: deploy1
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: nginx1
    template:
      metadata:
        labels:
          app: nginx1
      spec:
        containers:
        - name: c-nginx1
          image: gongcqq/myapp:4.0
          imagePullPolicy: IfNotPresent
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: deploy2
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: nginx2
    template:
      metadata:
        labels:
          app: nginx2
      spec:
        containers:
        - name: c-nginx2
          image: gongcqq/myapp:4.0
          imagePullPolicy: IfNotPresent
  ---
  apiVersion: v1
  kind: Service
  metadata:
    name: service1
  spec:
    selector:
      app: nginx1
    ports:
      - port: 80
        targetPort: 80
  ---
  apiVersion: v1
  kind: Service
  metadata:
    name: service2
  spec:
    selector:
      app: nginx2
    ports:
      - port: 80
        targetPort: 80
  ---
  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata:
    name: ingress-wildcard-host
  spec:
    rules:
    - host: "app.bar.com"
      http:
        paths:
        - path: /
          backend:
            serviceName: service1
            servicePort: 80
    - host: "*.foo.com"
      http:
        paths:
        - path: /
          backend:
            serviceName: service2
            servicePort: 80
  ```

- 基于yaml文件创建资源并查看运行情况：

  ```shell
  kubectl apply -f ingress-wildcard-host.yaml
  kubectl get pod,svc,ing -o wide
  ```

  ![image-20210627202453463](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210627202456.png) 

- 在本机配置hosts：

  ![image-20210627202618927](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210627202626.png) 

- 浏览器访问`http://app.bar.com/hostname.html`，可以发现，总是轮询访问service1负载下的两个Pod，如下：

  ![image-20210627202904106](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210627203201.png)

  如果浏览器访问的是`http://abc.foo.com/hostname.html`或者`http://gsl.foo.com/hostname.html`，就会轮询访问service2负载的两个Pod，如下：

  ![image-20210627203111737](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210627203207.png) 

> **说明**：以上案例说明，ingress是可以根据不用域名去访问不用serviceName对应的Pod的，并且域名配置了通配符的话，只要访问的域名符合通配符的匹配规则，就可以访问到我们想要的后端资源。

##### 10.3.4 Ingress Class

Ingress可以由不同的控制器实现，通常使用不同的配置。每个Ingress应当指定一个类，也就是一个对IngressClass 资源的引用。IngressClass资源包含额外的配置，其中包括应当实现该类的控制器名称。

IngressClass资源包含一个可选的`parameters`字段，可用于为该类引用额外的、特定于具体实现的配置。

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: IngressClass
metadata:
  name: external-lb
spec:
  controller: example.com/ingress-controller
  parameters:
    apiGroup: k8s.example.com
    kind: IngressParameters
    name: external-lb
```

`parameters`字段有一个`scope`和`namespace`字段，可用来引用特定于名称空间的资源，对Ingress类进行配置。

`scope`字段默认为`Cluster`，表示默认是集群作用域的资源。将`scope`设置为`Namespace`并设置`namespace`字段就可以引用某特定名称空间中的参数资源。

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: IngressClass
metadata:
  name: external-lb
spec:
  controller: example.com/ingress-controller
  parameters:
    apiGroup: k8s.example.com
    kind: IngressParameters
    name: external-lb
    namespace: external-configuration
    scope: Namespace
```

我们可以将一个特定的IngressClass标记为集群默认Ingress类。将一个IngressClass资源的如下注解设置为`true`就可以让新的未指定`ingressClassName`字段的Ingress分配为这个默认的IngressClass。

```yaml
ingressclass.kubernetes.io/is-default-class
```

比如像下面这样的IngressClass：

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: IngressClass
metadata:
  name: external
  annotations:
    ingressclass.kubernetes.io/is-default-class: "true"
spec:
  controller: example.com/ingress-controller
  parameters:
    apiGroup: k8s.example.com
    kind: IngressParameters
    name: external
```

> **注意：**如果集群中有多个 IngressClass 被标记为默认，准入控制器将阻止创建新的未指定`ingressClassName`的Ingress对象。解决这个问题只需确保集群中最多只能有一个IngressClass被标记为默认即可。

如果我们没有通过上述注解的方式设置默认的IngressClass，但是在创建ingress的时候又想使用这个IngressClass的话，也可以通过`ingressClassName`字段来实现。举例如下：

- 我们先创建一个名为external.yaml的ingressClass资源，内容如下：

  ```yaml
  apiVersion: networking.k8s.io/v1beta1
  kind: IngressClass
  metadata:
    name: test-external
  spec:
    controller: example.com/ingress-controller
    parameters:
      apiGroup: k8s.example.com
      kind: IngressParameters
      name: test-external
  ```

- 创建后通过`kubectl apply -f external.yaml`命令基于yaml文件创建资源并查看：

  ![image-20210627224316774](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210627224320.png) 

- 然后再基于class-ingress.yaml文件创建两个ingress，内容如下：

  ```yaml
  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata:
    name: class-ingress1
  spec:
    rules:
    - host: "app.test.com"
      http:
        paths:
        - path: /foo
          pathType: Prefix
          backend:
            serviceName: svc1
            servicePort: 80
  ---
  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata:
    name: class-ingress2
  spec:
    ingressClassName: "test-external"
    rules:
    - host: "app.test.com"
      http:
        paths:
        - path: /foo
          pathType: Prefix
          backend:
            serviceName: svc2
            servicePort: 80
  ```

- 最后基于yaml文件创建ingress资源并查看：

  ```shell
  kubectl apply -f class-ingress.yaml
  kubectl get ing -o wide
  ```

  ![image-20210627225350408](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210627225428.png)  

> **说明**：一般只有用到多个ingress控制器的时候，才会需要使用IngressClass，像我这里只有ingres-nginx这一个控制器的话，其实是用不到这个IngressClass的。

#### 10.4 Ingress的类型

##### 10.4.1 单个service

如下所示，一个域名只对应一个指定路径下的指定service。

```
                                                         |---> pod1
foo.bar.com ---> 192.168.68.13 ---> /foo   service:80 ---|
                                                         |---> pod2
```

yaml案例如下：

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  rules:
  - host: "foo.bar.com"
    http:
      paths:
      - path: /foo
        pathType: Prefix
        backend:
          serviceName: service
          servicePort: 80
```

##### 10.4.2 根据路径指定service

如下所示，同一个域名，请求路径不同，ingress对应的service也不一样。

```
                                                               |---> pod1
                                  |---> /foo  service1:4200 ---|
                                  |                            |---> pod2
foo.bar.com ---> 192.168.68.13 ---|
                                  |                            |---> pod3
                                  |---> /bar  service2:8080 ---|
                                                               |---> pod4
```

yaml案例如下：

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /foo
        pathType: Prefix
        backend:
          serviceName: service1
          servicePort: 4200
      - path: /bar
        pathType: Prefix
        backend:
          serviceName: service2
          servicePort: 8080
```

![image-20210629120150685](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210629172238.png) 

##### 10.4.3 根据域名指定service

如下所示，根据不同域名访问后端不同的service。

```
                                                                 |---> pod1
                               |---> foo.bar.com  service1:80 ---|
foo.bar.com ---|               |                                 |---> pod2
               | 192.168.68.13 |
bar.foo.com ---|               |                                 |---> pod3
                               |---> bar.foo.com  service2:80 ---|
                                                                 |---> pod4
```

yaml案例如下：

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          serviceName: service1
          servicePort: 80
  - host: bar.foo.com
    http:
      paths:
      - backend:
          serviceName: service2
          servicePort: 80
```

![image-20210629142622196](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210629172245.png) 

##### 10.4.4 根据域名及路径指定service

如下所示，根据不同域名和不同路径访问后端不同的service。

```
                                                                             |---> pod1
                                |---> app.test.com ---> /test  service:80 ---|
                                |                                            |---> pod2
app.test.com ---|               |                                               
                | 192.168.68.13 |                                               |---> pod3
foo.bar.com  ---|               |                    |---> /foo  service1:80 ---| 
                                |                    |                          |---> pod4
                                |---> foo.bar.com ---|
                                                     |                          |---> pod5
                                                     |---> /bar  service2:80 ---|
                                                                                |---> pod6
```

yaml案例如下：

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  rules:
  - host: app.test.com
    http:
      paths:
      - path: /test
        pathType: Prefix
        backend:
          serviceName: service
          servicePort: 80
  - host: foo.bar.com
    http:
      paths:
      - path: /foo
        pathType: Prefix
        backend:
          serviceName: service1
          servicePort: 80
      - path: /bar
        pathType: Prefix
        backend:
          serviceName: service2
          servicePort: 80
```

![image-20210629150650155](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210629172254.png) 

> **说明**：以上几种类型涉及的类型图中都有用到的地址(`192.168.68.13`)是ingress控制器的地址，不过在部署控制器的时候我们开启了`hostNetwork`模式，所以这个地址也是控制器所在主机的地址。

#### 10.5 ingress https

以上介绍的通过ingress实现域名访问的方式都是http的方式，现在介绍下https的方式。由于https方式访问需要我们创建证书，所以下面先按照ingress-nginx[官方步骤](https://kubernetes.github.io/ingress-nginx/examples/PREREQUISITES/)生成证书并创建secret：

```shell
# 生成证书
openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj "/CN=nginxsvc/O=nginxsvc"

# 根据证书创建secret
kubectl create secret tls tls-secret --key tls.key --cert tls.crt
```

![image-20210629160033479](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210629172301.png) 

证书以及secret创建好之后，创建一个ingress资源进行测试：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: c-nginx
        image: gongcqq/myapp:4.0
        imagePullPolicy: IfNotPresent
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
spec:
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 80
---
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: tls-ingress
spec:
  tls:
  - hosts:
    - foo.bar.com
    secretName: tls-secret  #这个就是上面创建的secret的名称
  rules:
    - host: foo.bar.com
      http:
        paths:
        - path: /
          backend:
            serviceName: nginx-svc
            servicePort: 80
```

基于以上yaml的内容创建对应资源，本机配置hosts后，通过浏览器访问`https://foo.bar.com/hostname.html`，访问结果是Pod的名称，说明通过https的方式能够成功访问到后端。

![image-20210629162020567](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210629172307.png) 

#### 10.6 基础的权限认证

如果我们并不希望任何人都能随意通过域名访问到我们的后端，那我们可以增加鉴权功能，下面演示一下[官网](https://kubernetes.github.io/ingress-nginx/examples/auth/basic/)提及的最基础的权限认证的方式。加上权限认证后，只有输入正确的用户名密码后才能访问到我们的后端应用。

```shell
yum -y install httpd

# 下面的admin是用户名
htpasswd -c auth admin
```

![image-20210629163729069](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210629172313.png) 

```shell
# 创建secret
kubectl create secret generic basic-auth --from-file=auth

# 查看该secret对应的yaml内容
kubectl get secret basic-auth -o yaml
```

![image-20210629164117383](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210629172317.png) 

新建一个ingress资源来测试权限认证功能：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: c-nginx
        image: gongcqq/myapp:4.0
        imagePullPolicy: IfNotPresent
---
apiVersion: v1
kind: Service
metadata:
  name: http-svc
spec:
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 80
---
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: ingress-with-auth
  annotations:
    # type of authentication
    nginx.ingress.kubernetes.io/auth-type: basic
    # name of the secret that contains the user/password definitions
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    # message to display with an appropriate context why the authentication is required
    nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required - admin'
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /
        backend:
          serviceName: http-svc
          servicePort: 80
```

基于以上yaml创建对应资源后，本地浏览器访问`http://foo.bar.com`，可以发现，浏览器弹出了要求我们输入用户名密码的对话框，输入正确后，才可以成功访问到我们的后端应用。这里的用户名密码就是上面我们自己设置的。

![image-20210629165204998](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210629172222.png) 

### 11.k8s核心技术-配置管理

#### 11.1 ConfigMap

ConfigMap是一种API对象，用来将非机密性的数据保存到键值对中。使用时，Pod可以将其用作环境变量、命令行参数或者存储卷中的配置文件。ConfigMap将我们的环境配置信息和容器镜像解耦，便于应用配置的修改。

我们可以使用ConfigMap将我们的配置数据和应用程序代码分开，不过ConfigMap在设计上不是用来保存大量数据的，在ConfigMap中保存的数据不可超过1 MiB。如果我们需要保存超出此限制的数据，那么可以考虑挂载存储卷、使用独立的数据库或者文件服务等。

> **注意**：ConfigMap并不提供保密或者加密功能。如果我们存储的数据是机密的，可以使用[Secret](https://kubernetes.io/zh/docs/concepts/configuration/secret/)，或者其他第三方工具来保证我们数据的私密性，而不是使用ConfigMap。

##### 11.1.1 ConfigMap对象

ConfigMap是一个API对象，让我们可以存储其他对象所需要使用的配置。和其他Kubernetes对象都有一个`spec`不同的是，ConfigMap使用`data`和`binaryData`字段。这些字段能够接收键值对作为其取值。`data`和`binaryData`字段都是可选的。`data`字段被设计用来保存UTF-8字节序列，而`binaryData`则用来保存二进制数据作为base64编码的字串。

`data`或`binaryData`字段下面的每个键的名称都必须由字母数字字符或者 `-`、`_` 或 `.` 组成。在`data`下保存的键名不可以与在`binaryData`下出现的键名有重叠。

另外，在高版本的k8s中，我们可以通过`immutable`字段创建一个不可变更的ConfigMap。

##### 11.1.2 ConfigMap的入门案例

我们可以写一个引用ConfigMap的Pod的`spec`，并根据ConfigMap中的数据在该Pod中配置容器。需要注意，这个Pod和ConfigMap必须要在同一个名称空间下。

- 下面是一个ConfigMap的示例：

  ```yaml
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: cm-demo
  data:
    #类属性键；每一个键都映射到一个简单的值
    player_initial_lives: "3"
    ui_properties_file_name: "user-interface.properties"
  
    #类文件键
    game.properties: |
      enemy.types=aliens,monsters
      player.maximum-lives=5    
    user-interface.properties: |
      color.good=purple
      color.bad=yellow
      allow.textmode=true 
  ```

  我们可以使用以下四种方式来使用ConfigMap配置Pod中的容器：

  1. 在容器命令和参数内；
  2. 容器的环境变量；
  3. 在只读卷里面添加一个文件，让应用来读取；
  4. 编写代码在Pod中运行，使用Kubernetes API来读取ConfigMap。

  以上不同的方法适用于不同的数据使用方式。对于前面三个方法，kubelet会使用 ConfigMap 中的数据在 Pod 中启动容器。第四种方法意味着，我们必须编写代码才能读取 ConfigMap 和它的数据。然而，由于我们是直接使用的Kubernetes API，因此只要 ConfigMap 发生更改，我们的应用就能够通过订阅来获取更新，并且在这样的情况发生的时候做出反应。通过直接访问Kubernetes API，也可以让我们能够获取到不同名称空间下的ConfigMap。

- 下面是一个Pod的示例，它通过使用上面名为`cm-demo`的ConfigMap中的值来配置一个Pod：

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: cm-demo-pod
  spec:
    containers:
      - name: c-demo
        image: alpine:3.12
        command: ["sleep", "3600"]
        env:
          # 定义环境变量
          - name: PLAYER_INITIAL_LIVES # 请注意这里和ConfigMap中的键名是不一样的
            valueFrom:
              configMapKeyRef:
                name: cm-demo           # 这个值就是ConfigMap的名称
                key: player_initial_lives # 这个是需要取值的键
          - name: UI_PROPERTIES_FILE_NAME
            valueFrom:
              configMapKeyRef:
                name: cm-demo
                key: ui_properties_file_name
        volumeMounts:
        - name: config
          mountPath: "/config"
          readOnly: true
    volumes:
    # 我们可以在Pod级别设置卷，然后将其挂载到Pod内的容器中
    - name: config
      configMap:
        # 这里提供我们想要挂载的ConfigMap的名字
        name: cm-demo
        # 以下是来自ConfigMap的一组键，将被创建为文件
        items:
        - key: "game.properties"
          path: "game.properties"
        - key: "user-interface.properties"
          path: "user-interface.properties"
  ```

  ConfigMap不会区分单行属性值和多行类似文件的值，重要的是Pod和其他对象如何使用这些值。上面关于Pod的例子定义了一个卷并将它作为`/config`文件夹挂载到了名为`c-demo`的容器内。同时会创建`/config/game.properties`以及`/config/user-interface.properties`这两个文件。ConfigMap中包含了四个键，但是只创建两个文件，这是因为在Pod的定义中，在`volumes`字段下指定了一个`items`数组。如果不使用`items`数组，就会创建四个和ConfigMap中的键同名的文件。

- 创建好ConfigMap和Pod后，使用`kubectl apply -f`命令依次执行对应的yaml文件，然后查看运行情况：

  ```shell
  # 以下的cm是configmap的缩写形式
  kubectl get cm,pod -o wide
  ```

  ![image-20210629213625909](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701151926.png)

  > **说明**：如果想知道k8s中某个对象的缩写是什么，可以使用`kubectl api-resources`命令进行查看。

- 当ConfigMap和Pod创建成功后，查看Pod中环境变量里引用ConfigMap的部分是否生效：

  ```shell
  kubectl exec cm-demo-pod -it -- env
  ```

  ![image-20210629215323581](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701151930.png) 

- 环境变量处没有问题了，接下来再进去到Pod中，查看`/config`目录下是否创建了相应的文件：

  ```shell
  kubectl exec cm-demo-pod -it -- sh
  ```

  ![image-20210629220257868](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701151953.png) 

> **注意**：在该案例中，环境变量和挂载卷都用到了ConfigMap，需要注意的是，当ConfigMap更新时，挂载卷中相关的内容在一段时间后(大概10秒左右)是可以同步更新的，但是环境变量中相关的内容是不能同步更新的。

##### 11.1.3 不可变更的ConfigMap

在Kubernetes中，提供了一种将ConfigMap设置为不可变更的选项，对于大量使用ConfigMap的集群(至少有数万个各不相同的ConfigMap给Pod挂载)而言，禁止更改ConfigMap的数据有以下好处：

- 保护应用，使之免受意外(不想要的)更新所带来的负面影响；
- 通过大幅降低对kube-apiserver的压力提升集群性能，这是因为系统会关闭对已标记为不可变更的ConfigMap的监视操作。

我们可以通过将`immutable`字段设置为`true`创建不可变更的ConfigMap。例如：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  ...
data:
  ...
immutable: true
```

![image-20210630120430270](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701152006.png) 

一旦某ConfigMap被标记为不可变更，则 *无法* 逆转这一变化，也无法更改`data`或`binaryData`字段的内容，我们只能删除并重建ConfigMap。

##### 11.1.4 创建ConfigMap的几种方式

上面的入门案例中是直接使用yaml文件的方式创建ConfigMap的，这也是比较常规的一种方式，就不再多介绍了，下面主要介绍通过`kubectl create`命令创建ConfigMap的几种方式。

###### 11.1.4.1 基于目录创建ConfigMap

我们可以使用`kubectl create`命令基于同一目录中的多个文件创建ConfigMap。当我们基于目录来创建ConfigMap时，kubectl会识别目录下基本名可以作为合法键名的文件，并将这些文件打包到新的ConfigMap中。普通文件之外的所有目录项都会被忽略(例如，子目录、符号链接、设备、管道等等)。

具体演示步骤如下：

- 通过以下命令创建一个名为game.properties的文件：

  ```shell
  cat >> /root/game.properties << EOF
  enemies=aliens
  lives=3
  enemies.cheat=true
  enemies.cheat.level=noGoodRotten
  secret.code.passphrase=UUDDLRLRBABAS
  secret.code.allowed=true
  secret.code.lives=30
  EOF
  ```

- 通过以下命令再创建另一个名为ui.properties的文件：

  ```shell
  cat >> /root/ui.properties << EOF
  color.good=purple
  color.bad=yellow
  allow.textmode=true
  how.nice.to.look=fairlyNice
  EOF
  ```

- 创建一个本地目录，并将以上两个文件放到该目录下：

  ```shell
  #创建一个本地目录
  mkdir -p /root/dir-cm
  
  #移动文件到指定目录下
  mv /root/game.properties /root/ui.properties /root/dir-cm
  ```

- 基于目录创建ConfigMap并查看：

  ```shell
  kubectl create configmap game-config --from-file=/root/dir-cm/
  ```

  ![image-20210630172708470](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701152015.png)

  > **说明**：由以上截图可知，dir-cm目录下game.properties文件和ui.properties文件的内容都已经被存到了名为`game-config`的ConfigMap的data字段中了。

###### 11.1.4.2 基于文件创建ConfigMap

我们还可以使用`kubectl create`命令基于单个文件或多个文件创建ConfigMap。比如：

```shell
kubectl create configmap game-config-2 --from-file=/root/dir-cm/game.properties
```

我们也可以多次使用`--from-file`参数，从多个数据源创建ConfigMap。比如：

```shell
kubectl create configmap game-config-3 \
--from-file=/root/dir-cm/game.properties --from-file=/root/dir-cm/ui.properties
```

当`kubectl`基于非ASCII或UTF-8的输入创建ConfigMap时，这些输入会被放入ConfigMap的`binaryData`字段中，而不是`data`字段中。同一个ConfigMap中是可以同时包含文本数据和二进制数据源的。如果我们想要查看ConfigMap中的`binaryData`键(及其值)，我们可以运行`kubectl get configmap -o jsonpath='{.binaryData}' <name>`命令。

另外，我们可以还使用`--from-env-file`选项从环境文件创建ConfigMap，Env文件包含环境变量列表，其中适用以下语法规则：

- Env文件中的每一行必须为VAR=VAL格式；
- 以＃开头的行(即注释行)将被忽略；
- 空行将被忽略；
- 引号不会被特殊处理(即它们将成为ConfigMap值的一部分)。

下面进行一个案例演示：

- 使用以下命令创建一个名为game-env-file.properties的文件：

  ```shell
  cat >> /root/game-env-file.properties << EOF
  lives=3
  allowed="true"
  
  # This comment and the empty line above it are ignored
  enemies=aliens
  EOF
  ```

- 基于文件创建ConfigMap并查看：

  ```shell
  kubectl create configmap game-env-file --from-env-file=/root/game-env-file.properties
  ```

  ![image-20210630181532972](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701152030.png)

  > **说明**：通过以上截图可知，game-env-file.properties文件中的空行和注释行都没有被保存到ConfigMap里面，而且带引号的地方也没有被特殊处理，而是被原样地保存进了`data`字段中。

**注意**：当使用多个`--from-env-file`选项来从多个数据源创建ConfigMap时，仅仅最后一个env文件有效。

###### 11.1.4.3 指定从文件创建ConfigMap的键

在使用`--from-file`参数时，我们可以自定义在ConfigMap的`data`部分出现的键名，而不是按默认行为使用文件名：

```shell
kubectl create configmap test-config --from-file=<my-key-name>=<path-to-file>
```

以上`<my-key-name>`是我们要在ConfigMap中使用的键名，`<path-to-file>`是我们想要键表示数据源文件的位置。

具体案例如下：

- 使用以下命令创建一个名为test-file.properties的文件：

  ```shell
  cat >> /root/test-file.properties << EOF
  lives=3
  allowed="true"
  color.good=purple
  color.bad=yellow
  allow.textmode=true
  how.nice.to.look=fairlyNice
  EOF
  ```

- 基于文件创建ConfigMap并查看：

  ```shell
  kubectl create configmap test-config --from-file=special-key=/root/test-file.properties
  ```

  ![image-20210630201908439](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701152054.png) 

###### 11.1.4.4 根据字面值创建ConfigMap

我们可以通过`--from-literal`选项直接在命令行定义键值对的key值和value值，而且还可以通过使用多个该选项设置多个键值对，比如：

```shell
kubectl create configmap special-config \
--from-literal=special.how=very --from-literal=special.type=charm
```

![image-20210630203138433](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701152103.png) 

###### 11.1.4.5 基于生成器创建ConfigMap 

自1.14开始，`kubectl`开始支持`kustomization.yaml`。我们还可以基于生成器创建ConfigMap，然后将其应用于API服务器上创建对象。生成器应在`kustomization.yaml`中指定。

**使用kustomization.yaml基于文件创建ConfigMap：**

```shell
#在当前路径下创建一个generator.properties文件
cat >> generator.properties << EOF
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
EOF
```

```shell
#创建kustomization.yaml文件，里面包含了generator.properties文件
cat <<EOF >./kustomization.yaml
configMapGenerator:
- name: generator-config
  files:
  - generator.properties
EOF
```

```shell
#基于kustomization.yaml文件创建ConfigMap对象
kubectl apply -k .
```

![image-20210701112359104](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701152126.png) 

**使用kustomization.yaml指定从文件创建ConfigMap的键：**

```shell
#文件还使用上面的generator.properties，然后下面创建kustomization.yaml文件
cat <<EOF >./kustomization.yaml
configMapGenerator:
- name: generator-config-1
  files:
  - game-special-key=generator.properties
EOF
```

```shell
#基于kustomization.yaml文件创建ConfigMap对象
kubectl apply -k .
```

![image-20210701113230958](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701152146.png)

**使用kustomization.yaml从字面值生成ConfigMap：**

```shell
#创建kustomization.yaml文件
cat <<EOF >./kustomization.yaml
configMapGenerator:
- name: generator-config-2
  literals:
  - special.how=very
  - special.type=charm
EOF
```

```shell
#基于kustomization.yaml文件创建ConfigMap对象
kubectl apply -k .
```

![image-20210701114110151](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701152216.png) 

##### 11.1.5 ConfigMap在容器环境变量中的应用

###### 11.1.5.1 使用ConfigMap的数据定义容器环境变量

- 新增一个名为env-configmap.yaml的ConfigMap文件，内容如下：

  ```yaml
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: special-config
  data:
    special.how: very
  ---
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: env-config
  data:
    log_level: INFO
  ```

- 基于yaml文件创建ConfigMap并查看：

  ```shell
  kubectl apply -f env-configmap.yaml
  kubectl get cm
  ```

  ![image-20210701115405723](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701152225.png) 

- 新增一个名为env-pod.yaml的Pod文件，并在环境变量中引用ConfigMap的内容：

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: dapi-test-pod
  spec:
    containers:
      - name: test-container
        image: busybox:1.32
        command: [ "/bin/sh", "-c", "env" ]
        env:
          - name: SPECIAL_LEVEL_KEY
            valueFrom:
              configMapKeyRef:
                name: special-config
                key: special.how
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                name: env-config
                key: log_level
    restartPolicy: Never
  ```

- 基于yaml文件创建Pod并查看：

  ```shell
  kubectl apply -f env-pod.yaml
  kubectl get pod -o wide
  ```

  ![image-20210701120119927](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701152234.png) 

- 使用`kubectl logs dapi-test-pod`命令查看打印的环境变量：

  ![image-20210701142955714](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701152240.png)

我们也可以在Pod命令中使用ConfigMap定义的环境变量，比如下面这样：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: busybox:1.32
      command: [ "/bin/sh", "-c", "echo $(SPECIAL_LEVEL_KEY) $(LOG_LEVEL)" ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: env-config
              key: log_level
  restartPolicy: Never
```

以上Pod运行后，使用`kubectl logs`命令查看日志时，会输出以下内容：

```shell
very INFO
```

###### 11.1.5.2 将ConfigMap中的所有键值对配置为容器环境变量

- 创建一个名为env-multikeys.yaml的包含多个键值对的ConfigMap，内容如下：

  ```yaml
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: special-config
  data:
    SPECIAL_LEVEL: very
    SPECIAL_TYPE: charm
  ```

- 基于yaml文件创建ConfigMap并查看：

  ```shell
  kubectl apply -f env-multikeys.yaml
  kubectl get cm
  ```

  ![image-20210701143947348](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701152249.png) 

- 使用`envFrom`将所有ConfigMap的数据定义为容器环境变量，ConfigMap中的键会成为Pod中的环境变量名称，Pod相关的yaml文件内容如下：

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: dapi-test-pod
  spec:
    containers:
      - name: test-container
        image: busybox:1.32
        command: [ "/bin/sh", "-c", "env" ]
        envFrom:
        - configMapRef:
            name: special-config
    restartPolicy: Never
  ```

- 基于yaml文件创建Pod并查看打印的环境变量：

  ![image-20210701144632871](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210701152259.png) 

> **说明**：关于ConfigMap在数据卷挂载的使用就不再细说了，入门案例中也有介绍到，基本上也够用了。需要特别注意的是，如果Pod引用的ConfigMap不存在，Pod是不会启动的。同样，引用ConfigMap中不存在的键也会阻止Pod的启动。如果我们使用`envFrom`基于ConfigMap定义环境变量，那么无效的键将被忽略。

#### 11.2 Secret

`Secret`对象类型是用来保存敏感信息的，例如密码、OAuth令牌和SSH密钥。将这些信息放在`secret`中比放在Pod的定义或者容器镜像中更加安全和灵活。

要使用Secret，Pod需要去引用Secret。Pod可以用以下三种方式之一来使用Secret：

- 作为挂载到一个或多个容器上的volume中的文件；
- 作为容器的环境变量；
- 由kubelet在为Pod拉取镜像时使用。

Secret对象的名称必须是合法的[DNS子域名](https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)。在为创建 Secret 编写配置文件时，可以设置`data`与/或`stringData`字段。`data`和`stringData`字段都是可选的。`data` 字段中所有键值都必须是base64编码的字符串。如果不希望执行这种base64字符串的转换操作，可以选择设置`stringData`字段，其中可以使用任何字符串作为其取值。

##### 11.2.1 Secret的类型

在创建Secret对象时，我们可以使用Secret资源的`type`字段设置Secret的类型。

Kubernetes提供若干种内置的类型，用于一些常见的使用场景。针对这些类型，Kubernetes所执行的合法性检查操作以及对其所实施的限制各不相同。

| 内置类型                              | 用法                                   |
| ------------------------------------- | -------------------------------------- |
| `Opaque`                              | 用户定义的任意数据                     |
| `kubernetes.io/service-account-token` | 服务账号令牌                           |
| `kubernetes.io/dockercfg`             | ~/.dockercfg 文件的序列化形式          |
| `kubernetes.io/dockerconfigjson`      | ~/.docker/config.json 文件的序列化形式 |
| `kubernetes.io/basic-auth`            | 用于基本身份认证的凭据                 |
| `kubernetes.io/ssh-auth`              | 用于SSH身份认证的凭据                  |
| `kubernetes.io/tls`                   | 用于TLS客户端或者服务器端的数据        |
| `bootstrap.kubernetes.io/token`       | 启动引导令牌数据                       |

通过为Secret对象的`type`字段设置一个非空的字符串值，我们也可以定义并使用自己的Secret类型。如果`type`值为空字符串，则被视为`Opaque`类型。Kubernetes并不对类型的名称作任何限制。不过，如果我们要使用内置类型之一，则必须满足为该类型所定义的所有要求。

###### 11.2.1.1 Opaque Secret

当Secret配置文件中未作显式设定时，默认的Secret类型是`Opaque`。当我们使用`kubectl create secret`来创建一个Secret时，我们需要使用`generic`子命令来创建一个`Opaque`类型的Secret。子命令并不止`generic`这一个，如果我们想知道什么场景下应该使用什么子命令，可以使用如下命令查看：

```shell
kubectl create secret -h
```

![image-20210702105454108](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210703001142.png) 

现在我们可以通过下面的命令创建一个空的Secret对象，然后来看看具体效果：

```shell
#创建一个空的secret
kubectl create secret generic empty-secret

#查看创建的secret
kubectl get secret empty-secret

#查看secret的详情
kubectl describe secret empty-secret
```

![image-20210702110152816](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210703001209.png) 

> **说明**：以上`DATA`列展示的是Secret中保存的数据条目个数。在这个例子中，可以发现，个数为`0`，这意味着我们刚刚创建了一个空的Secret，所以并没有数据条目。

###### 11.2.1.2 服务账号令牌Secret

类型为`kubernetes.io/service-account-token`的Secret用来存放标识某服务账号的令牌。使用这种Secret类型时，我们需要确保对象的注解`kubernetes.io/service-account-name`被设置为某个已有的服务账号名称。k8s控制器会填写Secret 的其它字段，例如`kubernetes.io/service-account.uid`注解以及`data`字段中的`token`键值，以便使之包含实际的令牌内容。

`kubernetes.io/service-account-token`类型用于被ServiceAccount引用，当创建ServiceAccount时，k8s默认会创建对应的Secret，比如使用下面的命令创建一个ServiceAccount：

```shell
kubectl create -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa-name
EOF
```

![image-20210702120600045](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210703001215.png) 

k8s在创建Pod时会自动创建一个服务账号Secret并自动修改我们的Pod以使用该Secret。该服务账号令牌Secret中包含了访问Kubernetes API所需要的凭据，它会被自动挂载到Pod的`/run/secrets/kubernetes.io/serviceaccount`目录中。下面进行一个案例演示，看看效果：

- 首先通过以下yaml文件创建一个Pod：

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-nginx
  spec:
    containers:
    - name: nginx
      image: nginx:1.20
      imagePullPolicy: IfNotPresent
  ```

- 然后通过`kubectl exec pod-nginx -it -- sh`命令进入到Pod中；

- 最后通过`ls -l /run/secrets/kubernetes.io/serviceaccount`命令可以查看到该Pod中的证书以及凭据等文件。

  ![image-20210702142536792](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210703001220.png) 

###### 11.2.1.3 Docker配置Secret

我们可以使用下面两种`type`值之一来创建Secret，用以存放访问Docker仓库来下载镜像的凭据：

- `kubernetes.io/dockercfg`
- `kubernetes.io/dockerconfigjson`

`kubernetes.io/dockercfg`是一种保留类型，用来存放`~/.dockercfg`文件的序列化形式。该文件是配置Docker命令行的一种老旧形式。使用此Secret类型时，我们需要确保Secret的`data`字段中包含名为`.dockercfg`的主键，其对应键值是用base64编码的某`~/.dockercfg`文件的内容。

下面是一个`kubernetes.io/dockercfg`类型Secret的示例：

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret-dockercfg
type: kubernetes.io/dockercfg
data:
  .dockercfg: |
        "<base64 encoded ~/.dockercfg file>"
```

> **说明**：如果我们不希望执行base64编码转换，可以使用`stringData`字段代替。

类型`kubernetes.io/dockerconfigjson`被设计用来保存JSON数据的序列化形式，该JSON和`~/.docker/config.json`文件的格式规则是一样的，使用此Secret类型时，Secret对象的`data`字段必须包含`.dockerconfigjson`键，其键值为base64编码的字符串。比如通过下面的方式创建一个`kubernetes.io/dockerconfigjson`类型的Secret：

```shell
kubectl create secret docker-registry secret-tiger-docker \
  --docker-username=tiger \
  --docker-password=pass113 \
  --docker-email=tiger@acme.com
```

创建Secret后使用如下命令进行查看：

```shell
kubectl get secret secret-tiger-docker
kubectl describe secret secret-tiger-docker
kubectl get secret secret-tiger-docker -o yaml
```

![image-20210702184838104](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210703001228.png) 

我们还可以对上图中的base64编码的字符串进行解码：

```shell
#可以使用如下命令进行解码操作
echo -n "需要被解码的字符串"|base64 -d
```

![image-20210702185439375](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210703001237.png) 

解码后会得到下面这样的JSON串：

```json
{
  "auths": {
    "https://index.docker.io/v1/": {
        "username": "tiger",
        "password": "pass113",
        "email": "tiger@acme.com",
        "auth": "dGlnZXI6cGFzczExMw=="
    }
  }
}
```

我们还可以对以上JSON串中`auth`字段的值进行解码：

```shell
echo -n "dGlnZXI6cGFzczExMw=="|base64 -d
```

通过以上命令对`auth`字段的值解码后的结果是**tiger:pass113**。

###### 11.2.1.4 基本身份认证Secret

`kubernetes.io/basic-auth`类型用来存放用于基本身份认证所需的凭据信息。当我们使用这种Secret类型时，Secret的`data`字段必须包含以下两个键：

- `username`: 用于身份认证的用户名；
- `password`: 用于身份认证的密码或令牌。

以上两个键的键值都是base64编码的字符串。当然我们也可以在创建Secret时使用`stringData`字段来提供明文形式的内容。下面的yaml是基本身份认证Secret的一个示例清单：

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret-basic-auth
type: kubernetes.io/basic-auth
stringData:
  username: admin
  password: t0p-Secret
```

![image-20210702192135408](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210703001243.png) 

> **说明**：提供基本身份认证类型的Secret仅仅是出于对用户方便性的考虑。我们也可以使用`Opaque`类型来保存用于基本身份认证的凭据。不过，使用内置的Secret类型有助于对凭据格式进行归一化处理，并且 API 服务器确实会检查Secret配置中是否提供了所需要的主键。

###### 11.2.1.5 SSH身份认证Secret

Kubernetes所提供的内置类型`kubernetes.io/ssh-auth`用来存放SSH身份认证中所需要的凭据。使用这种Secret类型时，必须在其`data`(或`stringData`)字段中提供一个`ssh-privatekey`键值对，作为要使用的SSH凭据。

下面的yaml是一个SSH身份认证Secret的配置示例：

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret-ssh-auth
type: kubernetes.io/ssh-auth
data:
  # 以下只截取键值对中值的部分内容
  ssh-privatekey: |
          MIIEpQIBAAKCAQEAulqb/Y ...
```

提供 SSH 身份认证类型的 Secret 仅仅是出于用户方便性考虑。我们也可以使用 `Opaque ` 类型来保存用于 SSH 身份认证的凭据。不过，使用内置的 Secret 类型有助于对凭据格式进行归一化处理，并且 API 服务器确实会检查 Secret 配置中是否提供了所需要的主键。

> **注意：**SSH私钥自身无法建立SSH客户端与服务器端之间的可信连接。需要其它方式来建立这种信任关系，以缓解"中间人(Man In The Middle)"攻击，例如向ConfigMap中添加一个`known_hosts`文件。

###### 11.2.1.6 TLS Secret

Kubernetes提供的内置类型`kubernetes.io/tls`用来存放证书及其相关密钥(通常用在TLS场合)。此类数据主要提供给Ingress资源，用以终结TLS链接，不过也可以用于其他资源或者负载。当使用此类型的 Secret 时，Secret 配置中的`data`(或`stringData`)字段必须包含`tls.key`和`tls.crt`主键，尽管API服务器实际上并不会对每个键的取值作进一步的合法性检查。

下面的yaml包含一个TLS Secret的配置示例：

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret-tls
type: kubernetes.io/tls
data:
  # 以下只截取键值对中值的部分内容
  tls.crt: |
        MIIC2DCCAcCgAwIBAgIBATANBgkqh ...
  tls.key: |
        MIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ...
```

提供TLS类型的Secret仅仅是出于用户方便性考虑 我们也可以使用`Opaque`类型来保存用于TLS服务器与/或客户端的凭据。不过，使用内置的Secret类型有助于对凭据格式进行归一化处理，并且 API 服务器确实会检查 Secret 配置中是否提供了所需要的主键。

当使用`kubectl`来创建TLS Secret时，我们也可以像下面的例子一样使用`tls`子命令：

```shell
kubectl create secret tls my-tls-secret \
  --cert=path/to/cert/file \
  --key=path/to/key/file
```

以上的公钥/私钥对都必须事先已存在。用于 `--cert` 的公钥证书必须是 .PEM 编码的(Base64编码的 DER 格式)，且与`--key`所给定的私钥匹配。私钥必须是通常所说的PEM私钥格式，且未加密。对这两个文件而言，PEM格式数据的第一行和最后一行(例如，证书所对应的`--------BEGIN CERTIFICATE-----`和`-------END CERTIFICATE----`)都不会包含在其中。

###### 11.2.1.7 启动引导令牌Secret

通过将Secret的 `type` 设置为`bootstrap.kubernetes.io/token`可以创建启动引导令牌类型的Secret。这种类型的Secret被设计用来支持节点的启动引导过程。其中包含用来为周知的ConfigMap签名的令牌。

启动引导令牌 Secret 通常创建于`kube-system`名字空间内，并以`bootstrap-token-<令牌 ID>`的形式命名；其中的令牌ID是一个由6个字符组成的字符串，用作令牌的标识。

以Kubernetes清单文件的形式，某启动引导令牌Secret可能看起来像下面这样：

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: bootstrap-token-5emitj
  namespace: kube-system
type: bootstrap.kubernetes.io/token
data:
  auth-extra-groups: c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=
  expiration: MjAyMC0wOS0xM1QwNDozOToxMFo=
  token-id: NWVtaXRq
  token-secret: a3E0Z2lodnN6emduMXAwcg==
  usage-bootstrap-authentication: dHJ1ZQ==
  usage-bootstrap-signing: dHJ1ZQ==
```

启动引导令牌类型的Secret会在`data`字段中包含如下主键：

- `token-id`：由6个随机字符组成的字符串，作为令牌的标识符。必需。
- `token-secret`：由16个随机字符组成的字符串，包含实际的令牌机密。必需。
- `description`：供用户阅读的字符串，描述令牌的用途。可选。
- `expiration`：一个使用RFC3339来编码的UTC绝对时间，给出令牌要过期的时间。可选。
- `usage-bootstrap-<usage>`：布尔类型的标志，用来标明启动引导令牌的其他用途。
- `auth-extra-groups`：用逗号分隔的组名列表，身份认证时除被认证为`system:bootstrappers`组之外，还会被添加到所列的用户组中。

上面的yaml文件可能看起来令人费解，因为其中的数值均为base64编码的字符串。实际上，我们完全可以使用下面的文件来创建一个一模一样的Secret：

```yaml
apiVersion: v1
kind: Secret
metadata:
  # 注意 Secret 的命名方式
  name: bootstrap-token-5emitj
  # 启动引导令牌 Secret 通常位于 kube-system 名字空间
  namespace: kube-system
type: bootstrap.kubernetes.io/token
stringData:
  auth-extra-groups: "system:bootstrappers:kubeadm:default-node-token"
  expiration: "2020-09-13T04:39:10Z"
  # 此令牌 ID 被用于生成 Secret 名称
  token-id: "5emitj"
  token-secret: "kq4gihvszzgn1p0r"
  # 此令牌还可用于 authentication （身份认证）
  usage-bootstrap-authentication: "true"
  # 且可用于 signing （证书签名）
  usage-bootstrap-signing: "true"
```

##### 11.2.2 创建Secret的几种方式

###### 11.2.2.1 使用kubectl命令创建Secret

一个`Secret`可以包含Pod访问数据库所需的用户凭证。例如，由用户名和密码组成的数据库连接字符串。我们可以在本地计算机上，将用户名存储在文件`username.txt`中，将密码存储在文件`password.txt`中。

```shell
echo -n 'admin' > ./username.txt
echo -n '1f2d1e2e67df' > ./password.txt
```

> **说明**：在以上命令中，`-n`选项确保生成的文件在文本末尾不包含额外的换行符。这一点很重要，因为当`kubectl`读取文件并将内容编码为base64字符串时，多余的换行符也会被编码。

使用`kubectl create secret`命令将这些文件打包成一个Secret并在API服务器上创建对象：

```shell
kubectl create secret generic db-user-pass \
  --from-file=./username.txt \
  --from-file=./password.txt
```

通过以上命令生成的 Secret 的默认密钥名称是文件名，我们可以选择使用`--from-file=[key=]source`来设置密钥名称。例如：

```shell
kubectl create secret generic db-user-pass \
  --from-file=username=./username.txt \
  --from-file=password=./password.txt
```

> **说明**：我们不需要对文件中包含的密码字符串中的特殊字符进行转义。

我们还可以使用`--from-literal=<key>=<value>`形式提供Secret数据。可以多次使用此标签，提供多个键值对。请注意，特殊字符(例如：`$`，`\`，`*`，`=` 和 `!`)由我们的shell解释执行，而且需要转义。

在大多数shell中，转义密码最简便的方法是用单引号括起来。比如，如果我们的密码是`S!B\*d$zDsb=`，可以像下面一样执行命令：

```shell
kubectl create secret generic dev-db-secret \
  --from-literal=username=devuser \
  --from-literal=password='S!B\*d$zDsb='
```

当Secret创建后，如果我们想要查看创建的Secret的内容，可以运行以下命令：

```shell
kubectl get secret db-user-pass -o jsonpath='{.data}'
```

然后会输出类似于下面的内容：

```shell
map[password:MWYyZDFlMmU2N2Rm username:YWRtaW4=]
```

我们可以使用以下命令进行解码查看：

```shell
# 对用户名进行解码
echo 'YWRtaW4=' | base64 -d

# 对密码进行解码
echo 'MWYyZDFlMmU2N2Rm' | base64 -d
```

###### 11.2.2.2 使用配置文件创建Secret

比如，现在要使用Secret的`data`字段存储两个字符串，那么首先需要将字符串转换为base64形式，如下所示：

```shell
#以下命令输出结果为：YWRtaW4=
echo -n 'admin' | base64

#以下命令输出结果为：MWYyZDFlMmU2N2Rm
echo -n '1f2d1e2e67df' | base64
```

然后我们再编写一个Secret的配置文件，如下所示：

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: YWRtaW4=   #这个就是"admin"的base64形式
  password: MWYyZDFlMmU2N2Rm  #这个就是"1f2d1e2e67df"的base64形式
```

编写好以上yaml文件后，只需要使用`kubectl apply -f`命令基于yaml文件创建Secret即可。

我们也可以使用类似ConfigMap那种类文件键的方式创建Secret，比如下面这样：

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
stringData:
  config.yaml: |
    apiUrl: "https://my.api.com/api/v1"
    username: admin
    password: 1f2d1e2e67df 
```

如果在`data`和`stringData`中都指定了同一个字段，比如`username`，那么最终使用的是`stringData`中的。比如：

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: YWRtaW4=
stringData:
  username: administrator
```

基于以上yaml文件创建Secret后，进行查看：

```shell
kubectl get secret mysecret -o yaml|head -5
```

![image-20210702215906737](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210703001255.png) 

上图中通过`echo "YWRtaW5pc3RyYXRvcg=="|base64 -d`命令解码后发现值为**administrator**，说明当同时指定同一个字段的时候，最终使用的确实是`stringData`中的。

###### 11.2.2.3 使用Kustomize创建Secret

我们可以在`kustomization.yaml`中定义`secreteGenerator`，并在定义中引用其他现成的文件，生成Secret。比如下面的kustomization文件引用了`./username.txt`和`./password.txt`文件：

```yaml
secretGenerator:
- name: db-user-pass
  files:
  - username.txt
  - password.txt
```

我们也可以在`kustomization.yaml`文件中指定一些字面量定义`secretGenerator`。比如下面的`kustomization.yaml`文件中包含了`username`和`password`两个字面量：

```yaml
secretGenerator:
- name: db-user-pass
  literals:
  - username=admin
  - password=1f2d1e2e67df
```

我们也可以使用`.env`文件在`kustomization.yaml`中定义`secretGenerator`。比如下面的`kustomization.yaml`文件从`.env.secret`文件获取数据：

```yaml
secretGenerator:
- name: db-user-pass
  envs:
  - .env.secret
```

> **注意**：上面提到的情况，我们都不需要使用base64编码。

比如我们使用以下方式创建一个`kustomization.yaml`文件：

```shell
cat <<EOF >./kustomization.yaml
secretGenerator:
- name: db-user-pass
  literals:
  - username=admin
  - password=1f2d1e2e67df
EOF
```

然后使用`kubectl apply -k .`命令执行，执行后再进行查看：

![image-20210702221809622](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210703001302.png) 

##### 11.2.3 Secret的使用场景

Secret可以作为数据卷被挂载，或作为环境变量暴露出来以供Pod中的容器使用。它们也可以被系统的其他部分使用，而不直接暴露在Pod内。例如，它们可以保存凭据，系统的其他部分将用它来代表我们与外部系统进行交互。

###### 11.2.3.1 以挂载卷的方式使用Secret

在Pod中使用存放在卷中的Secret：

1. 创建一个Secret或者使用已有的Secret。多个Pod可以引用同一个Secret。
2. 修改我们的Pod定义，在`spec.volumes[]`下增加一个卷。可以给这个卷随意命名，但它的`.secret.secretName`字段必须是Secret对象的名字。
3. 将`spec.containers[].volumeMounts[]`加到需要用到该Secre 的容器中。指定其下的`.readOnly`值为true，以及`spec.containers[].volumeMounts[].mountPath`为我们想要该Secret出现的尚未使用的目录。
4. 修改我们的镜像与/或命令行，让程序从该目录下寻找文件。Secret的`data`映射中的每一个键都对应`mountPath`下的一个文件名。

下面是一个在Pod中使用存放在挂载卷中Secret的案例：

- 先创建一个mysecret.yaml文件，内容如下：

  ```yaml
  apiVersion: v1
  kind: Secret
  metadata:
    name: mysecret
  type: Opaque
  data:
    username: YWRtaW4K
  stringData:
    config.yaml: |
      apiUrl: "https://my.api.com/api/v1"
      username: admin
      password: 1f2d1e2e67df 
  ```

- 然后创建一个mypod.yaml文件，内容如下：

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: mypod
  spec:
    containers:
    - name: c-pod
      image: nginx:1.20
      volumeMounts:
      - name: foo
        mountPath: "/etc/foo"
        readOnly: true
    volumes:
    - name: foo
      secret:
        secretName: mysecret
  ```

- 使用`kubectl apply -f`命令分别基于以上两个yaml文件创建对应资源，然后进入到Pod中：

  ```shell
  kubectl exec mypod -it -- bash
  ```

  ![image-20210702225723981](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210703001311.png) 

我们还可以控制Secret键名在存储卷中映射的的路径。我们可以使用`spec.volumes[].secret.items`字段修改每个键对应的目标路径，比如下面这样：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: c-pod
    image: nginx:1.20
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: mysecret
      items:
      - key: username
        path: my-group/my-username
```

> **说明**：以上指定目标路径后，`username`就会存储在Pod的`/etc/foo/my-group/my-username`文件中了，不过也会带来一些问题，就是如果Secret中有多个键的话，一旦有的键指定了目标路径，那么其余的没有在`items`中指定的键就不会再被映射了，除非将所有键都列在`items`字段中。而且所有列出的键名必须存在于相应的Secret中。否则，不会创建卷。

我们还可以指定Secret将拥有的权限模式位。如果不指定，默认使用`0644`。我们可以为整个Secret卷指定默认模式；如果需要，可以为每个密钥设定重载值。

例如，我们可以指定如下默认模式：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: c-pod
    image: nginx:1.20
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
  volumes:
  - name: foo
    secret:
      secretName: mysecret
      defaultMode: 256
```

指定之后，Secret将被挂载到`/etc/foo`目录，而所有通过该Secret卷挂载所创建的文件的权限都是`0400`。需要注意的是，JSON规范不支持八进制符号，因此使用256值作为0400权限。如果我们使用YAML而不是JSON，则可以使用八进制符号以更自然的方式指定权限。

如果我们通过`kubectl exec`命令进入到了Pod中，我们需要沿着符号链接来找到所期望的文件模式。例如，通过下面的一系列操作来检查Secret文件的访问模式：

![image-20210702231943335](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210703001317.png) 

我们还可以使用映射，并为不同的文件指定不同的权限，如下所示：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: c-pod
    image: nginx:1.20
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
  volumes:
  - name: foo
    secret:
      secretName: mysecret
      items:
      - key: username
        path: my-group/my-username
        mode: 511
```

> **说明**：在这里，位于`/etc/foo/my-group/my-username`的文件的权限值为`0777`。 由于JSON限制，必须以十进制格式指定模式，即`511`。需要注意的是，如果稍后读取此权限值，可能会以十进制格式显示。

###### 11.2.3.2 以环境变量的方式使用Secret

将Secret作为Pod中的环境变量使用：

1. 创建一个Secret或者使用一个已存在的Secret。多个Pod可以引用同一个Secret。
2. 修改 Pod 的定义，为每个要使用 Secret 的容器添加对应 Secret 键的环境变量。使用 Secret 键的环境变量应在 `env[x].valueFrom.secretKeyRef` 中指定要包含的 Secret 名称和键名。
3. 更改镜像并与/或命令行，以便程序在指定的环境变量中查找值。

以下是一个使用来自环境变量中的Secret值的Pod示例：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
  - name: mycontainer
    image: nginx:1.20
    command: 
    - /bin/sh
    - -c
    - env;sleep 3600
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: username
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: password
  restartPolicy: Never
```

上面用到的Secret的内容如下：

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm
```

基于以上的yaml文件创建相应的资源后，可以查看Pod打印的环境变量信息，也可以进入容器中通过`echo`命令打印指定的环境变量信息，所有涉及的命令如下：

```shell
#查看Pod运行情况
kubectl get pod -o wide

#查看打印的环境变量信息是否包含引用的secret中的
kubectl logs secret-env-pod

#进入容器
kubectl exec secret-env-pod -it -- bash

#进入容器中单独打印指定的环境变量的信息
echo $SECRET_USERNAME
echo $SECRET_PASSWORD
```

![image-20210703111953290](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210703112021.png)

> **说明**：如果某个容器已经在通过环境变量使用某Secret，那么对该Secret的更新是不会被容器马上看见的，除非容器被重启，或者通过一些第三方的解决方案在Secret发生变化时触发容器重启。

##### 11.2.4 不可更改的Secret

Kubernetes的特性 *不可变的Secret和ConfigMap* 提供了一种可选配置，可以设置各个Secret和ConfigMap为不可变的。对于大量使用Secret的集群(至少有成千上万各不相同的Secret供Pod挂载)，禁止变更它们的数据有下列好处：

- 防止意外(或非预期的)更新导致应用程序中断；
- 通过将Secret标记为不可变来关闭kube-apiserver对其的监视，从而显著降低kube-apiserver的负载，提升集群性能。

这个特性通过`ImmutableEmphemeralVolumes` [特性门控](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/feature-gates/) 来控制，从v1.19开始默认启用。我们可以通过将Secret的`immutable`字段设置为`true`创建不可更改的Secret。 例如：

```yaml
apiVersion: v1
kind: Secret
metadata:
  ...
data:
  ...
immutable: true
```

> **说明**：一旦一个Secret或ConfigMap被标记为不可更改，撤销此操作或者更改`data`字段的内容都是不可能的。只能删除并重新创建这个Secret。现有的Pod将维持对已删除Secret的挂载点，所以建议重新创建这些Pod。

##### 11.2.5 关于imagePullSecret

`imagePullSecrets`字段包含一个列表，列举对同一名称空间中的Secret的引用。我们可以使用`imagePullSecrets`将包含Docker(或其他)镜像仓库密码的Secret传递给kubelet，kubelet使用此信息来替我们的Pod拉取私有镜像。下面进行一个案例演示：

- 在集群中搭建一个harbor仓库(地址：`192.168.68.11:8086`)，登录进去后新建一个名为`test`的项目：

  ![image-20210704202256439](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210704211811.png)

  ![image-20210704202450268](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210704211819.png)  

- 在上面创建的`test`项目中传入一个私有镜像，供下文演示使用：

  ![image-20210704202616743](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210704211824.png) 

- 基于yaml文件创建一个Pod，涉及的yaml文件内容如下：

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-nginx
  spec:
    containers:
    - name: nginx
      image: 192.168.68.11:8086/test/mynginx:1.20
    imagePullSecrets:
    - name: myregistrykey  #这个值要和下文创建的secret的名称保持一致
  ```

- Pod创建成功后，查看Pod的运行情况：

  ![image-20210704203624490](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210704211830.png) 

  > **说明**：由上图可以发现，由于权限问题，镜像下载不下来，导致Pod创建失败。

- 创建一个给Docker registry使用的Secret：

  ```shell
  kubectl create secret docker-registry myregistrykey\
    --docker-server=192.168.68.11:8086 \
    --docker-username=admin \
    --docker-password=admin \
    --docker-email=registrykey@163.com
  ```
  
  ![image-20210704205018157](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210704211836.png)
  
  > **说明**：docker-registry后跟的是secret的名称，docker-server后跟的是harbor仓库的地址，docker-username后跟的是harbor的用户名，docker-password后跟的是harbor的密码，docker-email后跟的是harbor的邮箱，除了邮箱外，其余参数必须填写正确。

##### 11.2.6 关于Secret的一些说明

1. Kubernetes会验证Secret作为卷来源时所给的对象引用确实指向一个类型为Secret的对象。因此，Secret需要先于任何依赖于它的Pod创建，而且Secret只能被同一名称空间内的Pod引用；
2. 每个Secret的大小限制为 1MB。这是为了防止创建非常大的Secret导致API服务器和kubelet的内存耗尽。然而，创建过多较小的Secret也可能耗尽内存；
3. 如果以环境变量形式在Pod中使用Secret之前必须先创建Secret，除非该环境变量被标记为可选的。Pod中引用不存在的Secret时将无法启动；
4. 使用`secretKeyRef`时，如果引用了指定Secret不存在的键，对应的Pod也无法启动；
5. 对于通过`envFrom`填充环境变量的Secret，如果Secret中包含的键名无法作为合法的环境变量名称，对应的键会被跳过，该Pod将被允许启动。不过这时会产生一个事件，其原因为`InvalidVariableNames`，其消息中包含被跳过的无效键的列表；
6. 通过API创建Pod时，不会检查引用的Secret是否存在。一旦Pod被调度，kubelet 就会尝试获取该Secret的值。如果获取不到该Secret，或者暂时无法与API服务器建立连接，kubelet将会定期重试。kubelet将会报告关于Pod的事件，并解释它无法启动的原因。一旦获取到Secret，kubelet将创建并挂载一个包含它的卷。在Pod的所有卷被挂载之前，Pod中的容器不会启动；
7. 因为Secret对象可以独立于使用它们的Pod而创建，所以在创建、查看和编辑Pod的流程中Secret被暴露的风险较小。系统还可以对Secret对象采取额外的预防性保护措施，例如，在可能的情况下避免将其写到磁盘；
8. 只有当某节点上的Pod需要用到某Secret时，该Secret才会被发送到该节点上。Secret不会被写入磁盘，而是被kubelet存储在tmpfs中。一旦依赖于它的Pod被删除，Secret数据的本地副本就会被删除；
9. 同一节点上的很多个Pod可能拥有多个Secret。但是，只有Pod所请求的Secret在其容器中才是可见的。因此，一个Pod不能访问另一个Pod的Secret；
10. 同一个Pod中可能有多个容器。但是，Pod中的每个容器必须通过 `volumeeMounts` 请求挂载Secret 卷才能使卷中的Secret对容器可见；
11. 我们可以为Secret数据开启[静态加密](https://kubernetes.io/zh/docs/tasks/administer-cluster/encrypt-data/)，这样Secret数据就不会以明文形式存储到[etcd](https://kubernetes.io/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/)中了。

### 12.k8s核心技术-存储管理

#### 12.1 Volume

Container中的文件在磁盘上是临时存放的，这就给Container中运行的较重要的应用程序带来了一些问题，比如当容器崩溃时文件丢失。虽然kubelet会重新启动容器，但容器会以干净的状态重启。同一Pod中运行多个容器并共享文件时，也会出现一些问题，而卷(Volume)就可以解决以上提到的问题。

Kubernetes支持很多类型的卷。Pod可以同时使用任意数目的卷类型。临时卷类型的生命周期与Pod相同，但持久卷可以比Pod的存活期长。当Pod不再存在时，Kubernetes也会销毁临时卷；不过Kubernetes不会销毁持久卷。对于给定Pod中任何类型的卷，在容器重启期间数据都不会丢失。

卷的核心是一个目录，其中可能存有数据，Pod中的容器可以访问该目录中的数据。所采用的特定的卷类型将决定该目录是如何形成的、使用何种介质保存数据以及目录中存放的内容。

使用卷时，在`.spec.volumes`字段中设置为Pod提供的卷，并在`.spec.containers[*].volumeMounts`字段中声明卷在容器中的挂载位置。

##### 12.1.1 Volume的类型

Kubernetes支持很多类型的卷，比如下面这些：

<kbd>awsElasticBlockStore</kbd>，<kbd>azureDisk</kbd>，<kbd>azureFile</kbd>，<kbd>cephfs</kbd>，<kbd>cinder</kbd>，<kbd>**configMap**</kbd>，<kbd>downwardAPI</kbd>，<kbd>**emptyDir**</kbd>，<kbd>fc</kbd>，<kbd>gcePersistentDisk</kbd>，<kbd>**glusterfs**</kbd>，<kbd>**hostPath**</kbd>，<kbd>iscsi</kbd>，<kbd>**local**</kbd>，<kbd>**nfs**</kbd>，<kbd>**persistentVolumeClaim**</kbd>，<kbd>portworxVolume</kbd>，<kbd>projected</kbd>，<kbd>quobyte</kbd>，<kbd>rbd</kbd>，<kbd>**secret**</kbd>，<kbd>storageOS</kbd>，<kbd>vsphereVolume</kbd>

下文会对以上提到的`emptyDir`、`hostPath`等常用类型进行一些简单的介绍。

###### 12.1.1.1 emptyDir

当Pod被分派到某个Node上时，`emptyDir`卷会被创建，并且在Pod在该节点上运行期间，卷一直存在。就像其名称表示的那样，卷最初的时候是空的。尽管Pod中的多个容器挂载`emptyDir`卷的路径可能相同也可能不同，但是这些容器都可以读写`emptyDir`卷中相同的文件。当Pod因为某些原因被从节点上删除时，`emptyDir`卷中的数据也会被永久删除。

> **说明：**容器崩溃并不会导致Pod被从节点上移除，因此容器崩溃期间`emptyDir`卷中的数据是安全的。

`emptyDir`的一些用途：

- 缓存空间，例如基于磁盘的归并排序；
- 为耗时较长的计算任务提供检查点，以便任务能方便地从崩溃前状态恢复执行；
- 在Web服务器容器服务数据时，保存内容管理器容器获取的文件。

下面进行一个案例演示：

- 基于yaml文件创建一个Pod，对应的yaml文件内容如下：

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: test-pod
  spec:
    containers:
    - image: nginx:1.20
      name: c-nginx
      volumeMounts:
      - mountPath: /test1
        name: test-volume
    - image: tomcat:8
      name: c-tomcat
      volumeMounts:
      - mountPath: /test2
        name: test-volume
    volumes:
    - name: test-volume
      emptyDir: {}
  ```

  ![image-20210705163637420](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234436.png) 

- 打开两个终端窗口，分别通过以下两个命令进入到Pod中的两个容器内：

  ```shell
  #进入到名为c-nginx的容器中
  kubectl exec test-pod -c c-nginx -it -- bash
  
  #进入到名为c-tomcat的容器中
  kubectl exec test-pod -c c-tomcat -it -- bash
  ```

- 在名为c-nginx的容器中的`/test1`目录下使用`date > index.html`命令将当前时间输入到新建的index.html文件中，我们再到名为c-tomcat的容器中的`/test2`目录下查看，是可以看到该文件的，内容也一致。再次修改该文件的内容，c-nginx容器的`/test1`目录下的index.html的文件的内容也会同步更新，最终实现了同一Pod中的多个容器可以读写使用`emptyDir`卷挂载的相同的文件。

  ![20210705181249](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234443.gif) 

###### 12.1.1.2 glusterfs

`glusterfs`卷能将[Glusterfs](https://www.gluster.org/)(一个开源的网络文件系统)挂载到我们的Pod中。不像`emptyDir`那样会在删除Pod的同时也会被删除，`glusterfs`卷的内容在删除Pod时会被保存，卷只是被卸载。这意味着`glusterfs`卷可以被预先填充数据，并且这些数据可以在Pod之间共享。GlusterFS可以被多个写者同时挂载。

> **说明：**在使用前我们必须先安装运行自己的GlusterFS。

更多详情可以参考[GlusterFS示例](https://github.com/kubernetes/examples/tree/master/volumes/glusterfs)。

下面演示一下glusterfs集群的安装和使用，具体安装过程也可以查看[官网](https://docs.gluster.org/en/latest/Quick-Start-Guide/Quickstart/)。

- 首先准备三台裸机器和一台客户端机器，然后依次设置主机名：

  ```shell
  #在192.168.68.20主机执行以下命令
  hostnamectl set-hostname glusterfs-01
  
  #在192.168.68.21主机执行以下命令
  hostnamectl set-hostname glusterfs-02
  
  #在192.168.68.22主机执行以下命令
  hostnamectl set-hostname glusterfs-03
  
  #在192.168.68.30主机执行以下命令
  hostnamectl set-hostname glusterfs-client
  ```

- 然后分别在四台主机上执行以下命令：

  ```shell
  cat >> /etc/hosts << EOF
  192.168.68.20 glusterfs-01
  192.168.68.21 glusterfs-02
  192.168.68.22 glusterfs-03
  192.168.68.30 glusterfs-client
  EOF
  ```

- 关闭防火墙以及配置yum源：

  ```shell
  #分别在四台主机上执行如下命令关闭防火墙
  systemctl stop firewalld
  
  #分别在四台主机上执行如下命令配置yum源
  yum install -y centos-release-gluster
  ```

- 分别在四台主机上执行以下命令来安装并启动GlusterFS：

  ```shell
  #安装glusterfs
  yum install -y glusterfs-server
  
  #启动glusterfs并设置开机启动
  systemctl start glusterd && systemctl enable glusterd
  
  #查看运行状态
  systemctl status glusterd
  ```

  ![image-20210715100719563](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717004814.png) 

- 将服务端的三台主机(glusterfs-01，glusterfs-02，glusterfs-03)拉到一个集群中：

  ```shell
  #我们可以在服务端的任一主机上通过执行"gluster peer probe"命令创建集群，这里就在glusterfs-01主机上执行
  gluster peer probe glusterfs-02
  gluster peer probe glusterfs-03
  ```

- 集群创建成功后，通过`gluster pool list`命令查看集群状态：

  ![image-20210715101913335](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717004822.png)  

  或者也可以通过`gluster peer status`命令查看：

  ![image-20210715103117295](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717004827.png) 

  glusterfs是无中心管理节点的，所以能在glusterfs-01上执行的操作，在别的节点上也是能执行的，比如我们也可以在glusterfs-02节点上执行以上相同的命令：

  ![image-20210715103329558](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717004834.png)  

- 我已经在集群中的所有节点上通过分区的方式对外提供一个`/data/brick`目录，到时候这个目录就是glusterfs的共享目录了，单独找一块分区作为共享目录可以降低数据丢失的风险。

  ![image-20210715112408146](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717004841.png) 

- 接下来就可以创建一个GlusterFS卷了，我们可以在集群中的任一节点上执行以下命令，比如glusterfs-01节点：

  ```shell
  #先创建一个挂载目录，比如gv0
  mkdir -p /data/brick/gv0
  
  #然后创建一个gv0的卷
  gluster volume create gv0 replica 3 glusterfs-0{1..3}:/data/brick/gv0
  ```

  ![image-20210715114658176](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717004848.png) 

- 卷创建成功后，可以通过`gluster volume start gv0`命令启动该卷，启动后通过`gluster volume status gv0`命令可以查看该卷的状态：

  ![image-20210715115428010](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717004856.png) 

  我们可以通过`gluster volume info gv0`命令查看该卷的详情，如果有多个卷，也可以通过`gluster volume info`命令查看所有卷的详情：

  ![image-20210715120016424](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717004906.png) 

  以上`Replicate`类型的意思是，会将共享文件保存到集群中的每个`/data/brick/gv0`目录下，这种方式虽然会导致文件的重复，但是却是最安全的，即便某个节点挂了，也不会导致文件的丢失，原理如下图所示：

  ![New-ReplicatedVol](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717004913.png) 

  > **说明**：具体有哪几种类型，以及每个内容的用法和含义，可以参考[官方文档](https://docs.gluster.org/en/latest/Quick-Start-Guide/Architecture/)。

- 如果以上`gv0`卷未能正常启动，可以通过`/var/log/glusterfs/glusterd.log`日志文件排查原因，如果已经正常启动，就可以在客户端机器(即glusterfs-client主机)上进行目录挂载了，在客户端机器上执行以下命令：

  ```shell
  #先创建一个挂载目录
  mkdir /mnt
  
  #开始进行目录挂载
  mount -t glusterfs glusterfs-01:gv0 /mnt
  ```

  挂载完成后，我们在glusterfs-client主机上通过`df -h`命令是可以查看到相应的记录的：

  ![image-20210715122731823](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717004922.png) 

- 挂载成功后，我们在glusterfs-client主机的`/mnt`目录下新建一些文件进行测试，观察这些文件是否都能够同步复制到glusterfs集群中每个节点的`/data/brick/gv0`目录下：

  ```shell
  #在glusterfs-client主机上执行以下命令
  for i in `seq -w 1 10`; do cp -rp /var/log/messages /mnt/copy-test-$i; done
  ```

  ![image-20210715123354406](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717004929.png) 

  通过以上截图可知，glusterfs-client主机的`/mnt`目录下已经新增了10个文件，现在到glusterfs集群中查看：

  ![image-20210715123951827](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717004944.png) 

  > **说明**：通过以上截图可知，glusterfs-client主机的`/mnt`目录下新增的文件，都已经同步复制到glusterfs集群中每个节点的`/data/brick/gv0`目录下了。至此，glusterfs集群安装成功，并测试通过。

###### 12.1.1.3 hostPath

`hostPath`卷能在Pod和其所在宿主机之间实现目录共享。虽然这不是大多数Pod需要的，但是它为一些应用程序提供了强大的逃生舱。

例如，`hostPath`的一些用法有：

- 运行一个需要访问Docker内部机制的容器，可使用`hostPath`挂载`/var/lib/docker`路径；
- 在容器中运行cAdvisor时，以`hostPath`方式挂载`/sys`；
- 允许Pod指定给定的`hostPath`在运行Pod之前是否应该存在，是否应该创建以及应该以什么方式存在。

除了必需的`path`属性之外，用户可以选择性地为`hostPath`卷指定`type`。

支持的`type`值如下：

| 取值                | 行为                                                         |
| :------------------ | :----------------------------------------------------------- |
|                     | 空字符串(默认)用于向后兼容，这意味着在安装hostPath卷之前不会执行任何检查。 |
| `DirectoryOrCreate` | 如果在给定路径上什么都不存在，那么将根据需要创建空目录，权限设置为0755，具有与kubelet相同的组和属主信息。 |
| `Directory`         | 在给定路径上必须存在的目录。                                 |
| `FileOrCreate`      | 如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为0644，具有与kubelet相同的组和所有权。 |
| `File`              | 在给定路径上必须存在的文件。                                 |
| `Socket`            | 在给定路径上必须存在的UNIX套接字。                           |
| `CharDevice`        | 在给定路径上必须存在的字符设备。                             |
| `BlockDevice`       | 在给定路径上必须存在的块设备。                               |

当使用这种类型的卷时要小心，因为：

- 具有相同配置(例如基于同一PodTemplate创建)的多个Pod会由于节点上文件的不同而在不同节点上有不同的行为；
- 下层主机上创建的文件或目录只能由root用户写入，我们需要在[特权容器](https://kubernetes.io/zh/docs/tasks/configure-pod-container/security-context/)中以root身份运行进程，或者修改主机上的文件权限以便容器能够写入`hostPath`卷。

下面是hostPath的一个简单案例：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: gongcqq/myapp:3.0
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # 宿主机上的目录位置
      path: /test
      # 此字段为可选
      type: Directory
```

![image-20210706103616731](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234459.png) 

> **说明**：上述案例中使用的hostPath的类型是`Directory`，这种类型的含义是，我们指定的`/test`目录必须是要先于该Pod在宿主机上存在的，不存在的话，Pod是无法正常运行的，就像上面截图的那样。

我们也可以不设置hostPath的类型，默认就是空字符串，这种是可以向后兼容的，比如下面这样：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: gongcqq/myapp:3.0
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # 宿主机上的目录位置
      path: /test
```

![image-20210706104935745](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234507.png) 

> **说明**：通过上图可知，Pod被调度到了node1节点上，所以node1节点上也会被自动创建一个`/test`目录。

进入到Pod中容器的挂载目录里，新增一个文件测试一下，看是否同步更新至宿主机对应的挂载目录中：

```shell
kubectl exec test-pd -it -- sh
```

![image-20210706105613334](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234514.png) 

> **说明**：进入到Pod中的容器的挂载目录中后，新建一个index.html文件，并将当前时间存入其中。

由于Pod是被调度到node1节点上了，所以到该节点对应的挂载目录中进行查看，看数据是否通过过去了：

![image-20210706110002051](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234521.png)  

> **说明**：通过以上截图可知，我们在容器中新增的内容已经同步到了宿主机对应的挂载目录中了，如果我们在宿主机对应的挂载目录中新增文件，也是会同步到容器对应的挂载目录中的。

**关于FileOrCreate类型的一些情况：**

`FileOrCreate`类型不会负责创建文件的父目录。如果欲挂载的文件的父目录不存在，Pod启动会失败。为了确保这种模式能够工作，可以尝试把文件和它对应的目录分开挂载，比如下面这样：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-app
spec:
  containers:
  - name: test-app
    image: gongcqq/myapp:3.0
    volumeMounts:
    - mountPath: /var/local/aaa
      name: mydir
    - mountPath: /var/local/aaa/1.txt
      name: myfile
  volumes:
  - name: mydir
    hostPath:
      # 确保文件所在目录成功创建。
      path: /var/local/aaa
      type: DirectoryOrCreate
  - name: myfile
    hostPath:
      path: /var/local/aaa/1.txt
      type: FileOrCreate
```

###### 12.1.1.4 nfs

`nfs`卷能将NFS(网络文件系统)挂载到我们的Pod中。不像`emptyDir`那样会在删除Pod的同时也会被删除，`nfs`卷的内容在删除Pod时会被保存，卷只是被卸载。这意味着`nfs`卷可以被预先填充数据，并且这些数据可以在Pod之间共享。

> **注意**：在使用NFS卷之前，我们必须运行自己的NFS服务器并将目标share导出备用。

下面开始安装nfs，并进行简单的案例演示：

**安装nfs服务并演示主机目录共享：**

- 为了演示nfs可以将集群内部主机的文件挂载至集群外部的主机上，下面在一个非集群内部主机(192.168.68.10)上安装nfs：

  ```shell
  yum install -y nfs-utils rpcbind
  ```

  ![image-20210707222317148](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234529.png) 

- nfs服务安装成功后，默认会创建一个**nfsnobody**用户，我们可以通过`cut -d: -f1 /etc/passwd`命令进行查看：

  ![image-20210707223704731](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234534.png) 

- 在10主机成功安装nfs服务后，接下来在k8s集群中每个主机上也通过`yum install -y nfs-utils rpcbind`命令安装nfs服务；

- 上一步安装完成后，在10主机上通过`mkdir /nfstest`命令创建一个用于挂载集群中文件的目录，然后对目录赋权：

  ```shell
  chmod 777 /nfstest && chown nfsnobody /nfstest
  ```

- 接下来通过`vim /etc/exports`命令打开exports文件，将以下内容加入到该文件中后保存退出：

  ```shell
  # 给所有主机针对该目录添加读写等权限
  /nfstest *(rw,sync,no_root_squash)
  ```

  > **说明**：关于以上`rw`、`sync`等参数的详细说明，可以参考[nfs官方解释](http://nfs.sourceforge.net/nfs-howto/ar01s03.html)。

- 然后启动nfs服务，并设置开启启动：

  ```shell
  systemctl start rpcbind && systemctl enable rpcbind
  systemctl start nfs && systemctl enable nfs
  ```

- 启动nfs服务后，我们可以通过`ps -ef|grep nfs`命令查看运行情况：

  ![image-20210707222745896](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234542.png) 

- 然后我们可以在k8s集群的任意一台主机上执行`showmount -e 192.168.68.10`命令查看nfs可以用于挂载的目录：

  ![image-20210707225220755](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234549.png) 

  > **说明**：通过上图可以发现，命令展现的目录就是我们在10主机上创建的目录。

- 之后我们可以在k8s集群中的一个或多个主机上执行`mkdir /test`命令创建一个共享目录，然后通过以下命令和10主机上的挂载目录进行关联：

  ```shell
  mount -t nfs 192.168.68.10:/nfstest /test/
  ```

- 然后回到10主机，通过`cd /nfstest/ && date > index.html`命令在挂载目录下创建一个文件：

  ![image-20210707225857619](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234554.png) 

- 当我们再次回到k8s集群中创建`/test`目录的主机上时，发现该目录下也会存在一个index.html文件，而且该文件内容和10主机上`/nfstest`目录下的index.html文件中的一样，说明挂载成功了。

  ![image-20210707230405272](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234559.png) 

  > **说明**：如果我们在集群主机上的`/test`目录下新增内容，也会同步到10主机的`/nfstest`目录下。

- 如果我们想要解除挂载，只需要在集群中创建`/test`目录的主机上执行`umount /test/`命令即可，解除挂载后，集群主机中`/test`目录中的内容就会被删除，但是10主机中`/nfstest`目录下的内容还会被保留。

  > **注意**：我们必须在`/test`目录外才能执行解除挂载的命令，如果已经进入到这个目录中了的话，执行解除挂载的命令是无效的。

**演示通过nfs进行容器文件挂载：**

- 新建一个Pod，对应的yaml内容如下：

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: nfs-pd
  spec:
    containers:
    - image: nginx:1.20
      name: c-nfs
      volumeMounts:
      - mountPath: /usr/share/nginx/html
        name: wwwroot
    volumes:
    - name: wwwroot
      nfs:
        server: 192.168.68.10
        path: /nfstest
  ```

- 基于以上yaml文件创建Pod后，查看其运行情况：

  ![image-20210707232307107](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234609.png) 

- 通过`kubectl exec nfs-pd -it -- bash`命令进入到Pod中，然后在**/usr/share/nginx/html**目录下创建一个名为index.html的文件，并将当前时间写入文件内：

  ![image-20210707233129486](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234616.png) 

- 回到10主机，当我们执行`cat /nfstest/index.html`命令时，发现也存在index.html文件，并且该文件的内容和集群中正在运行的Pod的容器中对应路径下同名文件的内容是一样的，所以容器文件通过nfs挂载成功。

  ![image-20210707233651494](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234622.png)

  > **说明**：如果我们删除了Pod，该Pod中对应路径下挂载的文件依然会存在于10主机的`/nfstest`目录下。

要了解更多详情可以参考[NFS示例](https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs)。

###### 12.1.1.5 local

`local`卷所代表的是某个被挂载的本地存储设备，例如磁盘、分区或者目录。`local`卷只能用作静态创建的持久卷，尚不支持动态配置。与`hostPath`卷相比，`local`卷能够以持久和可移植的方式使用，而无需手动将Pod调度到节点。系统通过查看PersistentVolume的节点亲和性配置，就能了解卷的节点约束。

然而，`local`卷仍然取决于底层节点的可用性，并不适合所有应用程序。如果节点变得不健康，那么`local`卷也将变得不可被Pod访问。使用它的Pod将不能运行。使用`local`卷的应用程序必须能够容忍这种可用性的降低，以及因底层磁盘的耐用性特征而带来的潜在的数据丢失风险。

下面是一个使用`local`卷和`nodeAffinity`的持久卷示例：

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 100Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/disks/ssd1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - example-node
```

使用`local`卷时，我们需要设置PV对象的`nodeAffinity`字段。Kubernetes调度器使用PV的`nodeAffinity`信息来将使用`local`卷的Pod调度到正确的节点。

PersistentVolume对象的`volumeMode`字段可被设置为"Block"(而不是默认值"Filesystem")，以将`local`卷作为原始块设备暴露出来。

使用`local`卷时，建议创建一个StorageClass并将其`volumeBindingMode`设置为`WaitForFirstConsumer`。延迟卷绑定的操作可以确保Kubernetes在为PersistentVolumeClaim作出绑定决策时，会评估Pod可能具有的其他节点约束，例节点资源需求、节点选择器、Pod亲和性和Pod反亲和性。

我们可以在Kubernetes之外单独运行静态驱动以改进对local卷的生命周期管理。请注意，此驱动尚不支持动态配置。有关如何运行外部`local`卷驱动，可以参考[local卷驱动用户指南](https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner)。

> **说明**：如果不使用外部静态驱动来管理卷的生命周期，用户需要手动清理和删除local类型的持久卷。

###### 12.1.1.6 persistentVolumeClaim

`persistentVolumeClaim`卷用来将**持久卷**(PersistentVolume)挂载到Pod中。PersistentVolumeClaim(PVC)是用户在不知道特定云环境细节的情况下"申领"持久存储(例如GCE PersistentDisk或者iSCSI volume)的一种方法。

更多详情可以参考[持久卷示例](https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/)。

##### 12.1.2 使用subPath

有时，在单个Pod中共享卷以供多方使用是很有用的。`volumeMounts.subPath`属性可用于指定所引用的卷内的子路径，而不是其根路径。案例演示如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: nginx:1.20
    name: c-nginx
    volumeMounts:
    - mountPath: /test-nginx
      name: volume-sub
      subPath: nginx
  - image: tomcat:8
    name: c-tomcat
    volumeMounts:
    - mountPath: /test-tomcat
      name: volume-sub
      subPath: tomcat
  volumes:
  - name: volume-sub
    hostPath:
      path: /test
      type: DirectoryOrCreate
```

当基于以上yaml文件创建Pod后，到Pod被调度到的worker节点上执行`tree /test`命令，可以发现，在test目录下默认被创建了nginx目录和tomcat目录，这也说明`subPath`属性生效了：

![image-20210706181843367](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234629.png) 

然后我们可以使用如下命令分别进入到Pod中的两个容器内，并在容器中对应的挂载目录下新增文件：

```shell
#进入到名为c-nginx的容器中
kubectl exec test-pd -c c-nginx -it -- bash

#进入到名为c-tomcat的容器中
kubectl exec test-pd -c c-tomcat -it -- bash
```

![image-20210706182421238](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234635.png) 

当我们再次查看宿主机对应的挂载目录时，可以发现，在容器中新增的文件已经同步到宿主机对应的挂载目录下了：

![image-20210706182735244](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234657.png) 

通过使用`subPath`的方式，虽然可以指定子路径，但是这个路径是固定写死的，我们可以通过`subPathExpr`属性基于环境变量来构造`subPath`目录名，案例演示如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-app
spec:
  containers:
  - name: c-test
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    image: gongcqq/myapp:3.0
    command: [ "/bin/sh", "-c", "while [ true ]; do date >> /logs/hello.txt;sleep 60; done" ]
    volumeMounts:
    - name: sub-volume
      mountPath: /logs
      subPathExpr: $(POD_NAME)
  restartPolicy: Never
  volumes:
  - name: sub-volume
    hostPath:
      path: /test
      type: DirectoryOrCreate
```

基于以上yaml文件创建Pod后，查看运行情况：

![image-20210706191914316](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234705.png) 

由上图可知，Pod运行在了node1节点上，下面查看该节点的`/test`目录下是否存在和Pod名称同名的子目录：

![image-20210706192218455](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210707234710.png) 

由以上截图可知，确实存在一个和Pod名称同名的子目录，并且Pod运行的命令是每分钟将当前时间写入到hello.txt中，通过查看宿主机上对应的挂载目录可知，Pod中的hello.txt文件的内容也同步更新到宿主机对应的挂载目录中了。

#### 12.2 PV和PVC的使用

这一章节主要讲解共享存储的使用，涉及的主要概念就是PersistentVolume(PV)、PersistentVolumeClaim(PVC)以及存储类(StorageClass)。

前面在讲存储卷(Volume)的时候提到`emptyDir`类型的Volume可以为Pod中的多个容器之间提供目录共享，但是这种方式的缺陷是，在Pod被删除后，共享目录中的数据也会被删除。

当然，我们也可以通过`hostPath`的方式将Pod中的数据挂载到宿主机上，这样在Pod被删除后，挂载的数据依然可以存在于宿主机上。但是当我们通过Deployment等方式管理Pod的时候，每当对Pod进行升级或者重启等操作后，Pod所在宿主机是在不断变化的，这也就使得挂载目录中的文件可能会存在于集群中的任何一个worker节点上，这就导致了管理上的不便。我们虽然也可以通过打标签的方式固定节点调度，以便使得挂载内容固定到指定的worker节点上。但是一旦该节点出现问题，就会导致Pod无法被调度，而且由于数据没有备份，所以节点一挂，挂载的数据就会随之丢失。基于以上问题，共享存储就显得尤为重要了。为了能够屏蔽共享存储底层存储实现的细节，让用户方便使用，同时让管理员方便管理，k8s便引入了PV和PVC的概念，并通过这两个资源对象来实现对存储的管理。

关于Pod、PVC、PV、StorageClass以及存储服务之间的关系如下图所示：

![20210711111234](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005058.jpg) 

##### 12.2.1 基本概念

持久卷(PersistentVolume，PV)是集群中的一块存储，可以由管理员事先供应，或者使用[存储类(Storage Class)](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/)来动态供应。持久卷是集群资源，就像节点也是集群资源一样。PV持久卷和普通的Volume一样，也是使用卷插件来实现的，只是它们拥有独立于任何使用PV的Pod的生命周期。此API对象中记述了存储的实现细节，无论其背后是NFS、iSCSI 还是特定于云平台的存储系统。

持久卷申领(PersistentVolumeClaim，PVC)表达的是用户对存储的请求，概念上与Pod类似。Pod会消耗节点的CPU或者内存等资源，而PVC则会消耗PV资源。Pod可以请求特定数量的资源(CPU、内存)，同样PVC也可以请求特定大小和访问模式的PV(例如，可以要求PV卷能够以ReadWriteOnce、ReadOnlyMany或ReadWriteMany模式之一来挂载等)。

尽管PersistentVolumeClaim允许用户消耗抽象的存储资源，但是常见的情况是针对不同的问题，用户需要的是具有不同属性(比如性能)的PersistentVolume卷。集群管理员需要能够提供不同性质的PersistentVolume，并且这些PV卷之间的差别不仅限于卷大小和访问模式，同时又不能将卷是如何实现的这些细节暴露给用户。为了满足这类需求，就有了 存储类(StorageClass)资源。

##### 12.2.2 生命周期

PV卷是集群中的资源。PVC是对这些资源的请求，也被用来执行对资源的申领检查。PV卷和PVC申领之间的互动遵循如下生命周期：

###### 供应

PV卷的供应有两种方式：`静态供应`和`动态供应`。

**静态供应：**

由集群管理员手动创建若干PV卷。这些卷对象带有真实存储的细节信息，并且对集群用户可用。PV卷对象存在于k8s API中，可供用户消费使用。

**动态供应：**

如果管理员所创建的所有静态PV卷都无法与用户的PersistentVolumeClaim匹配，那么可以尝试为该PVC动态供应一个存储卷。这一供应操作是基于StorageClass来实现的。PVC必须请求某个存储类，同时集群管理员必须已经创建并配置了该类，这样动态供应卷的动作才会发生。如果PVC指定存储类为`""`，则相当于为自身禁止使用动态供应的卷。

为了基于存储类完成动态的存储供应，集群管理员需要在API服务器上启用`DefaultStorageClass`[准入控制器](https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)。举例而言，可以通过保证`DefaultStorageClass`出现在API服务器组件的`--enable-admission-plugins`标志值中实现这点，该标志的值可以是逗号分隔的有序列表。关于API服务器标志的更多信息，可以参考[kube-apiserver](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/)文档。

###### 绑定

当用户创建一个带有特定存储容量和特定访问模式需求的PVC对象时，主控节点中的控制回路监测到该PVC对象，会寻找与之匹配的PV卷(如果可能的话)，并将二者绑定到一起。如果是通过StorageClass为新的PVC动态供应了PV卷，则控制回路就会总是将该PV卷绑定到这个PVC上。一旦绑定关系建立，则PVC绑定就是排他性的，无论该PVC是如何与PV卷建立的绑定关系。PVC与PV卷之间的绑定是一种一对一的映射关系，实现上使用ClaimRef来记述PV卷与PVC间的双向绑定关系。

如果找不到匹配的PV卷，PVC会无限期地处于未绑定状态。当出现与之匹配的PV卷时，PVC才会被绑定。例如，即使某集群上供应了很多50Gi大小的PV卷，也无法与请求100Gi大小的存储的PVC匹配。只有新的100Gi的PV卷被加入到集群时，该PVC才有可能被绑定。

###### 使用

Pod会将PVC当做存储卷(volume)来使用。集群会检查PVC，找到所绑定的卷，并为Pod挂载该卷。对于支持多种访问模式的卷，用户要在Pod中以卷的形式使用申领时指定期望的访问模式。

一旦用户有了申领对象并且该申领已经被绑定，则所绑定的PV卷会在用户仍然需要它期间一直属于该用户。用户通过在Pod的`volumes`块中包含`persistentVolumeClaim`来调度Pod，访问所申领的PV卷。

###### 保护使用中的存储对象

保护使用中的存储对象(Storage Object in Use Protection)这一功能特性的目的是确保仍被Pod使用的PVC对象及其所绑定的PV对象在系统中不会被删除，因为这样做可能会引起数据丢失。

> **说明：**当使用某PVC的Pod对象仍然存在时，认为该PVC仍被此Pod使用。

如果用户删除被某Pod使用的PVC对象，该PVC不会被立即移除。PVC对象的移除会被推迟，直至其不再被任何Pod使用。此外，如果管理员删除已绑定到某PVC的PV卷，该PV卷也不会被立即移除。PV对象的移除也要推迟到该PV不再绑定到PVC。

当我们看到PVC的状态为`Terminating`且其`Finalizers`列表中包含`kubernetes.io/pvc-protection`时，说明PVC对象是处于被保护状态的。好比下面这样：

```shell
kubectl describe pvc hostpath
```

```shell
Name:          hostpath
Namespace:     default
StorageClass:  example-hostpath
Status:        Terminating
Volume:
Labels:        <none>
Annotations:   volume.beta.kubernetes.io/storage-class=example-hostpath
               volume.beta.kubernetes.io/storage-provisioner=example.com/hostpath
Finalizers:    [kubernetes.io/pvc-protection]
...
```

当我们看到PV对象的状态为`Terminating`且其`Finalizers`列表中包含`kubernetes.io/pv-protection`时，说明PV对象是处于被保护状态的。好比下面这样：

```shell
kubectl describe pv task-pv-volume
```

```shell
Name:            task-pv-volume
Labels:          type=local
Annotations:     <none>
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    standard
Status:          Terminating
Claim:
Reclaim Policy:  Delete
Access Modes:    RWO
Capacity:        1Gi
Message:
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /tmp/data
    HostPathType:
Events:            <none>
```

###### 回收策略

当用户不再使用其存储卷时，他们可以从API中将PVC对象删除，从而允许该资源被回收再利用。PV对象的回收策略告诉集群，当其被从申领中释放时如何处理该数据卷。目前，数据卷支持以下三种策略：

- `Retain`：保留；
- `Recycle`：回收；
- `Delete`：删除。

**保留(Retain)**：

回收策略`Retain`使得用户可以手动回收资源。当PersistentVolumeClaim对象被删除时，PersistentVolume卷仍然存在，对应的数据卷被视为"已释放(`released`)"。但是由于卷上仍然存在着前一申领人的数据，所以该卷还不能用于其他申领。管理员可以通过下面的步骤来手动回收该卷：

1. 删除PV对象。与之相关的、位于外部基础设施中的存储服务(如AWS EBS、GCE PD、Azure Disk或Cinder卷)在PV被删除之后仍然存在；
2. 根据情况，手动清除所关联的存储服务上的数据；
3. 手动删除所关联的存储服务。如果我们希望重新使用该存储服务，可以基于存储服务的定义创建新的PV卷对象。

**删除(Delete)**：

对于支持`Delete`回收策略的卷插件，删除动作会将PersistentVolume对象从Kubernetes中移除，同时也会从外部基础设施(如AWS EBS、GCE PD、Azure Disk或Cinder卷）中移除所关联的存储服务。动态供应的卷会继承其[存储类中设置的回收策略](https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/#reclaim-policy)，该策略默认为`Delete`。管理员需要根据用户的期望来配置StorageClass，否则PV卷被创建之后必须要被编辑或者修补。参阅[更改PV卷的回收策略](https://kubernetes.io/zh/docs/tasks/administer-cluster/change-pv-reclaim-policy/)。

**~~回收(Recycle)~~**：

如果下层的卷插件支持，回收策略`Recycle`会在卷上执行一些基本的擦除(`rm -rf /挂载目录/*`)操作，之后允许该卷用于新的PVC申领。

> **警告：**回收策略`Recycle`已被废弃，取而代之的建议方案是使用动态供应。

###### 预留PersistentVolume

通过在PersistentVolumeClaim中指定PersistentVolume，我们可以声明该特定PV与PVC之间的绑定关系。如果该PersistentVolume存在且其未使用`claimRef`字段预留给某PersistentVolumeClaim，则该PersistentVolume会和该PersistentVolumeClaim绑定到一起。绑定操作不会考虑某些卷匹配条件是否满足，包括节点亲和性等等。

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: foo-pvc
  namespace: foo
spec:
  storageClassName: ""  #此处须显式设置空字符串，否则会被设置为默认的StorageClass
  volumeName: foo-pv
  ...
```

此方法无法对PersistentVolume的绑定特权做出任何形式的保证。如果有其他PersistentVolumeClaim可以使用我们所指定的PV，则我们应该首先预留该存储卷。我们可以将PV的`claimRef`字段设置为相关的PersistentVolumeClaim以确保其他PVC不会绑定到该PV卷。

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: foo-pv
spec:
  storageClassName: ""
  claimRef:
    name: foo-pvc
    namespace: foo
  ...
```

##### 12.2.3 持久卷的类型

PV 持久卷是用插件的形式来实现的。Kubernetes 目前支持以下插件：

- [`awsElasticBlockStore`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#awselasticblockstore)：AWS弹性块存储(EBS)；
- [`azureDisk`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#azuredisk)：Azure Disk；
- [`azureFile`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#azurefile)：Azure File；
- [`cephfs`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#cephfs)：CephFS volume；
- [`csi`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#csi)：容器存储接口(CSI)；
- [`fc`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#fc)：Fibre Channel(FC)存储；
- [`flexVolume`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#flexVolume)：FlexVolume；
- [`flocker`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#flocker)：Flocker存储；
- [`gcePersistentDisk`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#gcepersistentdisk)：GCE持久化盘；
- [`glusterfs`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#glusterfs)：Glusterfs卷；
- [`hostPath`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#hostpath)：HostPath卷(仅供单节点测试使用，不适用于多节点集群，请尝试使用`local`卷作为替代)；
- [`iscsi`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#iscsi)：iSCSI(SCSI over IP)存储；
- [`local`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#local)：节点上挂载的本地存储设备；
- [`nfs`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#nfs)：网络文件系统(NFS)存储；
- [`portworxVolume`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#portworxvolume)：Portworx卷；
- [`quobyte`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#quobyte)：Quobyte卷；
- [`rbd`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#rbd)：Rados块设备(RBD)卷；
- [`storageos`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#storageos)：StorageOS卷；
- [`vsphereVolume`](https://kubernetes.io/zh/docs/concepts/storage/volumes/#vspherevolume)：vSphere VMDK卷。

##### 12.2.4 PersistentVolume

每个PV对象都包含`spec`部分和`status`部分，分别对应卷的规约和状态。下面是一个简单案例：

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp
    server: 172.17.0.2
```

> **说明**：在集群中使用持久卷存储通常需要一些特定于具体卷类型的辅助程序。在这个例子中，PersistentVolume是NFS类型的，因此需要辅助程序`/sbin/mount.nfs`来支持挂载NFS文件系统。

###### 容量

一般而言，每个PV卷都有确定的存储容量。容量属性是使用PV对象的`capacity`属性来设置的。可以参考k8s的[资源模型](https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md)设计提案，了解`capacity`字段可以接受的单位。

目前，存储大小是可以设置和请求的唯一资源。未来可能会包含IOPS、吞吐量等属性。

###### 卷模式

针对PV持久卷，Kubernetes支持两种卷模式(`volumeModes`)：`Filesystem(文件系统)`和`Block(块)`。`volumeMode`是一个可选的API参数。如果该参数被省略，默认的卷模式是`Filesystem`。

`volumeMode`属性设置为`Filesystem`的卷会被Pod挂载(Mount)到某个目录。如果卷的存储来自某块设备而该设备目前为空，Kuberneretes会在第一次挂载卷之前在设备上创建文件系统。

我们可以将`volumeMode`设置为`Block`，以便将卷作为原始块设备来使用。这类卷以块设备的方式交给Pod使用，其上没有任何文件系统。这种模式对于为Pod提供一种使用最快可能方式来访问卷而言很有帮助，Pod和卷之间不存在文件系统层。另外，Pod中运行的应用必须知道如何处理原始块设备。关于如何在Pod中使用`volumeMode: Block`的卷，可以参阅[原始块卷支持](https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/#raw-block-volume-support)。

###### 访问模式

PersistentVolume卷可以用资源提供者所支持的任何方式挂载到宿主系统上。如下表所示，提供者(驱动)的能力不同，每个PV卷的访问模式都会设置为对应卷所支持的模式值。例如，NFS可以支持多个读写客户，但是某个特定的NFS PV卷可能在服务器上以只读的方式导出。每个PV卷都会获得自身的访问模式集合，描述的是特定PV卷的能力。

访问模式有：

- `ReadWriteOnce`：卷可以被一个节点以读写方式挂载；
- `ReadOnlyMany`：卷可以被多个节点以只读方式挂载；
- `ReadWriteMany`：卷可以被多个节点以读写方式挂载。

在命令行接口(CLI)中，访问模式也会使用以下缩写形式：

- ==RWO== - ReadWriteOnce
- ==ROX== - ReadOnlyMany
- ==RWX== - ReadWriteMany

> **注意**： 每个卷同一时刻只能以一种访问模式挂载，即使该卷能够支持多种访问模式。例如，某个卷可以被某节点以ReadWriteOnce模式挂载，或者被多个节点以ReadOnlyMany模式挂载，但不可以同时以两种模式挂载。

| 卷插件               | ReadWriteOnce | ReadOnlyMany |         ReadWriteMany          |
| :------------------- | :-----------: | :----------: | :----------------------------: |
| AWSElasticBlockStore |       ✓       |      -       |               -                |
| AzureFile            |       ✓       |      ✓       |               ✓                |
| AzureDisk            |       ✓       |      -       |               -                |
| CephFS               |       ✓       |      ✓       |               ✓                |
| Cinder               |       ✓       |      -       |               -                |
| CSI                  |  取决于驱动   |  取决于驱动  |           取决于驱动           |
| FC                   |       ✓       |      ✓       |               -                |
| FlexVolume           |       ✓       |      ✓       |           取决于驱动           |
| Flocker              |       ✓       |      -       |               -                |
| GCEPersistentDisk    |       ✓       |      ✓       |               -                |
| Glusterfs            |       ✓       |      ✓       |               ✓                |
| HostPath             |       ✓       |      -       |               -                |
| iSCSI                |       ✓       |      ✓       |               -                |
| Quobyte              |       ✓       |      ✓       |               ✓                |
| NFS                  |       ✓       |      ✓       |               ✓                |
| RBD                  |       ✓       |      ✓       |               -                |
| VsphereVolume        |       ✓       |      -       | - (Pod 运行于同一节点上时可行) |
| PortworxVolume       |       ✓       |      -       |               ✓                |
| ScaleIO              |       ✓       |      ✓       |               -                |
| StorageOS            |       ✓       |      -       |               -                |

###### 类(Class)

每个PV可以属于某个类(Class)，通过将其`storageClassName`属性设置为某个[StorageClass](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/)的名称来指定。特定类的PV卷只能绑定到请求该类存储卷的PVC上。未设置`storageClassName`的PV卷没有类设定，只能绑定到那些没有指定特定存储类的PVC上。

早期，Kubernetes使用注解`volume.beta.kubernetes.io/storage-class`而不是`storageClassName`属性。这一注解目前仍然起作用，不过在将来的Kubernetes发布版本中该注解会被彻底废弃。

###### 挂载选项

Kubernetes管理员可以指定持久卷被挂载到节点上时使用的附加挂载选项，可以通过`mountOptions`属性进行指定。并非所有持久卷类型都支持挂载选项，以下是支持挂载选项的卷类型：

- AWSElasticBlockStore
- AzureDisk
- AzureFile
- CephFS
- Cinder(OpenStack块存储)
- GCEPersistentDisk
- Glusterfs
- NFS
- Quobyte卷
- RBD(Ceph块设备)
- StorageOS
- VsphereVolume
- iSCSI

Kubernetes不会对挂载选项执行合法性检查，如果挂载选项是非法的，挂载就会失败。

早期，Kubernetes使用注解`volume.beta.kubernetes.io/mount-options`而不是`mountOptions`属性。这一注解目前仍然起作用，不过在将来的Kubernetes发布版本中该注解会被彻底废弃。

###### 节点亲和性

每个PV卷可以通过设置[节点亲和性](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.21/#volumenodeaffinity-v1-core)来定义一些约束，进而限制从哪些节点上可以访问此卷。使用这些卷的Pod只会被调度到节点亲和性规则所选择的节点上执行。

> **说明**：对大多数类型的卷而言，我们不需要设置节点亲和性字段。[AWS EBS](https://kubernetes.io/zh/docs/concepts/storage/volumes/#awselasticblockstore)、[GCE PD](https://kubernetes.io/zh/docs/concepts/storage/volumes/#gcepersistentdisk)和[Azure Disk](https://kubernetes.io/zh/docs/concepts/storage/volumes/#azuredisk)卷类型都能自动设置相关字段，我们需要为[local](https://kubernetes.io/zh/docs/concepts/storage/volumes/#local)卷显式地设置此属性。

###### 阶段 

每个卷都会处于以下阶段(Phase)之一：

- `Available`(可用)：卷是一个空闲资源，尚未绑定到任何申领；
- `Bound`(已绑定)：该卷已经绑定到某申领；
- `Released`(已释放)：所绑定的申领已被删除，但是资源尚未被集群回收；
- `Failed`(失败)：卷的自动回收操作失败。

##### 12.2.5 PersistentVolumeClaim

每个PVC对象都有`spec`和`status`部分，分别对应申领的规约和状态。下面是一个简单案例：

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
  storageClassName: slow
  selector:
    matchLabels:
      release: "stable"
    matchExpressions:
      - {key: environment, operator: In, values: [dev]}
```

PVC可以通过标签选择算符来进一步过滤卷集合，只有标签与选择算符相匹配的卷才能够绑定到PVC上，选择算符包含两个字段：

- `matchLabels`：卷必须包含带有此值的标签；
- `matchExpressions`：通过设定键、值列表和操作符(operator)来构造需求。

PVC可以通过为`storageClassName`属性设置**StorageClass**的名称来请求特定的存储类。只有所请求的类的PV卷，即`storageClassName`值与PVC设置相同的PV卷，才能绑定到该PVC上。

PVC不必一定要请求某个类。如果PVC的`storageClassName`属性值设置为`""`，则被视为要请求的是没有设置存储类的PV卷，因此该PVC只能绑定未设置存储类的PV卷。

如果某PVC除了请求StorageClass之外还设置了`selector`，那么就只有隶属于所请求类且带有所请求标签的PV才能绑定到该PVC上。

> **说明**：目前，设置了非空`selector`的PVC对象无法让集群为其动态供应PV卷。

##### 12.2.6 使用PVC作为卷

Pod将PVC作为卷来使用，并借此访问存储资源。PVC必须位于使用它的Pod所在的同一名称空间内。集群会在Pod的名称空间中查找PVC，并使用它来获得其所使用的PV卷。之后，卷会被挂载到宿主上并挂载到Pod中。下面是一个简单案例：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
```

> **说明**：PV卷的绑定是排他性的。PVC是名称空间作用域的对象，使用"Many"模式(`ROX`、`RWX`)来挂载申领的操作只能在同一名称空间内进行。

##### 12.2.7 PVC在Pod中的使用案例

###### 12.2.7.1 使用hostpath类型的PVC

通过yaml文件创建并运行一个PV，具体yaml内容如下：

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```

![image-20210712165135086](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005126.png) 

运行以上PV后，开始创建并运行一个PVC，对应的yaml内容如下：

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
```

![image-20210712165536761](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005132.png) 

运行以上PVC后，创建并运行一个使用以上PVC的Pod，对应的yaml内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: task-pv-claim
  containers:
    - name: task-pv-container
      image: nginx:1.20
      ports:
        - containerPort: 80
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
```

![image-20210712170658198](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005140.png)  

通过上图可知，Pod运行在了node1节点上，我们可以通过以下命令在该节点上创建一个index.html文件：

```shell
echo "Hello from PVC" > /mnt/data/index.html
```

![image-20210712170754798](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005155.png) 

然后我们回到master节点，直接通过`curl podIP`的方式检验index.html文件是否成功挂载到Pod中：

![image-20210712171138207](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005201.png) 

如果我们不放心，也可以直接通过`kubectl exec -it task-pv-pod -- bash`命令进入到容器中进行查看：

![image-20210712171431642](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005206.png) 

**注意事项：**

- 我们在创建资源的时候，应该先创建PV，再创建PVC，最后再创建使用PVC的Pod；
- 如果要删除资源，应该先删除Pod，再删除PVC，最后删除PV；
- 如果我们先删除PV，由于该PV正在被PVC使用，所以删除操作会被挂起。同样的道理，正在使用PVC的Pod没有被删除，直接删除PVC的话，删除操作也会被挂起；
- 如果我们删除了Pod和对应的PVC，并且PV的回收策略使用的是`Retain`的话，那么此时PV的状态会由之前的`Bound`变成`Released`，如果我们也想删除PV的话，直接使用`kubectl delete`命令删除即可。

**访问控制**：

使用组ID(GID)配置的存储仅允许Pod使用相同的GID进行写入。GID不匹配或缺失将会导致无权访问错误。为了减少与用户的协调，管理员可以对PersistentVolume添加GID注解。这样 GID 就能自动添加到使用PersistentVolume的任何Pod中。

使用`pv.beta.kubernetes.io/gid`注解的方法如下所示：

```yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv1
  annotations:
    pv.beta.kubernetes.io/gid: "1234"
```

当Pod使用带有GID注解的PersistentVolume时，注解的GID会被应用于Pod中的所有容器，应用的方法与Pod的安全上下文中指定的GID相同。每个GID，无论是来自PersistentVolume注解还是来自Pod规约，都会被应用于每个容器中运行的第一个进程。

> **说明**：当Pod使用PersistentVolume时，与PersistentVolume关联的GID不会在Pod资源本身的对象上出现。

###### 12.2.7.2 使用nfs类型的PVC

在前面Volume章节中，讲到Volume的类型的时候有涉及nfs，那里讲解了nfs的安装和基本的使用，并在集群外的10主机创建了一个用于集群中Pod挂载的`nfstest`目录，为了方便起见，这里还是使用这个目录进行挂载。

首先还是创建并运行一个PV，对应的yaml内容如下：

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    server: 192.168.68.10
    path: "/nfstest"
```

![image-20210712182711511](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005216.png) 

然后创建并运行一个PVC，对应的yaml内容如下：

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
```

![image-20210712182845682](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005222.png) 

最后创建并运行一个使用以上PVC的Pod，对应的yaml内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: task-pv-claim
  containers:
    - name: task-pv-container
      image: nginx:1.20
      ports:
        - containerPort: 80
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
```

![image-20210712190107375](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005228.png) 

> **说明**：如果Pod一直处于`ContainerCreating`状态，可以在worker节点上使用`showmount -e 192.168.68.10`命令查看nfs可挂载的目录，如果出现类似"No route to host"这样的错误，可以通过关闭防火墙的方式解决。

Pod成功运行后，在10主机执行如下命令：

```shell
echo "nfs pvc test" > /nfstest/index.html
```

然后回到集群中的master节点使用`curl podIp`的方式验证10主机中`/nfstest`目录下的文件是否成功挂载到Pod中：

![image-20210712192100179](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005237.png) 

以上案例是直接创建的Pod，但是其实真实环境一般是不会直接创建Pod的，一般都是创建像deployment这种控制器来管理Pod。假设我们创建了带有多个副本的deployment控制器，并且该控制器的Pod一直在向一个文件中执行写的操作，如果我们把这个文件挂载到nfs的共享目录中的话，如果deployment的副本数大于1的话，就会出现多个Pod向共享目录中的同一个文件中写入同样内容的情况，所以会导致共享目录中该文件的内容重复。我们可以通过子目录的方式解决这一问题，案例如下：

PV和PVC的内容和上面一样就行，所以这里只创建deployment。当PV和PVC运行后，创建一个deployment控制器，其对应的yaml文件内容如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-name
  labels:
    app: nginx-deploy
spec:
  replicas: 5
  selector:
    matchLabels:
      app: nginx-label
  template:
    metadata:
      labels:
        app: nginx-label
    spec:
      volumes:
        - name: task-pv-storage
          persistentVolumeClaim:
            claimName: task-pv-claim
      containers:
        - name: task-pv-container
          env:
          - name: HOST_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.hostIP
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: nginx:1.20
          command: [ "/bin/sh", "-c", "while [ true ]; do date >> /usr/share/nginx/html/hello.txt;sleep 10; done" ]
          ports:
            - containerPort: 80
          volumeMounts:
            - mountPath: "/usr/share/nginx/html/"
              subPathExpr: $(HOST_IP)-$(POD_NAME)
              name: task-pv-storage
```

> **说明**：以上deployment控制器的副本数设置的是5个，对于容器中的文件，采用的是子目录的方式挂载到nfs的共享目录下，子目录的命名规则是Pod所在宿主机的IP加Pod的名称。

基于yaml文件创建deployment后，通过`kubectl get pv,pvc,pod -o wide`命令查看所有资源的运行情况：

![image-20210712213447858](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005248.png) 

资源正常运行后，我们再去nfs所在主机的共享目录下查看挂载情况：

![image-20210712213759043](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005256.png) 

> **说明**：由以上截图可知，容器中的文件已经都挂载到nfs的共享目录下了，并且在共享目录下都新建了子目录，子目录的命名规则也和我们在yaml文件中设置的是一样的，由于控制器管理的Pod都是一样的，所以每个子目录下都是具有相同内容的相同文件。

###### 12.2.7.3 使用glusterfs类型的PVC

在开始之前先准备三台主机，并将这三个节点加入到k8s集群中，如下所示：

![image-20210716112719382](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005303.png)  

> **说明**：glusterfs相关的三台主机在加入到k8s集群中前，要把搭建集群时针对worker节点上执行的操作在这些主机上都执行一遍，然后还要配置集群中主机相关的hosts，集群中原有的节点也要配置这三台主机相关的hosts。

准备操作完成后，下面开始演示如何使用使用glusterfs类型的PVC：

- 首先在集群中所有的worker节点上安装好glusterfs服务：

  ```shell
  yum install -y glusterfs glusterfs-fuse
  ```

- 然后我们在master节点上通过以下命令查看一下apiserver的启动参数中是否包含`--allow-privileged=true`，没有的话需要加上，我这边是有的：

  ```shell
  ps -ef|grep apiserver|grep "allow-privileged"
  ```

  ![image-20210715152910320](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005311.png) 

- 之后给要部署glusterfs管理服务的节点上打上`storagenode=glusterfs`的标签，后面部署daemonset的时候会根据这个标签固定节点调度：

  ```shell
  #在master节点上依次执行以下命令
  kubectl label node glusterfs01 storagenode=glusterfs
  kubectl label node glusterfs02 storagenode=glusterfs
  kubectl label node glusterfs03 storagenode=glusterfs
  ```

- 通过DaemonSet的方式部署GlusterFS管理服务：

  ```shell
  #在master节点上下载glusterfs-daemonset.yaml文件
  wget https://gitee.com/gongcqq/glusterfs-pvc/raw/master/glusterfs-daemonset.yaml
  
  #通过以下命令基于yaml文件创建资源
  kubectl apply -f glusterfs-daemonset.yaml
  ```

  ![image-20210716115603783](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005317.png) 

- GlusterFS服务部署完成之后，为了方便管理和维护运行的GlusterFS服务，这里引入Heketi服务。下载并创建相应的资源：

  ```shell
  #下载文件
  wget https://gitee.com/gongcqq/glusterfs-pvc/raw/master/heketi-security.yaml
  wget https://gitee.com/gongcqq/glusterfs-pvc/raw/master/heketi-deployment.yaml
  
  #基于yaml文件创建相应的资源
  kubectl apply -f heketi-security.yaml
  kubectl apply -f heketi-deployment.yaml
  ```

  ![image-20210716133435369](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005404.png)  

- 当heketi成功运行后，通过`kubectl exec <podName> -it -- bash`命令进入到容器里面：

  ![image-20210716133941357](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005432.png) 

- 进入容器中后，在容器中执行如下命令：

  ```shell
  export HEKETI_CLI_SERVER=http://localhost:8080
  ```

- 然后我们需要下载一个topology.json文件：

  ```shell
  wget https://gitee.com/gongcqq/glusterfs-pvc/raw/master/topology.json
  ```

  ![image-20210716144124026](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005444.png) 

   ![image-20210716144242991](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005452.png)  

  我们可以在容器中新建一个同名的topology.json文件，并将下载的该文件的内容复制到容器的topology.json文件中去。上面的`/dev/sdb`一定要是裸设备，如果是使用虚拟机安装的linux系统，可以通过下图中多添加一块硬盘的方式创建裸设备：

  ![image-20210716144454337](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005502.png) 

- 完成以上操作后，我们通过在容器中执行以下命令来完成GlusterFS集群的创建：

  ```shell
  heketi-cli topology load --json=topology.json
  ```

  ![image-20210716140954032](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005516.png) 

  有可能会报上面的错，这是因为新版本的heketi在创建GlusterFS集群时需要带上参数，声明用户名及密码，可以使用如下命令：

  ```shell
  heketi-cli -s $HEKETI_CLI_SERVER --user admin --secret 'My Secret' topology load --json=topology.json
  ```

  ![image-20210716144833825](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005521.png)

  由上图可知，heketi已经完成了glusterfs集群的创建，我们可以通过如下命令查看当前集群的拓扑信息：

  ```shell
  heketi-cli -s $HEKETI_CLI_SERVER --user admin --secret 'My Secret' topology info
  ```

  ![image-20210716145451409](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005527.png)

  > **说明**：由于内容太多，所以上图就只截取了一部分。

  我们也可以到部署glusterfs服务的worker节点上通过`netstat -nltp|grep glusterd`命令验证一下是否存在对应的监听端口，我们还可以使用`docker ps|grep gluster`命令找到对应的容器：

  ![image-20210716150145112](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005542.png) 

- 我们还可以进入到上面通过`docker ps|grep gluster`命令查到的容器中去看看gluster集群的信息，进入容器中后通过以下命令查看：

  ```shell
  gluster peer status
  ```

  ![image-20210716150816676](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005547.png)

  > **说明**：通过上图可知，glusterfs01节点和另外两个glusterfs节点的连接状态都是正常的。

- 以上步骤已经通过GlusterFS+Heketi的方式完成了k8s共享存储集群的部署，下面创建Secret：

  ```shell
  #下载文件
  wget https://gitee.com/gongcqq/glusterfs-pvc/raw/master/heketi-secret.yaml
  
  #基于yaml文件创建资源
  kubectl apply -f heketi-secret.yaml
  ```

  > **说明**：这里之所以要创建一个Secret，是因为上面使用`topology.json`文件创建GlusterFS集群的时候指定了用户名密码，而且指定密码时使用的是`--secret`属性。

- 然后我们开始创建StorageClass：

  ```shell
  #下载文件
  wget https://gitee.com/gongcqq/glusterfs-pvc/raw/master/glusterfs-storage-class.yaml
  
  #基于yaml文件创建资源
  kubectl apply -f glusterfs-storage-class.yaml
  ```

  ![image-20210716223512001](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005556.png) 

  ```yaml
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    name: glusterfs-storage-class
  provisioner: kubernetes.io/glusterfs
  parameters:
    restuser: "admin"
    secretName: "heketi-secret"
    secretNamespace: "default"
    volumetype: "replicate:3"
    resturl: "http://10.102.135.180:8080"
    restauthenabled: "true"
  ```

  > **说明**：以上是glusterfs-storage-class.yaml文件的内容，之前基于heketi-deployment.yaml文件创建资源的时候，创建的资源中包含一个名为`heketi`的Service，以上内容中的`resturl`属性后跟的就是之前创建的这个Service的地址和端口，通过这个地址我们可以访问到对应heketi的Pod。`secretName`属性值就是上面创建的Secret的名称。

- 接下来我们创建一个PVC：

  ```shell
  #下载文件
  wget https://gitee.com/gongcqq/glusterfs-pvc/raw/master/glusterfs-pvc.yaml
  
  #基于yaml文件创建资源
  kubectl apply -f glusterfs-pvc.yaml
  ```

  ![image-20210716225050207](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005603.png) 

  除此之外，还会给我们动态创建一个Service，如下所示：

  ![image-20210716225352553](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005608.png)  

- 所有前置操作准备完成之后，下面通过Deployment创建Pod进行测试：

  ```shell
  #下载文件
  wget https://gitee.com/gongcqq/glusterfs-pvc/raw/master/web-deploy.yaml
  
  #基于yaml文件创建资源
  kubectl apply -f web-deploy.yaml
  
  #查看正在运行的pod的状态
  kubectl get pod -l app=nginx-label -o wide
  ```

  ![image-20210717001137442](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005616.png) 

  上面下载的web-deploy.yaml文件内容如下：

  ```yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: web-deploy
    labels:
      app: nginx-deploy
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: nginx-label
    template:
      metadata:
        labels:
          app: nginx-label
      spec:
        containers:
        - name: web-deploy
          image: gongcqq/myapp:1.0
          ports:
          - containerPort: 80
          volumeMounts:
            - name: gluster-volume
              mountPath: "/test-data"
              readOnly: false
        volumes:
        - name: gluster-volume
          persistentVolumeClaim:
            claimName: glusterfs-pvc
  ```

  > **说明**：以上yaml文件中`persistentVolumeClaim`字段下的`claimName`字段的值就是上文创建的PVC的名称，上述yaml文件是把容器中的`/test-data`目录挂载到了PVC下对应的glusterfs集群中了。

- 下面验证一下目录挂载是否成功：

  ```shell
  #先通过以下命令进入到调度到node2节点上的pod中
  kubectl exec web-deploy-85f8497d5b-zwngn -it -- sh
  
  #然后进入到容器中的"test-data"目录下，使用如下命令新建一个文件
  echo "hello glusterfs" > test.txt
  ```

  ![image-20210717002153484](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005624.png) 

  ```shell
  #再进入到另一个Pod对应的容器中去，查看新建的文件是否同步过来
  kubectl exec web-deploy-85f8497d5b-nfhpm -it -- sh
  ```

  ![image-20210717002458914](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005633.png) 

  > **说明**：由以上截图可知，新增的文件已经通过到同一个Deployment创建的其他Pod中了。

- 下面再验证一下，挂载目录中的文件是否也已经保存到glusterfs集群中了：

  ```shell
  #先根据标签筛选出glusterfs对应的Pod
  kubectl get pod -l glusterfs=pod -o wide
  
  #然后随便进入到一个Pod中
  kubectl exec glusterfs-bzcbw -it -- bash
  
  #之后查看glusterfs集群中挂载目录的位置
  mount | grep heketi
  ```

  ![image-20210717003448137](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005639.png) 

  > **说明**：前面在创建StorageClass的时候，类型(也就是`volumetype`字段的值)设置的是`replicate`，这种类型的意思就是，会把挂载的目录中的内容同步复制到glusterfs集群中的其他节点上，所以下面可以测试一下，看看其他glusterfs对应的Pod中是否也存在我们挂载的文件。

  ```shell
  #进入到glusterfs对应的另一个Pod中
  kubectl exec glusterfs-gtqpz -it -- bash
  
  #查看glusterfs集群中挂载目录的位置
  mount | grep heketi
  ```

  ![image-20210717004239645](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210717005645.png) 

  > **说明**：由以上截图可知，挂载的文件也同步到glusterfs对应的另一个Pod中了，说明通过PVC和glusterfs集群的方式进行文件的共享存储是没有问题的。

##### 12.2.8 访问控制

使用组ID(GID)配置的存储仅允许Pod使用相同的GID进行写入。GID不匹配或缺失将会导致无权访问错误。为了减少与用户的协调，管理员可以对PersistentVolume添加GID注解。 这样GID就能自动添加到使用PersistentVolume的任何Pod中。

使用`pv.beta.kubernetes.io/gid`注解的方法如下所示：

```yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv1
  annotations:
    pv.beta.kubernetes.io/gid: "1234"
```

当Pod使用带有GID注解的PersistentVolume时，注解的GID会被应用于Pod中的所有容器，应用的方法与Pod的安全上下文中指定的GID相同。每个GID，无论是来自PersistentVolume注解还是来自Pod规约，都会被应用于每个容器中运行的第一个进程。

> **说明**：当Pod使用PersistentVolume时，与PersistentVolume关联的GID不会在Pod资源本身的对象上出现。

#### 12.3 StorageClass的使用

StorageClass即存储类，它为管理员提供了描述存储"类"的方法。不同的类型可能会映射到不同的服务质量等级或备份策略，或是由集群管理员制定的任意策略。Kubernetes本身并不清楚各种类代表的什么，这个类的概念在其他存储系统中有时被称为"配置文件"。

##### 12.3.1 StorageClass资源

每个StorageClass都包含`provisioner`、`parameters`和`reclaimPolicy`字段，这些字段会在StorageClass需要动态分配PersistentVolume时使用到。

StorageClass对象的命名很重要，用户使用这个命名来请求生成一个特定的类。当创建StorageClass对象时，管理员设置StorageClass对象的命名和其他参数，一旦创建了对象就不能再对其更新。

###### 12.3.1.1 存储制备器

每个StorageClass都有一个制备器(Provisioner)，用来决定使用哪个卷插件制备PV，该字段必须指定。

| 卷插件               | 内置制备器 |                           配置例子                           |
| :------------------- | :--------: | :----------------------------------------------------------: |
| AWSElasticBlockStore |     ✓      | [AWS EBS](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#aws-ebs) |
| AzureFile            |     ✓      | [Azure File](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#azure-文件) |
| AzureDisk            |     ✓      | [Azure Disk](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#azure-磁盘) |
| CephFS               |     -      |                              -                               |
| Cinder               |     ✓      | [OpenStack Cinder](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#openstack-cinder) |
| FC                   |     -      |                              -                               |
| FlexVolume           |     -      |                              -                               |
| Flocker              |     ✓      |                              -                               |
| GCEPersistentDisk    |     ✓      | [GCE PD](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#gce-pd) |
| Glusterfs            |     ✓      | [Glusterfs](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#glusterfs) |
| iSCSI                |     -      |                              -                               |
| Quobyte              |     ✓      | [Quobyte](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#quobyte) |
| NFS                  |     -      |                              -                               |
| RBD                  |     ✓      | [Ceph RBD](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#ceph-rbd) |
| VsphereVolume        |     ✓      | [vSphere](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#vsphere) |
| PortworxVolume       |     ✓      | [Portworx Volume](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#portworx-卷) |
| ScaleIO              |     ✓      | [ScaleIO](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#scaleio) |
| StorageOS            |     ✓      | [StorageOS](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#storageos) |
| Local                |     -      | [Local](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#本地) |

我们并不一定要限于以上指定列出的"内置"制备器(其名称前缀为"kubernetes.io"并打包在Kubernetes中)。我们还可以运行和指定外部制备器，这些独立的程序遵循由Kubernetes定义的[规范](https://git.k8s.io/community/contributors/design-proposals/storage/volume-provisioning.md)。外部供应商的作者完全可以自由决定他们的代码保存于何处、打包方式、运行方式、使用的插件(包括Flex)等。[代码仓库](https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner)包含一个用于为外部制备器编写功能实现的类库。我们可以访问代码仓库了解外部驱动列表。例如，NFS没有内部制备器，但可以使用外部制备器，也有第三方存储供应商提供自己的外部制备器。

###### 12.3.1.2 回收策略

可以在StorageClass的`reclaimPolicy`字段中指定回收策略，可以是`Delete`或者`Retain`。如果StorageClass对象被创建时没有指定`reclaimPolicy`，它将默认为`Delete`。例如下面这样：

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Retain
allowVolumeExpansion: true
mountOptions:
  - debug
volumeBindingMode: Immediate
```

> **说明**：通过StorageClass动态创建的PV会使用StorageClass被创建时指定的回收策略。

###### 12.3.1.3 挂载选项

由StorageClass动态创建的PersistentVolume将使用类中`mountOptions`字段指定挂载选项。

如果卷插件不支持挂载选项，却指定了挂载选项，则制备操作会失败。挂载选项在StorageClass和PersistentVolume上都不会做验证，如果其中一个挂载选项无效，那么这个PersistentVolume挂载操作就会失败。

###### 12.3.1.4 卷绑定模式

通过`volumeBindingMode`字段控制了卷绑定和动态制备应该发生在什么时候。

默认情况下，使用`Immediate`模式表示，一旦创建了PersistentVolumeClaim也就完成了卷绑定和动态制备。对于由于拓扑限制而非集群所有节点可达的存储后端，PersistentVolume会在不知道Pod调度要求的情况下绑定或者制备。

集群管理员可以通过指定`WaitForFirstConsumer`模式来解决此问题。该模式将延迟PV的绑定和制备，直到使用该PVC的Pod被创建。PV会根据Pod调度约束指定的拓扑来选择或制备。这些包括但不限于==资源需求==、==节点筛选器==、==pod亲和性和互斥性==以及==污点和容忍度==。

以下插件支持动态供应的`WaitForFirstConsumer`模式:

- [AWSElasticBlockStore](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#aws-ebs)
- [GCEPersistentDisk](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#gce-pd)
- [AzureDisk](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#azure-disk)

以下插件支持预创建绑定PersistentVolume的`WaitForFirstConsumer`模式：

- [AWSElasticBlockStore](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#aws-ebs)
- [GCEPersistentDisk](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#gce-pd)
- [AzureDisk](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#azure-disk)
- [Local](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#local)

动态配置和预先创建的PV也支持[CSI卷](https://kubernetes.io/zh/docs/concepts/storage/volumes/#csi)，但是我们需要查看特定CSI驱动程序的文档以查看其支持的拓扑键名和例子。

当集群操作人员使用了`WaitForFirstConsumer`的卷绑定模式，在大部分情况下就没有必要将制备限制为特定的拓扑结构了。然而，如果还是有需要的话，可以使用`allowedTopologies`属性。下面这个例子描述了如何将供应卷的拓扑限制在特定的区域。

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
volumeBindingMode: WaitForFirstConsumer
allowedTopologies:
- matchLabelExpressions:
  - key: failure-domain.beta.kubernetes.io/zone
    values:
    - us-central1-a
    - us-central1-b
```

##### 12.3.2 不同存储服务对应的常用参数

###### 12.3.2.1 AWS EBS

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io1
  iopsPerGB: "10"
  fsType: ext4
```

- `type`：`io1`，`gp2`，`sc1`，`st1`。详细信息参见[AWS文档](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html)。默认值：`gp2`。
- `zone`(弃用)：AWS区域。如果没有指定`zone`和`zones`，通常卷会在Kubernetes集群节点所在的活动区域中轮询调度分配。`zone`和`zones`参数不能同时使用。
- `zones`(弃用)：以逗号分隔的AWS区域列表。如果没有指定`zone`和`zones`，通常卷会在Kubernetes集群节点所在的 活动区域中轮询调度分配。`zone`和`zones`参数不能同时使用。
- `iopsPerGB`：只适用于`io1`卷。每GiB每秒I/O操作。AWS卷插件将其与请求卷的大小相乘以计算IOPS的容量，并将其限制在20000 IOPS（AWS支持的最高值，请参阅[AWS文档](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html)。这里需要输入一个字符串，即`"10"`，而不是`10`。
- `fsType`：受Kubernetes支持的文件类型。默认值：`"ext4"`。
- `encrypted`：指定EBS卷是否应该被加密。合法值为`"true"`或者`"false"`。这里需要输入字符串，即`"true"`，而非`true`。
- `kmsKeyId`：可选。加密卷时使用密钥的完整Amazon资源名称。如果没有提供，但`encrypted`值为true，AWS生成一个密钥。关于有效的ARN值，请参阅[AWS文档](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html)。

> **说明**：`zone`和`zones`已被弃用并被[允许的拓扑结构](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#allowed-topologies)取代。

###### 12.3.2.2 Glusterfs

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: "http://127.0.0.1:8081"
  clusterid: "630372ccdc720a92c681fb928f27b53f"
  restauthenabled: "true"
  restuser: "admin"
  secretNamespace: "default"
  secretName: "heketi-secret"
  gidMin: "40000"
  gidMax: "50000"
  volumetype: "replicate:3"
```

- `resturl`：制备gluster卷的需求的Gluster REST服务/Heketi服务url。通用格式应该是`IPaddress:Port`，这是GlusterFS动态制备器的必需参数。如果Heketi服务在OpenShift/kubernetes中安装并暴露为可路由服务，则可以使用类似于`http://heketi-storage-project.cloudapps.mystorage.com`的格式。
- `restauthenabled`：Gluster REST服务身份验证的布尔值，用于是否启用对REST服务器的身份验证。如果此值启用(即设置为`true`)，则必须填写`restuser`和`restuserkey`，或`restuser`、`secretNamespace` 和`secretName`。 此选项已弃用，当在指定`restuser`、`restuserkey`、`secretName`或`secretNamespace`时，身份验证会被启用。
- `restuser`：在Gluster可信池中有权创建卷的Gluster REST服务/Heketi用户。
- `restuserkey`：Gluster REST服务/Heketi用户的密码将被用于对RES 服务器进行身份验证。此参数已弃用，取而代之的是`secretNamespace` + `secretName`。

- `secretNamespace`和`secretName`：Secret 实例的标识，包含与 Gluster REST 服务交互时使用的用户密码。这些参数是可选的，`secretNamespace` 和 `secretName` 都省略时使用空密码。所提供的Secret必须将类型设置为 "kubernetes.io/glusterfs"，例如下面这样：

  ```shell
  kubectl create secret generic heketi-secret \
    --type="kubernetes.io/glusterfs" --from-literal=key='opensesame' \
    --namespace=default
  ```

  Secret的例子可以在[glusterfs-provisioning-secret.yaml](https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/glusterfs/glusterfs-secret.yaml)中找到。

- `clusterid`：`630372ccdc720a92c681fb928f27b53f`是集群的ID，当制备卷时，Heketi将会使用这个ID。这是一个可选参数，它也可以是一个clusterid列表，例如下面这样： 

  ```shell
  "8452344e2becec931ece4e33c4674e4e,42982310de6c63381718ccfa6d8cf397"
  ```

- `gidMin`和`gidMax`：StorageClass GID范围的最小值和最大值。在此范围(gidMin-gidMax)内的唯一值(GID)将用于动态制备卷。这些都是可选的参数。如果不指定，所制备的卷为一个2000-2147483647之间的值，这是gidMin 和gidMax的默认值。

- `volumetype`：卷的类型及其参数可以用这个可选值进行配置。如果未声明卷类型，则由制备器决定卷的类型。例如：

  - Replica volume类型：`volumetype: replicate:3`其中==3==是replica数量。
  - Disperse/EC volume类型：`volumetype: disperse:4:2` 其中==4==是数据，==2==是冗余数量。
  - Distribute volume类型：`volumetype: none`。

  更多相关的参考信息，请参阅[如何配置 Heketi](https://github.com/heketi/heketi/wiki/Setting-up-the-topology)。

  当动态制备持久卷时，Gluster插件会自动创建名为`gluster-dynamic-<claimname>`的endpoint和headless。在PVC被删除时endpoint和headless会自动被删除。

###### 12.3.2.3 Quobyte

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: slow
provisioner: kubernetes.io/quobyte
parameters:
    quobyteAPIServer: "http://138.68.74.142:7860"
    registry: "138.68.74.142:7861"
    adminSecretName: "quobyte-admin-secret"
    adminSecretNamespace: "kube-system"
    user: "root"
    group: "root"
    quobyteConfig: "BASE"
    quobyteTenant: "DEFAULT"
```

- `quobyteAPIServer`：Quobyte API服务器的格式是`"http(s)://api-server:7860"`。
- `registry`：用于挂载卷的Quobyte仓库。我们可以指定仓库为`<host>:<port>`或者如果想指定多个registry，可以在它们之间添加逗号，例如`<host1>:<port>,<host2>:<port>,<host3>:<port>`。主机可以是一个IP地址，或者如果我们有正在运行的DNS，也可以提供DNS名称。
- `adminSecretNamespace`：`adminSecretName`的名称空间，默认值是"default"。

- `adminSecretName`：保存关于Quobyte用户和密码的Secret，用于对API服务器进行身份验证。提供的secret必须有值为"kubernetes.io/quobyte"的type参数和`user`与`password`的键值，例如下面这样：

  ```shell
  kubectl create secret generic quobyte-admin-secret \
    --type="kubernetes.io/quobyte" --from-literal=key='opensesame' \
    --namespace=kube-system
  ```

- `user`：对这个用户映射的所有访问权限，默认是"root"。
- `group`：对这个组映射的所有访问权限，默认是"nfsnobody"。
- `quobyteConfig`：使用指定的配置来创建卷。我们可以创建一个新的配置，或者可以修改Web控制台或quobyte CLI中现有的配置，默认是"BASE"。
- `quobyteTenant`：使用指定的租户ID创建/删除卷。这个租户必须已经于Quobyte中存在。默认是"DEFAULT"。

###### 12.3.2.4 StorageOS

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/storageos
parameters:
  pool: default
  description: Kubernetes volume
  fsType: ext4
  adminSecretNamespace: default
  adminSecretName: storageos-secret
```

- `pool`：制备卷的StorageOS分布式容量池的名称。如果未指定，则使用通常存在的`default`池。
- `description`：指定给动态创建的卷的描述。所有卷描述对于存储类而言都是相同的，但不同的storage class可以使用不同的描述，以区分不同的使用场景。默认为`Kubernetes volume`。
- `fsType`：请求的默认文件系统类型。请注意，在StorageOS中用户定义的规则可以覆盖此值。默认为`ext4`。
- `adminSecretNamespace`：API配置secret所在的命名空间，如果设置了adminSecretName，则是必选的。
- `adminSecretName`：用于获取StorageOS API凭证的secret名称，如果未指定，则将尝试默认值。

StorageOS Kubernetes卷插件可以使Secret对象来指定用于访问StorageOS API的endpoint和凭据。只有当默认值已被更改时，这才是必须的。Secret必须使用`kubernetes.io/storageos`类型创建，如以下命令：

```shell
kubectl create secret generic storageos-secret \
--type="kubernetes.io/storageos" \
--from-literal=apiAddress=tcp://localhost:5705 \
--from-literal=apiUsername=storageos \
--from-literal=apiPassword=storageos \
--namespace=default
```

用于动态制备卷的Secret可以在任何名称空间中创建，并通过`adminSecretNamespace`参数引用，预先配置的卷使用的Secret必须在与引用它的PVC在相同的名称空间中。

###### 12.3.2.5 Local

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

本地卷还不支持动态制备，然而还是需要创建 StorageClass 以延迟卷绑定， 直到完成 Pod 的调度。这是由 `WaitForFirstConsumer` 卷绑定模式指定的。

延迟卷绑定使得调度器在为 PersistentVolumeClaim 选择一个合适的 PersistentVolume 时能考虑到所有Pod的调度限制。

### 13.集群安全机制

用户使用`kubectl`、客户端库或构造REST请求来访问[Kubernetes API](https://kubernetes.io/zh/docs/concepts/overview/kubernetes-api/)，当请求到达API时，它会经历多个阶段，如认证、鉴权、准入控制，具体过程如下图所示：

![image-20210718121508047](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210721135156.png) 

这一章节主要就是讲解这三个阶段每个阶段主要都是干什么的，以及如何使用等等。 

#### 13.1 用户认证

##### 13.1.1 Kubernetes中的用户

所有Kubernetes集群都有两类用户：由Kubernetes管理的服务账号和普通用户。

Kubernetes假定普通用户是由一个与集群无关的服务通过以下方式之一进行管理的：

- 负责分发私钥的管理员；
- 类似Keystone或者Google Accounts这类用户数据库；
- 包含用户名和密码列表的文件。

有鉴于此，*Kubernetes并不包含用来代表普通用户账号的对象*。普通用户的信息无法通过API调用添加到集群中。

尽管无法通过 API 调用来添加普通用户，Kubernetes 仍然认为能够提供由集群的证书机构签名的合法证书的用户是通过身份认证的用户。基于这样的配置，Kubernetes 使用证书中的"subject"的通用名称(Common Name)字段来确定用户名。接下来，基于角色访问控制(RBAC)的子系统会确定用户是否有权针对某资源执行特定的操作。进一步的细节可参阅[证书请求](https://kubernetes.io/zh/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user)下普通用户主题。

与此不同，服务账号是Kubernetes API所管理的用户。它们被绑定到特定的名称空间，或者由API服务器自动创建，或者通过API调用创建。服务账号与一组以Secret保存的凭据相关，这些凭据会被挂载到Pod中，从而允许集群内的进程访问Kubernetes API。

API请求则或者与某普通用户相关联，或者与某服务账号相关联，亦或者被视作[匿名请求](https://kubernetes.io/zh/docs/reference/access-authn-authz/authentication/#anonymous-requests)。这意味着集群内外的每个进程在向API服务器发起请求时都必须通过身份认证，否则会被视作匿名用户。这里的进程可以是在某工作站上输入`kubectl`命令的操作人员，也可以是节点上的`kubelet`组件，还可以是控制面的成员。

##### 13.1.2 身份认证策略

Kubernetes使用身份认证插件利用客户端证书、持有者令牌(Bearer Token)、身份认证代理(Proxy)或者HTTP基本认证机制来认证API请求的身份。HTTP请求发给API服务器时，插件会将以下属性关联到请求本身：

- 用户名：用来辩识最终用户的字符串，常见的值可以是`kube-admin`或`jane@example.com`。
- 用户ID：用来辩识最终用户的字符串，旨在比用户名有更好的一致性和唯一性。
- 用户组：取值为一组字符串，其中各个字符串用来标明用户是某个命名的用户逻辑集合的成员。常见的值可能是 `system:masters` 或者 `devops-team` 等。
- 附加字段：一组额外的键-值映射，键是字符串，值是一组字符串；用来保存一些鉴权组件可能觉得有用的额外信息。

所有(属性)值对于身份认证系统而言都是不透明的，只有被[鉴权组件](https://kubernetes.io/zh/docs/reference/access-authn-authz/authorization/)解释过之后才有意义。

我们可以同时启用多种身份认证方法，并且我们通常会至少使用两种方法：

- 针对服务账号使用服务账号令牌；
- 至少另外一种方法对用户的身份进行认证。

当集群中启用了多个身份认证模块时，第一个成功地对请求完成身份认证的模块会直接做出评估决定，API服务器并不保证身份认证模块的运行顺序。

对于所有通过身份认证的用户，`system:authenticated`组都会被添加到其组列表中。

与其它身份认证协议(LDAP、SAML、Kerberos等等)都可以通过使用[身份认证代理](https://kubernetes.io/zh/docs/reference/access-authn-authz/authentication/#authenticating-proxy)或[身份认证 Webhoook](https://kubernetes.io/zh/docs/reference/access-authn-authz/authentication/#webhook-token-authentication)来实现。

###### 13.1.2.1 X509客户证书

通过给API server传递`--client-ca-file=SOMEFILE`选项，就可以启动客户端证书身份认证。所引用的文件必须包含一个或者多个证书机构，用来验证向 API server 提供的客户端证书。如果提供了客户端证书并且证书被验证通过，则subject中的公共名称(Common Name)就被作为请求的用户名。自Kubernetes 1.4开始，客户端证书可以通过证书的organization字段标明用户的组成员信息。要包含用户的多个组成员信息，可在证书中包含多个organization字段。

例如，使用`openssl`命令行工具生成一个证书签名请求：

```bash
openssl req -new -key jbeda.pem -out jbeda-csr.pem -subj "/CN=jbeda/O=app1/O=app2"
```

此命令将使用用户名`jbeda`生成一个证书签名请求(CSR)，且该用户属于"app"和"app2"两个用户组。

可参阅[管理证书](https://kubernetes.io/zh/docs/tasks/administer-cluster/certificates/)了解如何生成客户端证书。

###### 13.1.2.2 静态令牌文件

当API server的命令行设置了`--token-auth-file=SOMEFILE`选项时，会从文件中读取持有者令牌。目前，令牌会长期有效，并且在不重启API server的情况下无法更改令牌列表。

令牌文件是一个CSV文件，包含至少3个列：token、用户名和用户的UID，其余列被视为可选的组名。

当使用持有者令牌来对某HTTP客户端执行身份认证时，API server希望看到一个名为`Authorization`的HTTP头，其值格式为 `Bearer THETOKEN`。持有者令牌必须是一个可以放入 HTTP 头部值字段的字符序列，可使用HTTP的编码和引用机制。例如，如果持有者令牌为`31ada4fd-adec-460c-809a-9e56ceb75269`，则其出现在 HTTP 头部时如下所示：

```http
Authorization: Bearer 31ada4fd-adec-460c-809a-9e56ceb75269
```

###### 13.1.2.3 Bootstrap令牌

为了支持平滑地启动引导新的集群，Kubernetes 包含了一种动态管理的持有者令牌类型，称作*Bootstrap Token*。这些令牌以Secret的形式保存在`kube-system`名称空间中，可以被动态管理和创建。控制器管理器包含的`TokenCleaner`控制器能够在启动引导令牌过期时将其删除。

这些令牌的格式为`[a-z0-9]{6}.[a-z0-9]{16}`。第一个部分是令牌的ID，第二个部分是令牌的Secret。我们可以用如下所示的方式来在HTTP头部设置令牌：

```http
Authorization: Bearer 781292.db7bc3a58fc5f07e
```

我们必须在 API server 上设置 `--enable-bootstrap-token-auth` 标志来启用基于Bootstrap令牌的身份认证组件。我们必须通过控制器管理器的 `--controllers` 标志来启用 TokenCleaner控制器，这可以通过类似 `--controllers=*,tokencleaner` 这种设置来做到。如果我们使用 `kubeadm` 来启动引导新的集群，该工具会帮我们完成这些设置。

身份认证组件的认证结果为 `system:bootstrap:<令牌 ID>`，该用户属于`system:bootstrappers`用户组。这里的用户名和组设置都是有意设计成这样，其目的是阻止用户在启动引导集群之后继续使用这些令牌。这里的用户名和组名可以用来(并且已经被`kubeadm`用来)构造合适的鉴权策略，以完成启动引导新集群的工作。

可以参阅[Bootstrap Token](https://kubernetes.io/zh/docs/reference/access-authn-authz/bootstrap-tokens/)以了解关于Bootstrap令牌身份认证组件与控制器的更深入信息，以及如何使用`kubeadm`来管理这些令牌。

###### 13.1.2.4 服务账号令牌

服务账号(Service Account)是一种自动被启用的用户认证机制，使用经过签名的持有者令牌来验证请求，该插件可接受两个可选参数：

- `--service-account-key-file`：一个包含用来为持有者令牌签名的PEM编码密钥。若未指定，则使用API服务器的TLS私钥。
- `--service-account-lookup`：如果启用，则从API删除的令牌会被回收。

服务账号通常由API server自动创建并通过`ServiceAccount` [准入控制器](https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/)关联到集群中运行的Pod上。持有者令牌会挂载到Pod中可预知的位置，允许集群内进程与API服务器通信，服务账号也可以使用Pod规约的`serviceAccountName`字段显式地关联到Pod上。

> **说明**：`serviceAccountName`通常会被省略，因为关联关系是自动建立的。

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-label
  template:
    metadata:
      labels:
        app: nginx-label
    spec:
      serviceAccountName: bob-the-bot
      containers:
        - name: nginx
          image: nginx:1.20
```

在集群外部使用服务账号持有者令牌也是完全合法的，且可用来为长时间运行的、需要与Kubernetes API服务器通信的任务创建标识。要手动创建服务账号，可以使用`kubectl create serviceaccount <名称>`命令，此命令会在当前的名称空间中生成一个服务账号和一个与之关联的Secret，比如像下面这样：

```shell
#sa是serviceaccount的缩写形式
kubectl create sa sa-test

#查询相关联的Secret
kubectl get sa sa-test -o yaml

#查看相关联的Secret的运行情况
kubectl get secret sa-test-token-kqmdd
```

![image-20210717234015172](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210721135228.png) 

> **说明**：所创建的Secret中会保存API server公开的CA证书和一个已签名的JSON Web令牌(JWT)。

已签名的JWT可以用作持有者令牌，并将被认证为所给的服务账号。通常，这些Secret数据会被挂载到Pod中以便集群内访问API服务器时使用，不过也可以在集群外部使用。

服务账号被身份认证后，所确定的用户名为 `system:serviceaccount:<名字空间>:<服务账号>`，并被分配到用户组 `system:serviceaccounts` 和 `system:serviceaccounts:<名字空间>`。

> **<font color="red">警告：</font>**由于服务账号令牌保存在 Secret 对象中，任何能够读取这些 Secret 的用户都可以被认证为对应的服务账号，所以在为用户授予访问服务账号的权限时，以及对 Secret 读权限时，要格外小心。

###### 13.1.2.5 OpenID Connect(OIDC)令牌

[OpenID Connect ](https://openid.net/connect/)是一种 OAuth2 认证方式，被某些 OAuth2 提供者支持，例如 Azure活动目录、Salesforce和Google。协议对 OAuth2 的主要扩充体现在有一个附加字段会和访问令牌一起返回，这一字段称作[ID Token](https://openid.net/specs/openid-connect-core-1_0.html#IDToken)。ID令牌是一种由服务器签名的JSON Web Token(JWT)，其中包含一些可预知的字段，例如用户的邮箱地址。

想要识别用户，身份认证组件要使用OAuth2 [令牌响应](https://openid.net/specs/openid-connect-core-1_0.html#TokenResponse)中的`id_token`(而非`access_token`)作为持有者令牌。

![image-20210717235848490](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210721135234.png) 

1. 登录到我们的身份服务(Identity Provider)；
2. 我们的身份服务将为我们提供`access_token`、`id_token`和`refresh_token`；
3. 在使用`kubectl`时，将`id_token`设置为`--token`标志值，或者将其直接添加到`kubeconfig`中；
4. `kubectl`将我们的`id_token`放到一个称作`Authorization`的头部，发送给API server；
5. API server将负责通过检查配置中引用的证书来确认JWT的签名是合法的；
6. 检查确认`id_token`尚未过期；
7. 确认用户有权限执行操作；
8. 鉴权成功之后，API server向`kubectl`返回响应；
9. `kubectl`向用户提供反馈信息。

由于用来验证我们是谁的所有数据都在`id_token`中，Kubernetes不需要再去联系身份服务。在一个所有请求都是无状态请求的模型中，这一工作方式可以使得身份认证的解决方案更容易处理大规模请求。不过，此访问也有一些挑战：

1. Kubernetes没有提供用来触发身份认证过程的"Web界面"，因为不存在用来收集用户凭据的浏览器或用户接口，我们必须自己先完成对身份服务的认证过程。
2. `id_token`令牌不可收回。因其属性类似于证书，其生命期一般很短(只有几分钟)，所以每隔几分钟就要获得一个新的令牌这件事可能很让人头疼。
3. 如果不使用`kubectl proxy`命令或者一个能够注入`id_token`的反向代理，向Kubernetes控制面板执行身份认证是很困难的。

###### 13.1.2.6 Webhook令牌身份认证

Webhook身份认证是一种用来验证持有者令牌的回调机制。

- `--authentication-token-webhook-config-file`：指向一个配置文件，其中描述如何访问远程的Webhook服务；
- `--authentication-token-webhook-cache-ttl`：用来设定身份认证决定的缓存时间，默认时长为2分钟。

配置文件使用[kubeconfig](https://kubernetes.io/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/)文件的格式。文件中，`clusters`指代远程服务，`users`指代远程API服务Webhook。下面是一个例子：

```yaml
# Kubernetes API版本
apiVersion: v1
# API对象类别
kind: Config
# clusters指代远程服务
clusters:
  - name: name-of-remote-authn-service
    cluster:
      certificate-authority: /path/to/ca.pem         # 用来验证远程服务的CA
      server: https://authn.example.com/authenticate # 要查询的远程服务URL，必须使用https

# users指代API服务的Webhook配置
users:
  - name: name-of-api-server
    user:
      client-certificate: /path/to/cert.pem # Webhook插件要使用的证书
      client-key: /path/to/key.pem          # 与证书匹配的密钥

# kubeconfig文件需要一个上下文(Context)，此上下文用于本API server
current-context: webhook
contexts:
- context:
    cluster: name-of-remote-authn-service
    user: name-of-api-sever
  name: webhook
```

当客户端尝试在API服务器上使用持有者令牌完成身份认证时，身份认证Webhook会用POST请求发送一个JSON序列化的对象到远程服务。该对象是`authentication.k8s.io/v1beta1`组的`TokenReview`对象，其中包含持有者令牌。k8s不会强制请求提供此HTTP头部。

要注意的是，Webhook API对象和其他Kubernetes API对象一样，也要受到同一[版本兼容规则](https://kubernetes.io/zh/docs/concepts/overview/kubernetes-api/)约束。实现者要了解对Beta阶段对象的兼容性承诺，并检查请求的`apiVersion`字段，以确保数据结构能够正常反序列化解析。此外，API服务器必须启用`authentication.k8s.io/v1`API扩展组。

POST请求的Body部分将是如下格式：

```json
{
    "apiVersion": "authentication.k8s.io/v1beta1",
    "kind": "TokenReview",
    "spec": {
        "token": "014fbff9a07c...",
        "audiences": [
            "https://myserver.example.com",
            "https://myserver.internal.example.com"
        ]
    }
}
```

远程服务将填充请求的`status`字段，以标明登录操作是否成功。响应的Body中的`spec`字段会被忽略，因此可以省略。如果持有者令牌验证成功，应该返回如下所示的响应：

```json
{
    "apiVersion": "authentication.k8s.io/v1beta1",
    "kind": "TokenReview",
    "status": {
        "authenticated": true,
        "user": {
            "username": "janedoe@example.com",
            "uid": "42",
            "groups": [
                "developers",
                "qa"
            ],
            "extra": {
                "extrafield1": [
                    "extravalue1",
                    "extravalue2"
                ]
            }
        },
        "audiences": [
            "https://myserver.example.com"
        ]
    }
}
```

而不成功的请求会返回：

```json
{
    "apiVersion": "authentication.k8s.io/v1beta1",
    "kind": "TokenReview",
    "status": {
        "authenticated": false,
        # 当"authenticated=true"时下面的error字段会被忽略
        "error": "Credentials are expired"
    }
}
```

###### 13.1.2.7 身份认证代理

API服务器可以配置成从请求的头部字段值(如`X-Remote-User`)中辩识用户。这一设计是用来与某身份认证代理一起使用API server，代理负责设置请求的头部字段值。

- `--requestheader-username-headers`：必选字段，大小写不敏感。用来设置要获得用户身份所要检查的头部字段名称列表(有序)。第一个包含数值的字段会被用来提取用户名。
- `--requestheader-group-headers`：可选字段，在 Kubernetes 1.6 版本以后支持，大小写不敏感。建议设置为"X-Remote-Group"。用来指定一组头部字段名称列表，以供检查用户所属的组名称。所找到的全部头部字段的取值都会被用作用户组名。
- `--requestheader-extra-headers-prefix`：可选字段，在 Kubernetes 1.6 版本以后支持，大小写不敏感。建议设置为"X-Remote-Extra-"。用来设置一个头部字段的前缀字符串，API server 会基于所给前缀来查找与用户有关的一些额外信息。这些额外信息通常用于所配置的鉴权插件。API server会将与所给前缀匹配的头部字段过滤出来，去掉其前缀部分，将剩余部分转换为小写字符串并在必要时执行[百分号解码](https://tools.ietf.org/html/rfc3986#section-2.1)后，构造新的附加信息字段键名。原来的头部字段值直接作为附加信息字段的值。

例如，使用下面的配置：

```http
--requestheader-username-headers=X-Remote-User
--requestheader-group-headers=X-Remote-Group
--requestheader-extra-headers-prefix=X-Remote-Extra-
```

针对所收到的如下请求：

```http
GET / HTTP/1.1
X-Remote-User: fido
X-Remote-Group: dogs
X-Remote-Group: dachshunds
X-Remote-Extra-Acme.com%2Fproject: some-project
X-Remote-Extra-Scopes: openid
X-Remote-Extra-Scopes: profile
```

会生成下面的用户信息：

```yaml
name: fido
groups:
- dogs
- dachshunds
extra:
  acme.com/project:
  - some-project
  scopes:
  - openid
  - profile
```

为了防范头部信息侦听，在请求中的头部字段被检视之前，身份认证代理需要向API server提供一份合法的客户端证书供后者使用所给的CA来执行验证。警告：*不要* 在不同的上下文中复用CA证书，除非我们清楚这样做的风险是什么以及应如何保护CA用法的机制。

- `--requestheader-client-ca-file`：必选字段，给出PEM编码的证书包。在检查请求的头部字段以提取用户名信息之前，必须提供一个合法的客户端证书，且该证书要能够被所给文件中的机构所验证。
- `--requestheader-allowed-names`：可选字段，用来给出一组公共名称(CN)。如果此标志被设置，则在检视请求中的头部以提取用户信息之前，必须提供包含此列表中所给的CN名的合法的客户端证书。

##### 13.1.3 匿名请求

启用匿名请求支持之后，如果请求没有被已配置的其他身份认证方法拒绝，则被视作匿名请求(Anonymous Requests)。这类请求会获得用户名`system:anonymous`和对应的用户组`system:unauthenticated`。

例如，在一个配置了令牌身份认证且启用了匿名访问的服务器上，如果请求提供了非法的持有者令牌，则会返回`401 Unauthorized`错误。如果请求没有提供持有者令牌，则被视为匿名请求。

在1.5.1-1.5.x版本中，匿名访问默认情况下是被禁用的，可以通过为 API 服务器设定`--anonymous-auth=true`来启用。

在1.6及之后版本中，如果所使用的鉴权模式不是`AlwaysAllow`，则匿名访问默认是被启用的。从1.6版本开始，ABAC和RBAC鉴权模块要求对`system:anonymous`用户或者`system:unauthenticated`用户组执行显式的权限判定，所以之前的为 `*` 用户或 `*` 用户组赋予访问权限的策略规则都不再包含匿名用户。

##### 13.1.4 用户伪装

一个用户可以通过伪装(Impersonation)头部字段来以另一个用户的身份执行操作。使用这一能力，我们可以手动重载请求被身份认证所识别出来的用户信息。例如管理员可以使用这一功能特性来临时伪装成另一个用户，查看请求是否被拒绝， 从而调试鉴权策略中的问题。

带伪装的请求首先会被身份认证识别为发出请求的用户，之后会切换到使用被伪装的用户的用户信息。

- 用户发起API调用时 *同时* 提供自身的凭据和伪装头部字段信息；
- API server对用户执行身份认证；
- API server确认通过认证的用户具有伪装特权；
- 请求用户的信息被替换成伪装字段的值；
- 评估请求，鉴权组件针对所伪装的用户信息执行操作。

以下HTTP头部字段可用来执行伪装请求：

- `Impersonate-User`：要伪装成的用户名。
- `Impersonate-Group`：要伪装成的用户组名。可以多次指定以设置多个用户组。可选字段，要求 "Impersonate-User" 必须被设置。
- `Impersonate-Extra-<附加名称>`：一个动态的头部字段，用来设置与用户相关的附加字段。这是一个可选字段，要求 "Impersonate-User"被设置。为了能够以一致的形式保留，`<附加名称>`部分必须是小写字符，如果有任何字符不是 [合法的 HTTP 头部标签字符](https://tools.ietf.org/html/rfc7230#section-3.2.6)，则必须是utf8字符，且转换为[百分号编码](https://tools.ietf.org/html/rfc3986#section-2.1)。

> **说明：**在1.11.3版本之前(以及1.10.7、1.9.11)，`<附加名称>`只能包含合法的HTTP标签字符。

头部字段集合的示例：

```http
Impersonate-User: jane.doe@example.com
Impersonate-Group: developers
Impersonate-Group: admins
Impersonate-Extra-dn: cn=jane,ou=engineers,dc=example,dc=com
Impersonate-Extra-acme.com%2Fproject: some-project
Impersonate-Extra-scopes: view
Impersonate-Extra-scopes: development
```

在使用 `kubectl` 时，可以使用 `--as` 标志来配置 `Impersonate-User` 头部字段值， 使用 `--as-group` 标志配置 `Impersonate-Group` 头部字段值。

```bash
kubectl drain mynode
```

```bash
Error from server (Forbidden): User "clark" cannot get nodes at the cluster scope. (get nodes mynode)
```

设置 `--as` 和 `--as-group` 标志：

```bash
kubectl drain mynode --as=superman --as-group=system:masters
```

```bash
node/mynode cordoned
node/mynode drained
```

要伪装成某个用户、某个组或者设置附加字段，执行伪装操作的用户必须具有对所伪装的类别(“user”、“group”等)执行 “impersonate”动词操作的能力。对于启用了RBAC鉴权插件的集群，下面的ClusterRole封装了设置用户和组伪装字段所需的规则：

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: impersonator
rules:
- apiGroups: [""]
  resources: ["users", "groups", "serviceaccounts"]
  verbs: ["impersonate"]
```

附加字段会被作为 `userextras` 资源的子资源来执行权限评估。如果要允许用户为附加字段"scopes"设置伪装头部，该用户需要被授予以下规则：

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: scopes-impersonator
rules:
# 可以设置 "Impersonate-Extra-scopes" 头部
- apiGroups: ["authentication.k8s.io"]
  resources: ["userextras/scopes"]
  verbs: ["impersonate"]
```

我们也可以通过约束资源可能对应的`resourceNames`限制伪装头部的取值：

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: limited-impersonator
rules:
# 可以伪装成用户 "jane.doe@example.com"
- apiGroups: [""]
  resources: ["users"]
  verbs: ["impersonate"]
  resourceNames: ["jane.doe@example.com"]

# 可以伪装成用户组 "developers" 和 "admins"
- apiGroups: [""]
  resources: ["groups"]
  verbs: ["impersonate"]
  resourceNames: ["developers","admins"]

# 可以将附加字段 "scopes" 伪装成 "view" 和 "development"
- apiGroups: ["authentication.k8s.io"]
  resources: ["userextras/scopes"]
  verbs: ["impersonate"]
  resourceNames: ["view", "development"]
```

#### 13.2 鉴权

##### 13.2.1 鉴权概述

在Kubernetes中，我们必须在鉴权(授予访问权限)之前进行身份验证(登录)，有关身份验证的信息，可参阅[访问控制](https://kubernetes.io/zh/docs/concepts/security/controlling-access/)。

Kubernetes期望请求中存在REST API常见的属性。这意味着Kubernetes鉴权适用于现有的组织范围或云提供商范围的访问控制系统，除了Kubernetes API之外，它还可以处理其他API。

Kubernetes 使用 API server 对 API 请求进行鉴权。它根据所有策略评估所有请求属性来决定允许或拒绝请求。一个API请求的所有部分都必须被某些策略允许才能继续，这意味着默认情况下拒绝权限。

> **说明**：尽管k8s使用API server，但是依赖于特定对象种类的特定字段的访问控制和策略由准入控制器处理。

当系统配置了多个鉴权模块时，Kubernetes将按顺序使用每个模块。如果任何鉴权模块批准或拒绝请求，则立即返回该决定，并且不会与其他鉴权模块协商。如果所有模块对请求没有意见，则拒绝该请求。被拒绝响应返回HTTP状态代码403。

###### 13.2.1.1 审查请求属性

Kubernetes仅审查以下API请求属性：

- **用户**：身份验证期间提供的`user`字符串。
- **组**：经过身份验证的用户所属的组名列表。
- **额外信息**：由身份验证层提供的任意字符串键到字符串值的映射。
- **API**：指示请求是否针对API资源。
- **请求路径**：各种非资源端点的路径，如 `/api` 或 `/healthz`。
- **API请求动词**：API动词`get`、`list`、`create`、`update`、`patch`、`watch`、 `proxy`、`redirect`、`delete` 和 `deletecollection`用于资源请求。要确定资源 API 端点的请求动词，可以参阅[确定请求动词](https://kubernetes.io/zh/docs/reference/access-authn-authz/authorization/#determine-the-request-verb)。
- **HTTP请求动词**：HTTP动词`get`、`post`、`put` 和 `delete`用于非资源请求。
- **Resource**：正在访问的资源的ID或名称(仅限资源请求) -- 对于使用`get`、`update`、`patch` 和 `delete`动词的资源请求，我们必须提供资源名称。
- **子资源**：正在访问的子资源(仅限资源请求)。
- **名称空间**：正在访问的对象的名称空间(仅适用于名称空间资源请求)。
- **API组**：正在访问的[API组](https://kubernetes.io/zh/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning)(仅限资源请求)，空字符串表示[核心API组](https://kubernetes.io/zh/docs/reference/using-api/#api-groups)。

###### 13.2.1.2 确定请求动词

**非资源请求**

对于 `/api/v1/...` 或 `/apis/<group>/<version>/...` 之外的端点的请求被视为"非资源请求(Non-Resource Requests)"，并使用该请求的 HTTP 方法的小写形式作为其请求动词。例如，对 `/api` 或 `/healthz` 这类端点的 `GET` 请求将使用 `get` 作为其动词。

**资源请求**

要确定对资源 API 端点的请求动词，需要查看所使用的 HTTP 动词以及该请求是针对单个资源还是一组资源：

| HTTP 动词 | 请求动词                                         |
| --------- | ------------------------------------------------ |
| POST      | create                                           |
| GET, HEAD | get(针对单个资源)、list(针对集合)                |
| PUT       | update                                           |
| PATCH     | patch                                            |
| DELETE    | delete(针对单个资源)、deletecollection(针对集合) |

Kubernetes有时使用专门的动词以对额外的权限进行鉴权。例如：

- [PodSecurityPolicy](https://kubernetes.io/docs/concepts/policy/pod-security-policy/)：`policy`API组中`podsecuritypolicies`资源使用`use`动词.
- [RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/#privilege-escalation-prevention-and-bootstrapping)：对`rbac.authorization.k8s.io`API组中`roles`和`clusterroles`资源的`bind`和`escalate`动词。
- [Authentication](https://kubernetes.io/docs/reference/access-authn-authz/authentication/)：对核心API组中`users`、`groups`和`serviceaccounts`以及`authentication.k8s.io`API组中的`userextras`所使用的`impersonate`动词。

###### 13.2.1.3 鉴权类型

- **Node**：一个专用鉴权组件，根据调度到kubelet上运行的Pod为kubelet授予权限。了解有关使用节点鉴权模式的更多信息，可以参阅[节点鉴权](https://kubernetes.io/zh/docs/reference/access-authn-authz/node/)。
- **ABAC**：基于属性的访问控制(ABAC)定义了一种访问控制范型，通过使用将属性组合在一起的策略，将访问权限授予用户。策略可以使用任何类型的属性(用户属性、资源属性、对象、环境属性等)。要了解有关使用 ABAC 模式的更多信息，可以参阅[ABAC模式](https://kubernetes.io/zh/docs/reference/access-authn-authz/abac/)。
- **RBAC**：基于角色的访问控制(RBAC)是一种基于企业内个人用户的角色来管理对计算机或网络资源的访问的方法。在此上下文中，权限是单个用户执行特定任务的能力，例如查看、创建或修改文件。要了解有关使用 RBAC 模式的更多信息，可以参阅[RBAC模式](https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/)。
  - 被启用之后，RBAC(基于角色的访问控制)使用`rbac.authorization.k8s.io`API组来驱动鉴权决策，从而允许管理员通过Kubernetes API动态配置权限策略；
  - 要启用RBAC，可以使用`--authorization-mode = RBAC`启动apiserver。
- **Webhook**：WebHook是一个HTTP回调：发生某些事情时调用的HTTP POST。通过HTTP POST进行简单的事件通知。实现WebHook的Web应用程序会在发生某些事情时将消息发布到 URL。要了解有关使用Webhook模式的更多信息，可以参阅[Webhook模式](https://kubernetes.io/zh/docs/reference/access-authn-authz/webhook/)。

`kubectl`提供`auth can-i`子命令，用于快速查询API鉴权。该命令使用`SelfSubjectAccessReview`API来确定当前用户是否可以执行给定操作，无论使用何种鉴权模式该命令都可以工作。比如：

```shell
#假设这个可以执行给定操作，返回的就是"yes"
kubectl auth can-i create deployments --namespace dev
```

```shell
#假设这个不可以执行给定操作，返回的就是"no"
kubectl auth can-i create deployments --namespace prod
```

管理员也可以将此与[用户伪装](https://kubernetes.io/zh/docs/reference/access-authn-authz/authentication/#user-impersonation)结合使用，以确定其他用户可以执行的操作：

```shell
kubectl auth can-i list secrets --namespace dev --as dave
```

`SelfSubjectAccessReview`是`authorization.k8s.io` API组的一部分，它将 API server鉴权公开给外部服务。该组中的其他资源包括：

- `SubjectAccessReview`：对任意用户的访问进行评估，而不仅仅是当前用户。当鉴权决策被委派给API server时很有用。例如，kubelet和扩展API server可以使用它来确定用户对自己的API的访问权限。
- `LocalSubjectAccessReview`：与`SubjectAccessReview`类似，但仅限于特定的名称空间。
- `SelfSubjectRulesReview`：返回用户可在名称空间内执行的操作集的审阅。用户可以快速汇总自己的访问权限，或者用于UI中的隐藏/显示动作。

可以通过创建普通的Kubernetes资源来查询这些API，其中返回对象的响应"status"字段是查询的结果：

```bash
kubectl create -f - -o yaml << EOF
apiVersion: authorization.k8s.io/v1
kind: SelfSubjectAccessReview
spec:
  resourceAttributes:
    group: apps
    name: deployments
    verb: create
    namespace: dev
EOF
```

生成的`SelfSubjectAccessReview`为：

```yaml
apiVersion: authorization.k8s.io/v1
kind: SelfSubjectAccessReview
metadata:
  creationTimestamp: null
spec:
  resourceAttributes:
    group: apps
    name: deployments
    namespace: dev
    verb: create
status:
  allowed: true
  denied: false
```

###### 13.2.1.4 鉴权模块参数设置

我们必须在策略中包含一个参数标志，以指明我们的策略包含哪个鉴权模块：

可以使用的参数有：

- `--authorization-mode=ABAC`：基于属性的访问控制(ABAC)模式允许我们使用本地文件配置策略。
- `--authorization-mode=RBAC`：基于角色的访问控制(RBAC)模式允许我们使用Kubernetes API创建和存储策略。
- `--authorization-mode=Webhook`：WebHook是一种HTTP回调模式，允许我们使用远程REST端点管理鉴权。
- `--authorization-mode=Node`：节点鉴权是一种特殊用途的鉴权模式，专门对kubelet发出的API请求执行鉴权。
- `--authorization-mode=AlwaysDeny`：该标志阻止所有请求。仅将此标志用于测试。
- `--authorization-mode=AlwaysAllow`：此标志允许所有请求。仅在我们不需要API请求的鉴权时才使用此标志。

我们可以选择多个鉴权模块。模块按顺序检查，以便较靠前的模块具有更高的优先级来允许或拒绝请求。

###### 13.2.1.5 通过创建Pod提升权限

能够在名称空间中创建Pod的用户可能会提升其在该名称空间内的权限。他们可以创建在该名称空间内访问其权限的Pod。他们可以创建Pod访问用户自己无法读取的Secret，或者在具有不同/更高权限的服务帐户下运行的Pod。

> **注意**：系统管理员在授予对Pod创建的访问权限时要小心。被授予在名称空间中创建Pod(或创建Pod的控制器)的权限的用户可以读取名称空间中的所有Secret、ConfigMap，并模拟名称空间中的任意服务账号以及执行账号可以执行的任何操作。无论采用何种鉴权方式，这都适用。

##### 13.2.2 RBAC鉴权

基于角色(Role)的访问控制(RBAC)是一种基于组织中用户的角色来调节控制对计算机或网络资源的访问的方法。

RBAC鉴权机制使用`rbac.authorization.k8s.io` [API组](https://kubernetes.io/zh/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning) 来驱动鉴权决定，允许我们通过Kubernetes API动态配置策略。

要启用RBAC，在启动[API server](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/)时将`--authorization-mode`参数设置为一个逗号分隔的列表并确保其中包含`RBAC`。

RBAC的API声明了四种Kubernetes对象：==Role==、==ClusterRole==、==RoleBinding==和==ClusterRoleBinding==。我们可以像使用其他Kubernetes对象一样，通过类似`kubectl`这类工具进行对象的创建或修改等操作。

###### 13.2.2.1 Role和ClusterRole

RBAC的`Role`或`ClusterRole`中包含一组代表相关权限的规则。这些权限是纯粹累加的(不存在拒绝某操作的规则)。

Role总是用来在某个名称空间内设置访问权限；在我们创建Role时，必须指定该Role所属的名称空间。

与之相对，ClusterRole则是一个集群作用域的资源。这两种资源的名字不同(Role和ClusterRole)是因为k8s对象要么是名称空间作用域的，要么是集群作用域的，不可两者兼具。

ClusterRole有若干用法，我们可以用它来：

1. 定义对某名称空间域对象的访问权限，并将在各个名称空间内完成授权；
2. 为名称空间作用域的对象设置访问权限，并跨所有名称空间执行授权；
3. 为集群作用域的资源定义访问权限。

**如果我们希望在名称空间内定义角色，应该使用Role；如果我们希望定义集群范围的角色，应该使用ClusterRole。**

关于Role的示例：

下面是一个位于"default"名称空间的Role的示例，可用来授予对pod的读访问权限：

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # ""标明core API组
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
```

关于ClusterRole的示例：

ClusterRole可以和Role相同完成授权。因为ClusterRole属于集群范围，所以它也可以为以下资源授予访问权限：

- 集群范围资源，比如`Node`；
- 非资源端点，比如`/healthz`；
- 跨名称空间访问别的名称空间作用域下的资源(如Pods等)，例如，我们可以使用ClusterRole来允许某特定用户执行`kubectl get pods --all-namespaces`命令。

下面是一个ClusterRole的示例，可用来为任一特定名称空间中的Secret授予读访问权限，或者跨名称空间的访问权限(取决于该角色是如何[绑定](https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding)的)：

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace"被忽略，因为ClusterRoles不受名字空间限制
  name: secret-reader
rules:
- apiGroups: [""]
  # 在HTTP层面，用来访问Secret对象的资源的名称为"secrets"
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
```

> **说明**：Role或ClusterRole对象的名称要求能被安全地用作路径中的片段。换句话说，其名称不能是`.`、`..`，也不可以包含`/`或`%`这些字符。

###### 13.2.2.2 RoleBinding和ClusterRoleBinding

角色绑定(Role Binding)是将角色中定义的权限赋予一个或者一组用户。它包含若干**主体**(用户、组或服务账户)的列表和对这些主体所获得的角色的引用。RoleBinding在指定的名称空间中执行授权，而ClusterRoleBinding在集群范围执行授权。

一个RoleBinding可以引用同一名称空间中的任何Role。或者，一个 RoleBinding 可以引用某 ClusterRole 并将该ClusterRole绑定到RoleBinding所在的名称空间。如果我们希望将某ClusterRole绑定到集群中所有名称空间，这时需要使用ClusterRoleBinding。

> **说明**：RoleBinding或ClusterRoleBinding对象的名称要求能被安全地用作路径中的片段。换句话说，其名称不能是`.`、`..`，也不可以包含`/`或`%`这些字符。

关于RoleBinding的示例：

下面例子中的RoleBinding将设置了具有读取pods权限的名为"pod-reader"的Role授予在"default"名称空间中的用户"jane"。这样，用户"jane"就具有了读取"default"名称空间中pods的权限。

```yaml
apiVersion: rbac.authorization.k8s.io/v1
# 此角色绑定允许"jane"读取"default"名称空间中的Pods
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
# 我们可以指定不止一个"subject(主体)"
- kind: User
  name: jane # "name"是不区分大小写的
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # "roleRef"指定与某Role或ClusterRole的绑定关系
  kind: Role # 此字段必须是Role或ClusterRole
  name: pod-reader # 此字段必须与我们要绑定的Role或ClusterRole的名称匹配
  apiGroup: rbac.authorization.k8s.io
```

RoleBinding也可以引用ClusterRole，以将对应ClusterRole中定义的访问权限授予RoleBinding所在名称空间的资源。这种引用使得我们可以跨整个集群定义一组通用的角色，之后在多个名称空间中复用。

比如下面的例子中，尽管下面的RoleBinding引用的是一个ClusterRole，"dave"(这里的主体，不区分大小写)只能访问"development"名称空间中的Secrets对象，因为RoleBinding所在的名称空间是"development"。

```yaml
apiVersion: rbac.authorization.k8s.io/v1
# 此角色绑定使得用户"dave"能够读取"development"名称空间中的Secrets
# 我们需要一个名为"secret-reader"的ClusterRole
kind: RoleBinding
metadata:
  name: read-secrets
  # RoleBinding的名称空间决定了访问权限的授予范围
  # 这里只授予在"development"名称空间内的访问权限
  namespace: development
subjects:
- kind: User
  name: dave # "name"是不区分大小写的
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
```

关于ClusterRoleBinding的示例：

要跨整个集群完成访问权限的授予，可以使用ClusterRoleBinding。下面的ClusterRoleBinding允许"manager"组内的所有用户访问任何名称空间中的Secrets。

```yaml
apiVersion: rbac.authorization.k8s.io/v1
# 此集群角色绑定允许"manager"组中的任何人访问任何名称空间中的secrets
kind: ClusterRoleBinding
metadata:
  name: read-secrets-global
subjects:
- kind: Group
  name: manager # "name"是不区分大小写的
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
```

创建了绑定之后，我们不能再修改绑定对象所引用的Role或ClusterRole。试图改变绑定对象的`roleRef`将导致合法性检查错误。如果我们想要改变现有绑定对象中`roleRef`字段的内容，必须删除重新创建绑定对象。

这种限制有两个主要原因：

1. 针对不同角色的绑定是完全不一样的绑定。要求通过删除/重建绑定来更改`roleRef`，这样可以确保要赋予绑定的所有主体会被授予新的角色(而不是在允许或者不小心修改了`roleRef`的情况下导致所有现有主体未经验证即被授予新角色对应的权限)。
2. 将`roleRef`设置为不可以改变，这使得可以为用户授予对现有绑定对象的`update`权限，这样可以让他们管理主体列表，同时不能更改被授予这些主体的角色。

命令`kubectl auth reconcile`可以创建或者更新包含RBAC对象的清单文件，并且在必要的情况下删除和重新创建绑定对象，以改变所引用的角色。

###### 13.2.2.3 资源的引用

在 Kubernetes API 中，大多数资源都是使用对象名称的字符串表示来呈现与访问的。例如，对于Pod应使用"pods"。RBAC使用对应 API 端点的 URL 中呈现的名字来引用资源。有一些Kubernetes API涉及子资源(subresource)，例如Pod的日志。对Pod日志的请求看起来像这样：

```http
GET /api/v1/namespaces/{namespace}/pods/{name}/log
```

在这里，`pods`对应名称空间作用域的Pod资源，而`log`是`pods`的子资源。在RBAC角色表达子资源时，使用斜线(`/`)来分隔资源和子资源。要允许某主体读取`pods`同时访问这些Pod的`log`子资源，我们可以这么写：

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-and-pod-logs-reader
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "list"]
```

对于某些请求，也可以通过`resourceNames`列表按名称引用资源。在指定时，可以将请求限定为资源的单个实例。下面的例子中限制可以"get"和"update"一个名为`my-configmap`的ConfigMap：

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: configmap-updater
rules:
- apiGroups: [""]
  # 在HTTP层面，用来访问ConfigMap的资源的名称为"configmaps"
  resources: ["configmaps"]
  resourceNames: ["my-configmap"]
  verbs: ["update", "get"]
```

> **说明**：我们不能针对`create`或者`deletecollection`请求来实施resourceName限制。对于`create`操作而言，这是因为在鉴权时还不知道对象名称。

###### 13.2.2.4 聚合的ClusterRole

我们可以将若干ClusterRole **聚合(Aggregate)**起来，形成一个复合的ClusterRole。某个控制器作为集群控制面的一部分会监视带有`aggregationRule`的ClusterRole对象集合。`aggregationRule`为控制器定义一个标签选择算符供后者匹配应该组合到当前ClusterRole的`roles`字段中的ClusterRole对象。

下面是一个聚合ClusterRole的示例：

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: monitoring
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.example.com/aggregate-to-monitoring: "true"
rules: [] # 控制面自动填充这里的规则
```

如果我们创建一个与某现有聚合ClusterRole的标签选择算符匹配的ClusterRole，这一变化会触发新的规则被添加到聚合ClusterRole的操作。下面的例子中，通过创建一个标签同样为`rbac.example.com/aggregate-to-monitoring: true`的ClusterRole，新的规则可被添加到"monitoring" ClusterRole中。

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: monitoring-endpoints
  labels:
    rbac.example.com/aggregate-to-monitoring: "true"
# 当你创建 "monitoring-endpoints" ClusterRole 时，
# 下面的规则会被添加到 "monitoring" ClusterRole 中
rules:
- apiGroups: [""]
  resources: ["services", "endpoints", "pods"]
  verbs: ["get", "list", "watch"]
```

默认的面向用户的角色使用ClusterRole聚合。这使得作为集群管理员的我们可以扩展默认规则，包括为定制资源设置规则，比如通过CustomResourceDefinitions或聚合API server提供的定制资源。

例如，下面的 ClusterRoles 让默认角色"admin"和"edit"拥有管理自定义资源"CronTabs"的权限，"view"角色对CronTab资源拥有读操作权限。我们可以假定CronTab对象在API server所看到的URL中被命名为`"crontabs"`。

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: aggregate-cron-tabs-edit
  labels:
    # 添加以下权限到默认角色 "admin" 和 "edit" 中
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
rules:
- apiGroups: ["stable.example.com"]
  resources: ["crontabs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: aggregate-cron-tabs-view
  labels:
    # 添加以下权限到 "view" 默认角色中
    rbac.authorization.k8s.io/aggregate-to-view: "true"
rules:
- apiGroups: ["stable.example.com"]
  resources: ["crontabs"]
  verbs: ["get", "list", "watch"]
```

**Role示例：**

以下示例均是从Role或CLusterRole对象中截取出来的，仅展示其`rules`部分。

允许读取在核心[API 组](https://kubernetes.io/zh/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning)下的`"Pods"`：

```yaml
rules:
- apiGroups: [""]
  # 在 HTTP 层面，用来访问 Pod 的资源的名称为 "pods"
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
```

允许读/写在"extensions"和"apps"API 组中的Deployment(在HTTP层面，对应URL中的资源为"deployments")：

```yaml
rules:
- apiGroups: ["extensions", "apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
```

允许读取核心API 组中的"pods"和读/写`"batch"`或`"extensions"`API 组中的"jobs"：

```yaml
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch", "extensions"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
```

允许读取名称为"my-config"的ConfigMap(需要通过RoleBinding绑定以限制为某名字空间中特定的ConfigMap)：

```yaml
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["my-config"]
  verbs: ["get"]
```

允许读取在核心组中的"nodes"资源(因为`Node`是集群作用域的，所以需要ClusterRole绑定到ClusterRoleBinding才生效)：

```yaml
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]
```

允许针对非资源端点`/healthz`和其子路径上发起GET和POST请求(必须在ClusterRole绑定ClusterRoleBinding才生效)：

```yaml
rules:
  - nonResourceURLs: ["/healthz", "/healthz/*"] # nonResourceURL 中的 '*' 是一个全局通配符
    verbs: ["get", "post"]
```

###### 13.2.2.5 对主体的引用

RoleBinding或者ClusterRoleBinding可绑定角色到某*主体(Subject)*上。主体可以是组，用户或者[服务账户](https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-service-account/)。

Kubernetes用字符串来表示用户名。用户名可以是普通的用户名，像"alice"，或者是邮件风格的名称，或者是以字符串形式表达的数字ID。我们作为k8s管理员负责配置[身份认证模块](https://kubernetes.io/zh/docs/reference/access-authn-authz/authentication/)以便后者能够生成我们所期望的格式的用户名。

> **注意**：前缀`system:`是Kubernetes系统保留的，所以我们要确保所配置的用户名或者组名不能出现上述`system:`前缀。除了对前缀的限制之外，RBAC鉴权系统不对用户名格式作任何要求。

在Kubernetes中，鉴权模块提供用户组信息。与用户名一样，用户组名也用字符串来表示，而且对该字符串没有格式要求，只是不能使用保留的前缀`system:`。

[服务账户](https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-service-account/)的用户名前缀为`system:serviceaccount:`，属于前缀为`system:serviceaccounts:`的用户组。

> **说明**：`system:serviceaccount:`(单数)是用于服务账户用户名的前缀；`system:serviceaccounts:`(复数)是用于服务账户组名的前缀。

**RoleBinding示例：**

下面示例是`RoleBinding`中的片段，仅展示其`subjects`的部分。

对于名称为`alice@example.com`的用户：

```yaml
subjects:
- kind: User
  name: "alice@example.com"
  apiGroup: rbac.authorization.k8s.io
```

对于名称为`frontend-admins`的用户组：

```yaml
subjects:
- kind: Group
  name: "frontend-admins"
  apiGroup: rbac.authorization.k8s.io
```

对于`kube-system`名称空间中的默认服务账户：

```yaml
subjects:
- kind: ServiceAccount
  name: default
  namespace: kube-system
```

对于任何名称空间中的"qa"组中所有的服务账户：

```yaml
subjects:
- kind: Group
  name: system:serviceaccounts:qa
  apiGroup: rbac.authorization.k8s.io
```

对于"development"名称空间中"dev"组中的所有服务帐户：

```yaml
subjects:
- kind: Group
  name: system:serviceaccounts:dev
  apiGroup: rbac.authorization.k8s.io
  namespace: development
```

对于在任何名字空间中的服务账户：

```yaml
subjects:
- kind: Group
  name: system:serviceaccounts
  apiGroup: rbac.authorization.k8s.io
```

对于所有已经过认证的用户：

```yaml
subjects:
- kind: Group
  name: system:authenticated
  apiGroup: rbac.authorization.k8s.io
```

对于所有未通过认证的用户：

```yaml
subjects:
- kind: Group
  name: system:unauthenticated
  apiGroup: rbac.authorization.k8s.io
```

对于所有用户：

```yaml
subjects:
- kind: Group
  name: system:authenticated
  apiGroup: rbac.authorization.k8s.io
- kind: Group
  name: system:unauthenticated
  apiGroup: rbac.authorization.k8s.io
```

###### 13.2.2.6 权限提升预防和引导

RBAC API会阻止用户通过编辑角色或者角色绑定来提升权限。由于这一点是在API级别实现的，所以在RBAC鉴权组件未启用的状态下依然可以正常工作。

**对角色创建或更新的限制：**

只有在符合下列条件之一的情况下，我们才能创建/更新角色:

1. 我们已经拥有角色中包含的所有权限，且其作用域与正被修改的对象作用域相同(对ClusterRole而言意味着集群范围，对Role而言意味着相同名称空间或者集群范围）。
2. 我们被显式授权在`rbac.authorization.k8s.io` API组中的`roles`或`clusterroles`资源使用`escalate`动词。

例如，如果`user-1`没有列举集群范围所有Secret的权限，他将不能创建包含该权限的ClusterRole。若要允许用户创建/更新角色：

1. 根据需要赋予他们一个角色，允许他们根据需要创建/更新Role或者ClusterRole对象。
2. 授予他们在所创建/更新角色中包含特殊权限的权限:
   - 隐式地为他们授权(如果它们试图创建或者更改Role或ClusterRole的权限，但自身没有被授予相应权限，API请求将被禁止)。
   - 通过允许他们在Role或ClusterRole资源上执行`escalate`显式完成授权。这里的`roles`和`clusterroles`资源包含在`rbac.authorization.k8s.io`API组中。

**对角色绑定创建或更新的限制：**

只有我们已经具有了所引用的角色中包含的全部权限时，或者我们被授权在所引用的角色上执行`bind`动词时，我们才可以创建或更新角色绑定。这里的权限与角色绑定的作用域相同。例如，如果用户`user-1`没有列举集群范围所有Secret的能力，则他不可以创建ClusterRoleBinding引用授予该许可权限的角色。如要允许用户创建或更新角色绑定：

1. 赋予他们一个角色，使得他们能够根据需要创建或更新RoleBinding或ClusterRoleBinding对象。
2. 授予他们绑定某特定角色所需要的许可权限：
   - 隐式授权下，可以将角色中包含的许可权限授予他们；
   - 显式授权下，可以授权他们在特定Role或ClusterRole上执行`bind`动词的权限。

例如，下面的ClusterRole和RoleBinding将允许用户`user-1`把名称空间`user-1-namespace`中的`admin`、`edit`和`view`角色赋予其他用户：

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: role-grantor
rules:
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["rolebindings"]
  verbs: ["create"]
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["clusterroles"]
  verbs: ["bind"]
  # 忽略 resourceNames 意味着允许绑定任何 ClusterRole
  resourceNames: ["admin","edit","view"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: role-grantor-binding
  namespace: user-1-namespace
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: role-grantor
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: user-1
```

当启动引导第一个角色和角色绑定时，需要为初始用户授予他们尚未拥有的权限。对初始角色和角色绑定进行初始化的时候需要：

- 使用用户组为`system:masters`的凭据，该用户组由默认绑定关联到`cluster-admin`这个超级用户角色。
- 如果我们的API server启动时启用了不安全端口(使用`--insecure-port`)，我们也可以通过该端口调用API，这样的操作会绕过身份验证或鉴权。

###### 13.2.2.7 一些命令行工具

<font size=4px color="croci">kubectl create role</font>

创建Role对象，定义在某一名称空间中的权限。例如：

- 创建名称为"pod-reader"的Role对象，允许用户对Pods执行`get`、`watch`和`list`操作：

  ```shell
  kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods
  ```

- 创建名称为"pod-reader"的Role对象并指定`resourceNames`：

  ```shell
  kubectl create role pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
  ```

- 创建名为"foo"的Role对象并指定`apiGroups`：

  ```shell
  kubectl create role foo --verb=get,list,watch --resource=replicasets.apps
  ```

- 创建名为"foo"的Role对象并指定子资源权限:

  ```shell
  kubectl create role foo --verb=get,list,watch --resource=pods,pods/status
  ```

- 创建名为"my-component-lease-holder"的Role对象，使其具有对特定名称的资源执行get/update的权限：

  ```shell
  kubectl create role my-component-lease-holder --verb=get,list,watch,update --resource=lease --resource-name=my-component
  ```

<font size=4px color="croci">kubectl create clusterrole</font>

创建ClusterRole对象。例如：

- 创建名称为"pod-reader"的`ClusterRole`对象，允许用户对Pods对象执行`get`、`watch`和`list`操作：

  ```shell
  kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods
  ```

- 创建名为"pod-reader"的ClusterRole对象并指定`resourceNames`：

  ```shell
  kubectl create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
  ```

- 创建名为"foo"的ClusterRole对象并指定`apiGroups`：

  ```shell
  kubectl create clusterrole foo --verb=get,list,watch --resource=replicasets.apps
  ```

- 创建名为"foo"的ClusterRole对象并指定子资源：

  ```shell
  kubectl create clusterrole foo --verb=get,list,watch --resource=pods,pods/status
  ```

- 创建名为"foo"的ClusterRole对象并指定`nonResourceURL`：

  ```shell
  kubectl create clusterrole "foo" --verb=get --non-resource-url=/logs/*
  ```

- 创建名为"monitoring"的ClusterRole对象并指定`aggregationRule`：

  ```shell
  kubectl create clusterrole monitoring --aggregation-rule="rbac.example.com/aggregate-to-monitoring=true"
  ```

<font size=4px color="croci">kubectl create rolebinding</font>

在特定的名称空间中对`Role`或`ClusterRole`授权。例如：

- 在名称空间"acme"中，将名为`admin`的ClusterRole中的权限授予名称"bob"的用户:

  ```shell
  kubectl create rolebinding bob-admin-binding --clusterrole=admin --user=bob --namespace=acme
  ```

- 在名称空间"acme"中，将名为`view`的ClusterRole中的权限授予名称空间"acme"中名为`myapp`的服务账户：

  ```shell
  kubectl create rolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp --namespace=acme
  ```

- 在名称空间 "acme" 中，将名为 `view` 的ClusterRole对象中的权限授予名称空间 "myappnamespace" 中名称为`myapp`的服务账户：

  ```shell
  kubectl create rolebinding myappnamespace-myapp-view-binding --clusterrole=view --serviceaccount=myappnamespace:myapp --namespace=acme
  ```

<font size=4px color="croci">kubectl create clusterrolebinding</font>

在整个集群(所有名称空间)中用ClusterRole授权。例如：

- 在整个集群范围，将名为`cluster-admin`的ClusterRole中定义的权限授予名为"root"用户：

  ```shell
  kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=root
  ```

- 在整个集群内，将名为`system:node-proxier`的ClusterRole的权限授予名为"system:kube-proxy"的用户：

  ```shell
  kubectl create clusterrolebinding kube-proxy-binding --clusterrole=system:node-proxier --user=system:kube-proxy
  ```

- 在整个集群范围内，将名为`view`的ClusterRole中定义的权限授予"acme"名称空间中名为"myapp"的服务账户：

  ```shell
  kubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp
  ```

<font size=4px color="croci">kubectl auth reconcile</font>

使用清单文件来创建或者更新`rbac.authorization.k8s.io/v1` API对象。

尚不存在的对象会被创建，如果对应的名称空间也不存在，必要的话也会被创建。已经存在的角色会被更新，使之包含输入对象中所给的权限。如果指定了`--remove-extra-permissions`，可以删除额外的权限。

已经存在的绑定也会被更新，使之包含输入对象中所给的主体。如果指定了`--remove-extra-permissions`，则可以删除多余的主体。

例如:

- 测试应用RBAC对象的清单文件，显示将要进行的更改：

  ```shell
  kubectl auth reconcile -f my-rbac-rules.yaml --dry-run
  ```

- 应用RBAC对象的清单文件，保留角色中的额外权限和绑定中的其他主体：

  ```shell
  kubectl auth reconcile -f my-rbac-rules.yaml
  ```

- 应用RBAC对象的清单文件，删除角色中的额外权限和绑定中的其他主体：

  ```shell
  kubectl auth reconcile -f my-rbac-rules.yaml --remove-extra-subjects --remove-extra-permissions
  ```

###### 13.2.2.8 服务账户权限

默认的RBAC策略为控制面组件、节点和控制器授予权限。但是不会对`kube-system`名称空间之外的服务账户授予权限(除了授予所有已认证用户的发现权限)。

这使得我们可以根据需要向特定服务账户授予特定权限。细粒度的角色绑定可带来更好的安全性，但需要更多精力管理。 粗粒度的授权可能导致服务账户被授予不必要的API访问权限(甚至导致潜在的权限提升)，但更易于管理。

按从最安全到最不安全的顺序，存在以下方法：

1. 为特定应用的服务账户授予角色(最佳实践)

   这要求应用在其Pod规约中指定`serviceAccountName`，并额外创建服务账户(包括通过API、应用程序清单、`kubectl create serviceaccount`等)。

   例如，在名称空间"my-namespace"中授予服务账户"my-sa"只读权限：

   ```shell
   kubectl create rolebinding my-sa-view \
     --clusterrole=view \
     --serviceaccount=my-namespace:my-sa \
     --namespace=my-namespace
   ```

2. 将角色授予某名称空间中的"default"服务账户

   如果某应用没有指定`serviceAccountName`，那么它将使用"default"服务账户。

   > **说明**："default"服务账户所具有的权限会被授予给名称空间中所有未指定`serviceAccountName`的Pod。

   例如，在名称空间"my-namespace"中授予服务账户"default"只读权限：

   ```shell
   kubectl create rolebinding default-view \
     --clusterrole=view \
     --serviceaccount=my-namespace:default \
     --namespace=my-namespace
   ```

   许多[插件组件](https://kubernetes.io/zh/docs/concepts/cluster-administration/addons/)在`kube-system`名称空间以"default"服务账户运行。要允许这些插件组件以超级用户权限运行，需要将集群的`cluster-admin`权限授予`kube-system`名称空间中的"default"服务账户。

   > **说明**：启用这一配置意味着在`kube-system`名称空间中包含以超级用户账号来访问API的Secrets。

   ```shell
   kubectl create clusterrolebinding add-on-cluster-admin \
     --clusterrole=cluster-admin \
     --serviceaccount=kube-system:default
   ```

1. 将角色授予名字空间中所有服务账户

   如果我们想要名称空间中所有应用都具有某角色，无论它们使用的什么服务账户，可以将角色授予该名称空间的服务账户组。

   例如，在名称空间"my-namespace"中的只读权限授予该名称空间中的所有服务账户：

   ```shell
   kubectl create rolebinding serviceaccounts-view \
     --clusterrole=view \
     --group=system:serviceaccounts:my-namespace \
     --namespace=my-namespace
   ```

1. 在集群范围内为所有服务账户授予一个受限角色(不鼓励)

   如果我们不想管理每一个名称空间的权限，可以向所有的服务账户授予集群范围的角色。

   例如，为集群范围的所有服务账户授予跨所有名称空间的只读权限：

   ```shell
   kubectl create clusterrolebinding serviceaccounts-view \
     --clusterrole=view \
     --group=system:serviceaccounts
   ```

1. 授予超级用户访问权限给集群范围内的所有服务帐户(强烈不鼓励)

   如果我们不关心如何区分权限，可以将超级用户访问权限授予所有服务账户。

   > **<font color="red">警告：</font>**这样做会允许所有应用都对我们的集群拥有完全的访问权限，并将允许所有能够读取Secret(或创建 Pod)的用户对我们的集群有完全的访问权限。

   ```shell
   kubectl create clusterrolebinding serviceaccounts-cluster-admin \
     --clusterrole=cluster-admin \
     --group=system:serviceaccounts
   ```

###### 13.2.2.9 宽松的RBAC权限

我们可以使用RBAC角色绑定在多个场合使用宽松的策略。

下面的策略允许**所有**服务帐户充当集群管理员。容器中运行的所有应用程序都会自动收到服务帐户的凭据，可以对API执行任何操作，包括查看Secrets和修改权限。这一策略是不被推荐的。

```shell
kubectl create clusterrolebinding permissive-binding \
  --clusterrole=cluster-admin \
  --user=admin \
  --user=kubelet \
  --group=system:serviceaccounts
```

在我们完成到RBAC的迁移后，应该调整集群的访问控制，确保相关的策略满足我们的信息安全需求。

##### 13.2.3 Node鉴权

节点鉴权是一种特殊用途的鉴权模式，专门对kubelet发出的API请求进行鉴权。

###### 13.2.3.1 概念

节点鉴权器允许kubelet执行API操作。包括：

读取操作：

- services；
- endpoints；
- nodes；
- pods；
- secrets、configmaps、pvcs以及绑定到kubelet节点的与pod相关的持久卷。

写入操作：

- 节点和节点状态(启用`NodeRestriction`准入插件以限制kubelet只能修改自己的节点)；
- Pod和Pod状态(启用`NodeRestriction`准入插件以限制kubelet只能修改绑定到自身的Pod)；
- 事件。

鉴权相关操作：

- 对于基于TLS的启动引导过程时使用的certificationsigningrequests API的读/写权限；
- 为委派的身份验证/授权检查创建tokenreviews和subjectaccessreviews的能力。

在将来的版本中，节点鉴权器可能会添加或删除权限，以确保kubelet具有正确操作所需的最小权限集。

为了获得节点鉴权器的授权，kubelet必须使用一个凭证以表示它在`system:nodes`组中，用户名为`system:node:<nodeName>`。上述的组名和用户名格式要与[kubelet TLS 启动引导](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/)过程中为每个kubelet创建的标识相匹配。

要启用节点授权器，请使用`--authorization-mode = Node`启动apiserver。

要限制kubelet具有写入权限的API对象，请使用`--enable-admission-plugins=...,NodeRestriction,...`启动 apiserver，从而启用[NodeRestriction](https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers#NodeRestriction)准入插件。

###### 13.2.3.2 RBAC节点权限

在1.6版本中，当使用[RBAC 鉴权模式](https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/)时，`system:nodes`集群角色会被自动绑定到`system:node`组。

在1.7版本中，不再推荐将 `system:nodes` 组自动绑定到 `system:node` 角色，因为节点鉴权器通过对 secret 和 configmap 访问的额外限制完成了相同的任务。如果同时启用了 `Node` 和 `RBAC` 授权模式，1.7 版本则不会创建 `system:nodes` 组到 `system:node` 角色的自动绑定。

在1.8版本中，绑定将根本不会被创建。

使用RBAC时，将继续创建`system:node`集群角色，以便与将其他用户或组绑定到该角色的部署方法兼容。

##### 13.2.4 Webhook模式

WebHook是一种HTTP回调：某些条件下触发的HTTP POST请求；通过HTTP POST发送的简单事件通知。一个基于web应用实现的WebHook会在特定事件发生时把消息发送给特定的URL。

具体来说，当在判断用户权限时，`Webhook`模式会使Kubernetes查询外部的REST服务。

###### 13.2.4.1 配置文件格式

`Webhook`模式需要一个HTTP配置文件，通过`--authorization-webhook-config-file=SOME_FILENAME`的参数声明。

配置文件的格式使用[kubeconfig](https://kubernetes.io/zh/docs/tasks/access-application-cluster/configure-access-multiple-clusters/)。在文件中，"users"代表着API服务器的webhook，而"cluster"代表着远程服务。

使用HTTPS客户端认证的配置案例：

```yaml
# Kubernetes API 版本
apiVersion: v1
# API 对象种类
kind: Config
# clusters 代表远程服务。
clusters:
  - name: name-of-remote-authz-service
    cluster:
      # 对远程服务进行身份认证的 CA。
      certificate-authority: /path/to/ca.pem
      # 远程服务的查询 URL。必须使用 'https'。
      server: https://authz.example.com/authorize

# users 代表 API 服务器的 webhook 配置
users:
  - name: name-of-api-server
    user:
      client-certificate: /path/to/cert.pem # webhook plugin 使用 cert
      client-key: /path/to/key.pem          # cert 所对应的 key

# kubeconfig 文件必须有 context。需要提供一个给 API 服务器。
current-context: webhook
contexts:
- context:
    cluster: name-of-remote-authz-service
    user: name-of-api-server
  name: webhook
```

###### 13.2.4.2 请求载荷

在做认证决策时，API服务器会POST一个JSON序列化的`authorization.k8s.io/v1beta1` `SubjectAccessReview`对象来描述这个动作。这个对象包含了描述用户请求的字段，同时也包含了需要被访问资源或请求特征的具体信息。

需要注意的是，webhook API对象与其他Kubernetes API对象一样都同样服从[版本兼容规则](https://kubernetes.io/zh/docs/concepts/overview/kubernetes-api/)。实施人员应该了解beta对象的更宽松的兼容性承诺，同时确认请求的 "apiVersion" 字段能被正确地反序列化。此外，API 服务器还必须启用 `authorization.k8s.io/v1beta1` API 扩展组(`--runtime-config=authorization.k8s.io/v1beta1=true`)。

一个请求内容的案例：

```json
{
  "apiVersion": "authorization.k8s.io/v1beta1",
  "kind": "SubjectAccessReview",
  "spec": {
    "resourceAttributes": {
      "namespace": "kittensandponies",
      "verb": "get",
      "group": "unicorn.example.org",
      "resource": "pods"
    },
    "user": "jane",
    "group": [
      "group1",
      "group2"
    ]
  }
}
```

期待远程服务填充请求的 `status` 字段并响应允许或禁止访问。响应主体的 `spec` 字段被忽略，可以省略。允许的响应将返回:

```json
{
  "apiVersion": "authorization.k8s.io/v1beta1",
  "kind": "SubjectAccessReview",
  "status": {
    "allowed": true
  }
}
```

为了禁止访问，有两种方法。

在大多数情况下，第一种方法是首选方法，它指示授权 webhook 不允许或对请求"无意见"，但是，如果配置了其他授权者，则可以给他们机会允许请求。如果没有其他授权者，或者没有一个授权者，则该请求被禁止。webhook 将返回:

```json
{
  "apiVersion": "authorization.k8s.io/v1beta1",
  "kind": "SubjectAccessReview",
  "status": {
    "allowed": false,
    "reason": "user does not have read access to the namespace"
  }
}
```

第二种方法立即拒绝其他配置的授权者进行短路评估。仅应由对集群的完整授权者配置有详细了解的 webhook 使用。webhook 将返回:

```json
{
  "apiVersion": "authorization.k8s.io/v1beta1",
  "kind": "SubjectAccessReview",
  "status": {
    "allowed": false,
    "denied": true,
    "reason": "user does not have read access to the namespace"
  }
}
```

对于非资源的路径访问是这么发送的:

```json
{
  "apiVersion": "authorization.k8s.io/v1beta1",
  "kind": "SubjectAccessReview",
  "spec": {
    "nonResourceAttributes": {
      "path": "/debug",
      "verb": "get"
    },
    "user": "jane",
    "group": [
      "group1",
      "group2"
    ]
  }
}
```

非资源类的路径包括：`/api`，`/apis`，`/metrics`，`/resetMetrics`，`/logs`，`/debug`，`/healthz`，`/swagger-ui/`，`/swaggerapi/`，`/ui`和`/version`。客户端需要访问`/api`，`/api/*`，`/apis`，`/apis/*`和`/version`以便能发现服务器上有什么资源和版本。对于其他非资源类的路径访问在没有REST API访问限制的情况下拒绝。

##### 13.2.5 ABAC鉴权

基于属性的访问控制(Attribute-based access control - ABAC)定义了访问控制范例，其中通过使用将属性组合在一起的策略来向用户授予访问权限。

###### 13.2.5.1 策略文件格式

基于`ABAC`模式，可以这样指定策略文件`--authorization-policy-file=SOME_FILENAME`。

此文件格式是[JSON Lines](https://jsonlines.org/)，不应存在封闭的列表或映射，每行一个映射。

每一行都是一个策略对象，策略对象是具有以下属性的映射：

- 版本控制属性：

  - `apiVersion`，字符串类型：有效值为`abac.authorization.kubernetes.io/v1beta1`，允许对策略格式进行版本控制和转换。

  - `kind`，字符串类型：有效值为`Policy`，允许对策略格式进行版本控制和转换。

- `spec`配置为具有以下映射的属性：
  - 主体匹配属性：
    - `user`，字符串类型；来自`--token-auth-file`的用户字符串，如果我们指定`user`，它必须与验证用户的用户名匹配。
    - `group`，字符串类型；若指定`group`，它必须与经过身份验证用户的一个组匹配，`system:authenticated`匹配所有经过身份验证的请求。`system:unauthenticated`匹配所有未经过身份验证的请求。

- 资源匹配属性：
  - `apiGroup`，字符串类型；一个API组。
    - 例如：`extensions`。
    - 通配符：`*`匹配所有 API 组。
  - `namespace`，字符串类型；一个命名空间。
    - 例如：`kube-system`。
    - 通配符：`*`匹配所有资源请求。
  - `resource`，字符串类型；资源类型。
    - 例如：`pods`。
    - 通配符：`*`匹配所有资源请求。
- 非资源匹配属性：
  - `nonResourcePath`，字符串类型；非资源请求路径。
    - 例如：`/version`或`/apis`。
    - 通配符：
      - `*`匹配所有非资源请求。
      - `/foo/*` 匹配 `/foo/` 的所有子路径。
- `readonly`，键入布尔值，如果为 true，则表示该策略仅适用于 get、list 和 watch 操作。

> **说明**：属性未设置等效于属性被设置为对应类型的零值(例如空字符串、0、false)，然而，出于可读性考虑，应尽量选择不设置这类属性。在将来，策略可能以JSON格式表示，并通过REST界面进行管理。

###### 13.2.5.2 鉴权算法

请求具有与策略对象的属性对应的属性。

当接收到请求时，确定属性。未知属性设置为其类型的零值（例如：空字符串，0，false）。

设置为 `"*"` 的属性将匹配相应属性的任何值。

检查属性的元组，以匹配策略文件中的每个策略。如果至少有一行匹配请求属性，则请求被鉴权(但仍可能无法通过稍后的合法性检查)。

要允许任何经过身份验证的用户执行某些操作，请将策略组属性设置为 `"system:authenticated"`。

要允许任何未经身份验证的用户执行某些操作，请将策略组属性设置为 `"system:authentication"`。

要允许用户执行任何操作，请使用 apiGroup，命名空间， 资源和 nonResourcePath 属性设置为 `"*"` 的策略。

要允许用户执行任何操作，请使用设置为 `"*"` 的 apiGroup，namespace，resource 和 nonResourcePath 属性编写策略。

#### 13.3 准入控制

##### 13.3.1 什么是准入控制插件

准入控制器是一段代码，它会在请求通过认证和授权之后、对象被持久化之前拦截到达API服务器的请求。控制器由下面的 [列表](https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do) 组成，并编译 进`kube-apiserver `二进制文件，并且只能由集群管理员配置。在该列表中，有两个特殊的控制器：MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook。它们根据 API 中的配置，分别执行变更和验证 [准入控制 webhook](https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks)。

准入控制器可以执行 “验证(Validating)” 和/或 “变更(Mutating)” 操作。 变更(mutating)控制器可以修改被其接受的对象；验证(validating)控制器则不行。

准入控制过程分为两个阶段。第一阶段，运行变更准入控制器。第二阶段，运行验证准入控制器。 再次提醒，某些控制器既是变更准入控制器又是验证准入控制器。

如果任何一个阶段的任何控制器拒绝了该请求，则整个请求将立即被拒绝，并向终端用户返回一个错误。

最后，除了对对象进行变更外，准入控制器还可以有其它作用：将相关资源作为请求处理的一部分进行变更。 增加使用配额就是一个典型的示例，说明了这样做的必要性。此类用法都需要相应的回收或回调过程，因为任一准入控制器都无法确定某个请求能否通过所有其它准入控制器。

##### 13.3.2 为什么需要准入控制器

Kubernetes的许多高级功能都要求启用一个准入控制器，以便正确地支持该特性。因此，没有正确配置准入控制器的 Kubernetes API服务器是不完整的，它无法支持我们期望的所有特性。

##### 13.3.3 如何启用一个准入控制器

Kubernetes API服务器的`enable-admission-plugins`标志接受一个用于在集群修改对象之前调用的(以逗号分隔的)准入控制插件顺序列表。

例如，下面的命令就启用了`NamespaceLifecycle`和`LimitRanger`准入控制插件：

```shell
kube-apiserver --enable-admission-plugins=NamespaceLifecycle,LimitRanger ...
```

> **说明**：根据我们Kubernetes集群的部署方式以及API server的启动方式的不同，我们可能需要以不同的方式应用设置。例如，如果将API server部署为systemd服务，我们可能需要修改systemd单元文件；如果以自托管方式部署 Kubernetes，我们可能需要修改API服务器的清单文件。

##### 13.3.4 怎么关闭准入控制器

Kubernetes API服务器的`disable-admission-plugins`标志，会将传入的(以逗号分隔的)准入控制插件列表禁用，即使是默认启用的插件也会被禁用。

```shell
kube-apiserver --disable-admission-plugins=PodNodeSelector,AlwaysDeny ...
```

##### 13.3.5 哪些插件是默认启用的

下面的命令可以查看哪些插件是默认启用的：

```shell
kube-apiserver -h | grep enable-admission-plugins
```

在目前版本中，它们是：

```shell
CertificateApproval, CertificateSigning, CertificateSubjectRestriction, DefaultIngressClass, DefaultStorageClass, DefaultTolerationSeconds, LimitRanger, MutatingAdmissionWebhook, NamespaceLifecycle, PersistentVolumeClaimResize, Priority, ResourceQuota, RuntimeClass, ServiceAccount, StorageObjectInUseProtection, TaintNodesByCondition, ValidatingAdmissionWebhook
```

##### 13.3.6 推荐的准入控制器

推荐使用的准入控制器默认情况下都处于启用状态(可以查看[kube-apiserver](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/#options))。 因此，我们无需显式指定它们。我们可以使用`--enable-admission-plugins`标志(顺序不重要)来启用默认设置以外的其他准入控制器。

### 14.网络策略

如果我们希望在IP地址或端口层面(OSI第3层或第4层)控制网络流量，则可以考虑为集群中特定应用使用Kubernetes网络策略(NetworkPolicy)。NetworkPolicy是一种以应用为中心的结构，允许我们设置如何允许Pod与网络上的各类网络""实体"(这里使用实体以避免过度使用诸如"端点"和"服务"这类常用术语，因为这些术语在Kubernetes中是有特定含义的)通信。

Pod可以通信的实体是通过如下三个标识符的组合来辩识的：

1. 其他被允许的Pods(例外Pod无法阻塞对自身的访问；
2. 被允许的名称空间；
3. IP组块(例外与Pod运行所在的节点的通信总是被允许的，无论Pod或节点的IP地址)。

在定义基于Pod或名称空间的NetworkPolicy时，我们可以通过使用[选择算符](https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/labels/)来设定哪些流量可以进入或离开与该算符匹配的 Pod。同时，当基于IP的NetworkPolicy被创建时，我们基于IP组块(CIDR范围)来定义策略。

> **说明**：网络策略通过[网络插件](https://kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)来实现。要使用网络策略，我们必须使用支持NetworkPolicy的网络解决方案。只是创建一个NetworkPolicy资源对象而没有控制器来使它生效的话，是没有任何作用的。

#### 14.1 隔离和非隔离的Pod

默认情况下，Pod是非隔离的，它们接受任何来源的流量。

Pod在被某NetworkPolicy选中时进入被隔离状态。一旦名称空间中有NetworkPolicy选择了特定的Pod，该Pod就会拒绝该NetworkPolicy所不允许的连接(名称空间下其他未被NetworkPolicy所选择的Pod会继续接受所有的流量)。

网络策略是不会冲突的，因为它们是累积的。如果任何一个或者多个策略选择了一个Pod，则该Pod受限于这些策略的入站(Ingress)/出站(Egress)规则的并集，因此评估的顺序并不会影响策略的结果。

为了允许两个Pod之间的网络数据流，源端Pod上的出站(Egress)规则和目标端Pod上的入站(Ingress)规则都需要允许该流量。如果源端的出站(Egress)规则或目标端的入站(Ingress)规则拒绝该流量，则流量将被拒绝。

#### 14.2 NetworkPolicy资源

下面是一个NetworkPolicy的示例:

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978
```

> **说明**：除非选择支持网络策略的网络解决方案，否则将上述示例发送到API服务器是没有任何效果的。

==必选字段==：与所有其他的Kubernetes配置一样，NetworkPolicy需要`apiVersion`、`kind`和`metadata`字段。

==spec==：NetworkPolicy规约中包含了在一个名称空间中定义特定网络策略所需的所有信息。

==podSelector==：每个NetworkPolicy都包括一个`podSelector`，它对该策略所适用的一组Pod进行选择。示例中的策略选择带有"role=db"标签的Pod，空的`podSelector`则选择名称空间下的所有Pod。

==policyTypes==: 每个 NetworkPolicy 都包含一个 `policyTypes` 列表，其中包含 `Ingress`、`Egress` 或者两者兼具。`policyTypes` 字段表示给定的策略是应用于进入所选Pod的入站流量还是来自所选Pod的出站流量，或两者兼有。如果NetworkPolicy未指定`policyTypes`则默认情况下始终设置 `Ingress`，如果NetworkPolicy有任何出口规则的话则设置`Egress`。

==ingress==: 每个NetworkPolicy可包含一个`ingress`规则的白名单列表。每个规则都允许同时匹配`from`和`ports`部分的流量。示例策略中包含一条简单的规则：它匹配某个特定端口，来自三个来源中的一个，第一个通过`ipBlock`指定，第二个通过`namespaceSelector`指定，第三个通过`podSelector`指定。

==egress==: 每个NetworkPolicy可包含一个`egress`规则的白名单列表。每个规则都允许匹配`to`和`port`部分的流量。该示例策略包含一条规则：该规则将指定端口上的流量匹配到`10.0.0.0/24`中的任何目的地。

所以以上网络策略示例会达到以下效果:

- 隔离默认名称空间下带有`role=db`标签的Pod(如果它们不是已经被隔离的话)。

- 案例中的**Ingress规则**允许以下Pod连接到默认名称空间下带有`role=db`标签的所有Pod的6379的TCP端口：
  - 默认名称空间下带有`role=frontend`标签的所有Pod；
  - 带有`project=myproject`标签的所有名称空间下的Pod；
  - IP地址范围为172.17.0.0-172.17.0.255和172.17.2.0-172.17.255.255(即除了172.17.1.0/24之外的所有172.17.0.0/16)的所有Pod。

- 案例中的**Egress规则**允许从带有`role=db`标签的名称空间下的任何Pod到CIDR 10.0.0.0/24下5978的TCP端口的连接。 

#### 14.3 选择器<font color="croci">to</font>和<font color="croci">from</font>的行为

可以在`ingress`的`from`属性或`egress`的`to`属性中指定四种选择器：

**podSelector**: 此选择器将在与NetworkPolicy相同的名称空间中选择特定的Pod，应该允许它作为入站流量来源或出站流量目的地。

**namespaceSelector**：此选择器将选择特定的名称空间，所有Pod都应该被允许作为入站流量来源或出站流量目的地。

**namespaceSelector**和**podSelector**：这两个可以同时使用，不过要注意正确使用。下面是一种使用方式：

```yaml
  ...
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
      podSelector:
        matchLabels:
          role: client
  ...
```

> **说明**：以上案例，在`from`数组中仅包含一个元素，意思是只允许来自带有`role=client`标签的Pod且该Pod所在的名称空间要带有`user=alice`标签。

下面是另一种使用方式：

```yaml
  ...
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
    - podSelector:
        matchLabels:
          role: client
  ...
```

> **说明**：以上案例，在`from`数组中包含两个元素，意思是允许来自相同名称空间下带有`role=client`标签的Pod，或来自任何名称空间下带有`user=alice`的Pod。

**ipBlock**: 此选择器将选择特定的IP CIDR范围以用作入站流量来源或出站流量目的地。这些应该是集群外部IP，因为Pod IP存在时间短暂且随机产生。

集群的入站和出站机制通常需要重写数据包的源IP或目标IP。在发生这种情况时，不确定在NetworkPolicy处理之前还是之后发生，并且对于网络插件、云提供商、`Service`实现等的不同组合，其行为可能会有所不同。

对入站流量而言，这意味着在某些情况下，可以根据实际的原始源IP过滤传入的数据包，而在其他情况下NetworkPolicy所作用的`源IP`则可能是`LoadBalancer`或Pod的节点等。

对于出站流量而言，这意味着从Pod到被重写为集群外部IP的`Service` IP的连接可能会或可能不会受到基于`ipBlock`的策略的约束。

#### 14.4 默认策略

默认情况下，如果名称空间中不存在任何策略，则所有进出该名称空间中Pod的流量都被允许。以下示例使我们可以更改该名称空间中的默认行为。

##### 14.4.1 默认拒绝所有入站流量

我们可以通过创建选择所有容器但不允许任何进入这些容器的入站流量的NetworkPolicy来为名称空间创建"default" 隔离策略。

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress
```

这样可以确保即使容器没有选择其他任何NetworkPolicy，也仍然可以被隔离。此策略不会更改默认的出口隔离行为。

##### 14.4.2 默认允许所有入站流量

如果要允许所有流量进入某个名称空间中的所有Pod(即使添加了导致某些Pod被视为"隔离"的策略)，则可以创建一个策略来明确允许该名称空间中的所有流量。

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-ingress
spec:
  podSelector: {}
  ingress:
  - {}
  policyTypes:
  - Ingress
```

##### 14.4.3 默认拒绝所有出站流量

我们可以通过创建选择所有容器但不允许来自这些容器的任何出站流量的NetworkPolicy来为名称空间创建"default" egress隔离策略。

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-egress
spec:
  podSelector: {}
  policyTypes:
  - Egress
```

此策略可以确保即使没有被其他任何NetworkPolicy选择的Pod也不会被允许流出流量。此策略不会更改默认的入站流量隔离行为。

##### 14.4.4 默认允许所有出站流量

如果要允许来自名称空间中所有Pod的所有流量(即使添加了导致某些Pod被视为"隔离"的策略)，则可以创建一个策略，该策略明确允许该名称空间中的所有出站流量。

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress
spec:
  podSelector: {}
  egress:
  - {}
  policyTypes:
  - Egress
```

##### 14.4.5 默认拒绝所有入站和所有出站流量

我们可以为名称空间创建默认策略，可以通过在该名称空间中创建以下NetworkPolicy来阻止所有入站和出站流量。

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
```

此策略可以确保即使没有被其他任何NetworkPolicy选择的Pod也不会被允许入站或出站流量。

#### 14.5 针对某个端口范围

在编写NetworkPolicy时，我们可以针对一个端口范围而不是某个固定端口。

这一目的可以通过使用`endPort`字段来实现，如下例所示：

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: multi-port-egress
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 32000
      endPort: 32768
```

上面的规则允许`default`名称空间中所有带有`role=db`标签的Pod使用TCP协议与`10.0.0.0/24`范围内的IP通信，只要目标端口介于32000和32768之间就可以。

使用用`endPort`字段时存在以下限制：

- 作为一种Alpha阶段的特性，端口范围设定默认是被禁用的。要在整个集群范围内允许使用`endPort`字段，我们需要为API server设置`-feature-gates=NetworkPolicyEndPort=true,...`以启用`NetworkPolicyEndPort`[特性门控](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/feature-gates/)。
- `endPort`字段的值必须等于或者大于`port`字段的值。
- 两个字段的设置值都只能是数字。

> **说明**：我们的集群所使用的[CNI](https://kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni)插件必须支持在NetworkPolicy规约中使用`endPort`字段。

#### 14.6 基于名称指向某名称空间

只要`NamespaceDefaultLabelName`[特性门控](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/feature-gates/)被启用，Kubernetes控制面会在所有名称空间上设置一个不可变更的标签 `kubernetes.io/metadata.name`，该标签的值是名称空间的名称。

如果NetworkPolicy无法在某些对象字段中指向某名称空间，我们可以使用标准的标签方式来指向特定名称空间。

#### 14.7 网络策略案例演示

1. 创建一个deployment并暴露80端口：

   ```shell
   #通过deployment创建pod，创建成功后Pod会被默认打上一个"app=nginx"的标签，这个后面会用到
   kubectl create deployment nginx --image=nginx:1.20
   
   #暴露80端口
   kubectl expose deployment nginx --port=80
   ```

   ![image-20210721094951494](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210721135345.png)

   > **说明**：上述命令通过Deployment创建了一个镜像为nginx的Pod，并将之通过名为`nginx`的Service暴露了出来。Pod和Service都位于`default`名称空间内。

2. 通过启动一个busybox容器的方式来访问上面创建的Service：

   ```shell
   #启动一个busybox容器
   kubectl run busybox --rm -ti --image=busybox /bin/sh
   
   #运行以下命令进行访问
   wget --spider --timeout=1 nginx
   ```

   ![image-20210721095910595](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210721135352.png) 

3. 给以上创建的镜像为nginx的Pod创建一个名为nginx-policy.yaml的网络策略，只让那些拥有`access=true`标签的Pod才能访问到它：

   ```yaml
   apiVersion: networking.k8s.io/v1
   kind: NetworkPolicy
   metadata:
     name: access-nginx
   spec:
     podSelector:
       matchLabels:
         app: nginx
     ingress:
     - from:
       - podSelector:
           matchLabels:
             access: "true"
   ```

   > **说明**：上面的策略针对的是带有`app=nginx`标签的Pod，此标签是被自动添加到上面通过Deployment创建的Pod上的。如果`podSelector`为空，则意味着选择的是名称空间中的所有Pod。

4. 通过`kubectl apply -f nginx-policy.yaml`命令基于yaml文件创建资源；

5. 测试没有定义访问标签时访问服务，请求将会超时：

   ```shell
   kubectl run busybox --rm -ti --image=busybox -- /bin/sh
   wget --spider --timeout=1 nginx
   ```
   
   ![123356](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210721135358.jpg)  

6. 定义访问标签后再次测试，就可以访问到了：

   ```shell
   kubectl run busybox --rm -ti --labels="access=true" --image=busybox -- /bin/sh
   wget --spider --timeout=1 nginx
   ```

   ![36890](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210721135419.jpg)  

> **注意**：如果我们在安装k8s集群的时候使用的是Flannel网络插件，那么是否配置网络策略应该是没有差别的，因为该插件不支持网络策略，而像Calico等网络插件则是支持网络策略的。

### 15.k8s知识拓展

#### 15.1 列出容器的镜像

- 我们可以使用如下命令列出集群中所有名称空间下Pod的镜像，并进行排序计数：

  ```shell
  kubectl get pod -A -o jsonpath="{.items[*].spec.containers[*].image}"|tr -s '[[:space:]]' '\n'|sort|uniq -c
  ```

  ![image-20210721223136849](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210722010701.png) 

  > **说明**：以上截图第一列的数字就是用来计数的，比如最后一行的意思就是，集群中所有名称空间下使用`tomcat:8`镜像的Pod一共有三个。

- 我们也可以将镜像对应的名称空间和Pod名称都展示出来，以便定位到该镜像是哪个名称空间下的Pod在使用：

  ```shell
  kubectl get pod -A -o=jsonpath='{range .items[*]}{"\n["}{.metadata.namespace}{"]\t"}{.metadata.name}{":\t"}{range .spec.containers[*]}{.image}{"  "}{end}{end}'|sort
  ```

  ![image-20210721225220341](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210722010708.png) 

- 我们还可以使用标签来过滤，比如我们想要查出所有名称空间下包含`app`标签的Pod的镜像：

  ```shell
  kubectl get pod -A -o jsonpath="{.items[*].spec.containers[*].image}" -l app|tr -s '[[:space:]]' '\n'|sort
  ```

  ![image-20210721225739240](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210722010715.png) 

- 以上都是使用的`jsonpath`选项来过滤的，我们也可以使用`go-template`选项：

  ```shell
  kubectl get pods -A -o go-template --template="{{range .items}}{{range .spec.containers}}{{.image}} {{end}}{{end}}"|tr -s '[[:space:]]' '\n'|sort|uniq -c
  ```

  ![image-20210721230454115](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210722010720.png) 

#### 15.2 给Pod的hosts添加内容

- 先运行一个Pod，这里使用的是nginx镜像：

  ```shell
  kubectl run nginx-test --image=nginx:1.20
  ```

  ![image-20210721231306742](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210722010727.png) 

- 然后我们查看一下默认情况下Pod中`/etc/hosts`文件的内容：

  ```shell
  kubectl exec nginx-test -- cat /etc/hosts
  ```

  ![image-20210721231511337](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210722010735.png) 

- 接下来我们创建一个hostaliases-pod.yaml文件，基于该文件创建Pod的时候，文件中会通过`hostAliases`属性向Pod的`hosts`中添加内容，文件内容如下：

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: hostaliases-pod
  spec:
    restartPolicy: Never
    hostAliases:
    - ip: "127.0.0.1"
      hostnames:
      - "foo.local"
      - "bar.local"
    - ip: "10.1.2.3"
      hostnames:
      - "foo.remote"
    containers:
    - name: cat-hosts
      image: nginx:1.20
      command: ["cat","/etc/hosts"]
  ```

- 使用`kubectl apply -f hostaliases-pod.yaml`命令基于yaml文件创建资源；

- 使用`kubectl logs hostaliases-pod`命令查看内容是否已经添加到Pod的hosts中了：

  ![image-20210721233405561](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210722010741.png) 

#### 15.3 升级kubeadm集群

- 首先查看目前集群的版本，我这边版本是`v1.18.0`，如下所示：

  ![image-20210721234951547](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210722010746.png)  

- 然后使用如下命令查看目前可升级的版本情况：

  ```shell
  yum list --showduplicates kubeadm --disableexcludes=kubernetes
  ```

  ![image-20210722001322542](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210722010752.png) 

- 接下来就是下载一个我们想要升级的版本，好像是不能跨版本升级，所以这边就升级到`1.19.0-0`版本：

  ```shell
  yum install -y kubeadm-1.19.0-0 --disableexcludes=kubernetes
  ```

- 下载完成之后可以通过`kubeadm version`命令验证版本是否正确：

  ![image-20210722001511205](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210722010758.png)  

- 验证完版本后，可以通过`kubeadm upgrade plan`命令验证升级计划，会打印很多东西，这里就不截图了；

- 然后我们可以执行`kubeadm upgrade apply v1.19.0`命令进行版本的升级，然后可能会需要等待一段时间，一段时间后打印如下内容说明升级成功：

  ![image-20210722002757814](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210722010811.png) 

- 我们需要查看一下自己使用的网络插件是否需要升级，如果需要，也是要升级一下的；

- 如果有多个master节点的话，我们可以使用`kubeadm upgrade node`命令升级，而不是`kubeadm upgrade apply`命令。此外，并不需要执行`kubeadm upgrade plan`命令和更新CNI驱动插件的操作；

- 接下来通过将节点标记为不可调度并腾空节点为节点作升级准备：

  ```shell
  #下面的master是节点的名称
  kubectl drain master --ignore-daemonsets
  ```

- 下面开始升级kubelet和kubectl：

  ```shell
  yum install -y kubelet-1.19.0-0 kubectl-1.19.0-0 --disableexcludes=kubernetes
  ```

- 执行完以上命令后，重启kubelet：

  ```shell
  systemctl daemon-reload
  systemctl restart kubelet
  ```

- 然后解除节点的保护，通过将节点标记为可调度，让其重新上线：

  ```shell
  #下面的master是节点的名称
  kubectl uncordon master
  ```

- 然后我们使用`kubectl get node`命令可以发现，master节点的版本已经变成`v1.19.0`了，但是worker节点的版本还是之前的，所以下面我们需要来一个一个地升级worker节点的版本：

  ![image-20210722003958791](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210722010816.png) 

- 在每个worker节点上依次执行以下命令：

  ```shell
  yum install -y kubeadm-1.19.0-0 --disableexcludes=kubernetes
  kubeadm upgrade node
  ```

- 然后下面需要将节点标记为不可调度并驱逐所有负载，准备节点的维护：

  ```shell
  #在master节点上依次执行以下命令
  kubectl drain node1 --ignore-daemonsets
  kubectl drain node2 --ignore-daemonsets
  ```

- 然后升级worker节点上的kubelet。如果worker节点上之前有安装kubectl的话，这里也可以一并升级，我这边没有，所以就只升级kubelet：

  ```shell
  #依次在每个worker节点上执行以下命令
  yum install -y kubelet-1.19.0-0 --disableexcludes=kubernetes
  ```

- 执行完以上命令后，重启kubelet：

  ```shell
  #依次在每个worker节点上执行以下命令
  systemctl daemon-reload
  systemctl restart kubelet
  ```

- 然后解除对节点的保护，通过将节点标记为可调度，让节点重新上线：

  ```shell
  #在master节点上执行以下命令
  kubectl uncordon node1 && kubectl uncordon node2
  ```

- 最后，我们可以通过`kubectl get node`命令验证集群版本是否都已升级成功，并都处于`Ready`状态：

  ![image-20210722010539975](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210722010825.png) 

#### 15.4 修改节点可调度Pod的限制

在k8s集群中，集群节点上默认情况下最多只能创建110个Pod，可以根据如下命令查看：

```shell
kubelet -h|grep "max-pods int32"
```

![image-20210723140005790](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210724175934.png) 

如果我们觉得这个默认值过大或者过小，也可以手动去修改。修改步骤如下：

- 在对应的worker节点上使用`vim /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf`命令打开相应的配置文件，配置文件内容如下：

  ```shell
  # Note: This dropin only works with kubeadm and kubelet v1.11+
  [Service]
  Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
  Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
  # This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
  EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
  # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
  # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
  EnvironmentFile=-/etc/sysconfig/kubelet
  ExecStart=
  ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
  ```

- 然后加上相应的配置后保存退出：

  ```shell
  #我这边是把最大值设置成了20
  Environment="KUBELET_NODE_MAX_PODS=--max-pods=20"
  ```

  ![image-20210723140613456](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210724175943.png)

  > **说明**：加上相应的配置后，文末要加上`$KUBELET_NODE_MAX_PODS`，否则不会生效。

- 最后再依次执行以下命令使配置生效：

  ```shell
  systemctl daemon-reload
  systemctl restart kubelet
  ```

- 我们也可以使用`ps -ef|grep kubelet`命令查看一下是否生效：

  ![image-20210723141408645](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210724175951.png) 

- 配置生效后，我们可以创建一个deployment来进行测试，比如像下面这样：

  ```yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: nginx
  spec:
    replicas: 5
    selector:
      matchLabels:
        app: nginx-label
    template:
      metadata:
        labels:
          app: nginx-label
      spec:
        nodeName: node1  #这里的意思是，让Pod都调度到node1节点上
        containers:
          - name: task-pv-container
            image: gongcqq/myapp:3.0
            imagePullPolicy: IfNotPresent
            resources:
              limits:
                cpu: 20m
                memory: 32212254720m
              requests:
                cpu: 20m
                memory: 32212254720m
  ```

- 然后通过`kubectl scale`命令对创建的deployment进行扩容，当node1节点上被调度的Pod数超过20个的时候，就会报下面的错：

  ![image-20210723142113945](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210724175959.png) 

  > **说明**：通过截图的`OutOfpods`报错可以清晰地看出，报错原因是节点上的Pod数大于可以被调度的数量导致的。

### 16.其他相关知识点

#### 16.1 部署harbor集群

这里主要在两个不同的主机上分别搭建一个harbor仓库，然后通过双主复制实现harbor的高可用。具体步骤如下：

1. 分别在两个主机上下载并解压harbor的压缩文件：

   ```shell
   #下载压缩包
   wget https://github.com/goharbor/harbor/releases/download/v2.3.1/harbor-offline-installer-v2.3.1.tgz
   
   #解压压缩包
   tar -zxvf harbor-offline-installer-v2.3.1.tgz
   ```

   ![image-20210724200818699](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210725110145.png) 

2. 解压后会出现一个harbor目录，可以通过`cd harbor`命令进入到harbor目录中，里面有一个`harbor.yml.tmpl`文件，我们需要将两个主机上的该文件都重命名：

   ```shell
   mv harbor.yml.tmpl harbor.yml
   ```

3. 重命名后我们需要通过`vim harbor.yml`命令对两个主机上的harbor.yml文件都进行修改。以下是修改前后对比：

   - **修改前：**

     ![image-20210724221811545](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210725110152.png) 

   - **修改后：**

     ![image-20210724222357196](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210725110157.png)   

4. 上一步修改完成后记得保存退出，然后通过`vim /etc/docker/daemon.json`命令修改两个主机上的daemon.json文件，在该文件内添加以下内容后保存退出：

   ```json
   //下面的IP是harbor所在宿主机的IP，不同主机这里的IP是不一样的
   "insecure-registries":["192.168.68.20:8085"]
   ```

   > **说明**：修改完daemon.json文件后记得使用`systemctl restart docker`命令重启下docker使之生效。

5. 由于启动harbor需要用到`docker-compose`，所以在启动前需要先把它下载好，github上下载比较慢，所以可以使用如下的方式进行快速下载：

   ```shell
   #在两台主机上都执行以下命令
   curl -L https://get.daocloud.io/docker/compose/releases/download/1.28.4/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose
   ```

6. 下载完成后，分别在两台主机上使用如下命令进行授权：

   ```shell
   chmod +x /usr/local/bin/docker-compose
   ```

   > **说明**：上面的步骤完成后，我们可以使用`docker-compose -v`验证下docker-compose是否下载安装成功。

7. 接下来直接在两个主机的harbor目录中都使用`./install.sh`分别启动各自的harbor即可，这一步要下载很多镜像文件，所以可能需要较长的等待时间。当打印如下信息的时候，说明启动成功：

   ![image-20210724222646268](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210725110203.png) 

8. 启动成功后，我们可以通过IP地址加端口的方式在浏览器进行访问，比如`http://192.168.68.21:8085/`，登录账号是`admin`，密码默认是`Harbor12345`，登录进去后界面如下：

   ![image-20210724223139277](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210725110207.png)

   > **说明**：两个主机上的harbor都安装完成之后，下面就开始通过nginx对两个harbor仓库进行负载。由于我们平常都是在k8s集群中从harbor仓库下载镜像的，所以这个nginx就通过deployment创建Pod的方式部署在k8s集群中好了，然后使用configMap管理配置。

9. 在k8s集群的master节点上使用如下命令创建一个用于harbor负载的配置文件：

   ```shell
   cat > /data/nginx.conf << EOF
   error_log /var/log/nginx/error.log warn;
   
   events {
       worker_connections 1024;
   }
   
   stream {
       
       upstream harbor {
           server 192.168.68.20:8085;
           server 192.168.68.21:8085;
       }
       server {
           listen 80;
           proxy_pass harbor;
           proxy_timeout 300s;
           proxy_connect_timeout 100s; 
       }
   
   }
   EOF
   ```

10. 然后使用以下命令基于文件创建一个configMap：

    ```shell
    #为了隔绝资源，在创建configMap前先创建一个名称空间
    kubectl create ns harbor-nginx
    
    #在执行名称空间下基于文件创建一个configMap
    kubectl create configmap harbor-nginx -n harbor-nginx --from-file=/data/nginx.conf
    ```

11. 之后再创建一个使用nginx镜像的Pod，并将上面创建的configMap挂载到Pod中去，相关的yaml文件内容如下：

    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: harbor
      namespace: harbor-nginx
      labels:
        app: harbor
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: harbor
      template:
        metadata:
          labels:
            app: harbor
        spec:
          containers:
          - name: c-harbor
            image: nginx:1.20
            imagePullPolicy: IfNotPresent
            volumeMounts:
            - name: config
              mountPath: "/etc/nginx"
              readOnly: true
          volumes:
          - name: config
            configMap:
              name: harbor-nginx
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: harbor
      namespace: harbor-nginx
    spec:
      selector:
        app: harbor
      ports:
        - protocol: TCP
          port: 80
          targetPort: 80
    ```

    ![image-20210725012719027](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210725110216.png) 

12. 我们需要将以上截图中的service的IP配置到集群中所有需要拉取镜像的节点的`/etc/docker/daemon.json`文件中，就像下面这样：

    ```json
    "insecure-registries":["10.104.240.36"]
    ```

    配置完成后通过`systemctl restart docker`命令重启docker服务使之生效。

13. 我们随便登录一个harbor仓库，比如20主机上的仓库，然后通过**新建项目**的方式给这个仓库建一个项目，下面的步骤中，镜像就会推送到这个项目下用于测试。

    ![image-20210725100117072](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210725110221.png) 

    ![image-20210725100135759](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210725110230.png)

14. 下面需要配置harbor仓库双主复制的规则，我这边就在20主机上配置了，首先在左侧`仓库管理`菜单栏里面新建一个目标，目标的url就是另一台harbor仓库的地址，信息填好后我们可以**测试连接**一下。

    ![image-20210725102127890](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210725110235.png)

    ![image-20210725102255756](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210725110240.png) 

15. 上一步创建完目标后，这一步就在`复制管理`里面新建一个复制规则即可，如下所示：

    ![image-20210725102927915](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210725110245.png) 

    > **注意**：我这边演示的是在20主机的harbor仓库上新建目标和复制规则，这个规则是把20主机的镜像复制到21主机上。但是也有可能我们是把镜像推送到21主机的harbor仓库上，所以我们也要在21主机新建一个目标和复制规则，这个规则是指把21主机上的镜像复制到20主机上，这个配置方式和上面截图的是一样的，所以这里就不再截图演示了。

17. 然后现在在20主机上推送一个镜像进行测试，看镜像是否会自动同步到21主机：

    ```shell
    #先登录进对应的harbor仓库，比如下面这样
    docker login 192.168.68.20:8085
    
    #然后我们先从hub仓库下载一个用于测试的镜像
    docker pull busybox:1.30
    
    #下面给这个镜像打个标签
    docker tag busybox:1.30 192.168.68.20:8085/test/busybox:1.30
    
    #然后推送镜像到harbor仓库
    docker push 192.168.68.20:8085/test/busybox:1.30
    ```

18. 我这边已经同步成功了，自动在21主机上新建了一个和20主机一样的`test`项目，然后镜像也同步到这个项目下了。

    ![image-20210725104521476](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210725110250.png) 

    我们也可以到20主机的`复制管理`里面查到到复制记录。

    ![image-20210725104126668](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210725110255.png) 

19. 仓库里面有了镜像后，现在就在k8s集群中拉取镜像试试：

    ```shell
    #下面的10.104.240.36是k8s中service的ip
    docker pull 10.104.240.36/test/busybox:1.30
    ```

    ![image-20210725105112715](https://cdn.jsdelivr.net/gh/gongcqq/FigureBed@main/Image/Typora/20210725110301.png) 

    > **说明**：由以上截图可知，镜像已经拉取成功了。我们还可以将某一个harbor仓库所在主机的harbor停掉，看看在k8s集群中通过`docker pull`命令还能拉取到镜像不，以此验证nginx的负载确实是没有问题的。我这边已经测试过了，停掉一个还是可以拉取到镜像的。

#### 16.2 搭建k8s日志解决方案

**说明**：这里主要是通过`log-pilot` + `elasticsearch` + `kibana`搭建一个kubernetes的日志解决方案。具体详情可以参考[这里](https://help.aliyun.com/document_detail/68264.html?spm=5176.12818093.help.dexternal.5adc16d0ZFWct3)。

##### 16.2.1 log-pilot介绍

log-Pilot是一个智能容器日志采集工具，它不仅能够高效便捷地将容器日志采集输出到多种存储日志后端，同时还能够动态地发现和采集容器内部的日志文件。

log-pilot通过声明式配置实现强大的容器事件管理，可同时获取容器标准输出和内部文件日志，解决了动态伸缩问题，此外，log-pilot具有自动发现机制，CheckPoint及句柄保持的机制，自动日志数据打标，有效应对动态配置、日志重复和丢失以及日志源标记等问题。

目前 log-pilot在Github完全开源，我们也可以访问[项目地址](https://github.com/AliyunContainerService/log-pilot)来深入了解更多实现原理。

##### 16.2.2 针对容器日志的声明式配置

Log-Pilot支持容器事件管理，它能够动态地监听容器的事件变化，然后依据容器的标签来进行解析，生成日志采集配置文件，然后交由采集插件来进行日志采集。

在kubernetes下，Log-Pilot可以依据环境变量`aliyun_logs_$name = $path` 动态地生成日志采集配置文件，其中包含两个变量：

- `$name`：该变量是我们自定义的一个字符串，它在不同的场景下指代不同的含义，在本场景中，将日志采集到es的时候，这个$name表示的是Index。
- `$path`：该变量支持两种输入形式，stdout和容器内部日志文件的路径，对应日志标准输出和容器内的日志文件。
  - 第一种约定关键字stdout表示的是采集容器的标准输出日志，如本例中我们要采集tomcat容器日志，那么我们可以通过配置标签`aliyun.logs.catalina=stdout`来采集tomcat标准输出日志，也可以通过环境变量的方式。
  - 第二种是容器内部日志文件的路径，支持通配符，可以通过配置环境变量的方式来采集tomcat容器内部的日志。当然如果我们不想使用aliyun这个关键字，Log-Pilot也提供了环境变量PILOT_LOG_PREFIX可以指定自己的声明式日志配置前缀，比如`PILOT_LOG_PREFIX: "aliyun,custom"`。

此外，Log-Pilot还支持多种日志解析格式，通过`aliyun_logs_$name_format=<format>`标签就可以告诉Log-Pilot 在采集日志的时候，同时以什么样的格式来解析日志记录，支持的格式包括：none、json、csv、nginx、apache2 和 regxp。

Log-Pilot同时支持自定义tag，我们可以在环境变量里配置`aliyun_logs_$name_tags="K1=V1,K2=V2"`，那么在采集日志的时候也会将 K1=V1 和 K2=V2 采集到容器的日志输出中。自定义tag可帮助我们给日志产生的环境打上tag，方便进行日志统计、日志路由和日志过滤。

##### 16.2.3 部署elasticsearch

```shell
# 下载elasticsearch.yaml文件
wget https://gitee.com/gongcqq/others/raw/master/elasticsearch.yaml

# 通过yaml方式部署elasticsearch
kubectl apply -f elasticsearch.yaml

# 查看运行状态
kubectl get pod -n kube-system -l app=es -o wide
```

![image-20210815123815752](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815123815752.png) 

##### 16.2.4 部署log-pilot

```shell
# 下载log-pilot.yaml文件
wget https://gitee.com/gongcqq/others/raw/master/log-pilot.yaml

# 通过yaml方式部署log-pilot
kubectl apply -f log-pilot.yaml

# 查看运行状态
kubectl get pod -n kube-system -l k8s-app=log-es -o wide
```

![image-20210815124202798](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815124202798.png) 

##### 16.2.5 部署kibana服务

```shell
# 下载kibana.yaml文件
wget https://gitee.com/gongcqq/others/raw/master/kibana.yaml

# 通过yaml方式部署kibana
kubectl apply -f kibana.yaml

# 查看运行状态
kubectl get pod,svc -n kube-system -l component=kibana -o wide
```

![image-20210815124530912](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815124530912.png) 

##### 16.2.6 部署测试应用tomcat

在elasticsearch + log-pilot + Kibana这套日志工具部署完毕后，现在开始部署一个日志测试应用tomcat，来测试日志是否能正常采集、索引和显示。

```shell
# 下载deploy-demo文件
wget https://gitee.com/gongcqq/others/raw/master/deploy-demo.yaml

# 通过yaml方式部署tomcat
kubectl apply -f deploy-demo.yaml

# 查看运行状态
kubectl get pod -l app=tomcat-demo -o wide
```

![image-20210815124843129](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815124843129.png) 

在deploy-demo.yaml文件中，增加了如下环境变量：

```yaml
env:
- name: aliyun_logs_catalina
  value: "stdout"
- name: aliyun_logs_access
  value: "/usr/local/tomcat/logs/*"
```

这里主要是通过在pod中定义环境变量的方式，动态地生成日志采集配置文件，环境变量的具体说明如下：

- `aliyun_logs_catalina=stdout`：表示要收集容器的stdout日志。
- `aliyun_logs_access=/usr/local/tomcat/logs/*`：表示要收集容器内/usr/local/tomcat/logs/目录下所有文件的日志。

在本方案的elasticsearch场景下，环境变量中的`$name`表示Index，本例中`$name`即是catalina和access。

##### 16.2.7 访问kibana服务

上面在部署kibana的时候，已经通过NodePort的方式将kibana服务暴露到公网了，我这边随机生成的端口是32194，我们可以通过`ip:port`的方式来访问kibana服务，从而查看日志采集情况。

访问进来以后默认是如下这样的界面：

![image-20210815130432621](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815130432621.png) 

接下来我们修改索引名为`catalina*`，然后点击create按钮：

![image-20210815130623378](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815130623378.png) 

创建成功后，默认会变成如下界面：

![image-20210815130702528](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815130702528.png) 

然后我们点击`Discover`可以看到具体的日志情况：

![image-20210815135249556](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815135249556.png) 

在tomcat启动成功后，日志中默认都会有"Server startup"这样的关键字，然后后面跟的是启动时间，我们可以搜索一下看看是否被采集到了：

![image-20210815140733495](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815140733495.png) 

> **说明**：由上图可知，根据关键字已经搜索出了具体内容，所以tomcat中的日志确实已经被采集到了。

#### 16.3 helm入门教程

Helm是Kubernetes的一个包管理工具，功能类似于linux中的yum。另外，我们需要根据Kubernetes集群的版本来选择对应版本的Helm，[helm官方](https://helm.sh/zh/docs/topics/version_skew/)提供的版本对应关系部分如下：

| Helm版本 | Kubernetes版本  |
| :------: | :-------------: |
|  3.6.x   | 1.21.x - 1.18.x |
|  3.5.x   | 1.20.x - 1.17.x |
|  3.4.x   | 1.19.x - 1.16.x |
|  3.3.x   | 1.18.x - 1.15.x |
|  3.2.x   | 1.18.x - 1.15.x |
|  3.1.x   | 1.17.x - 1.14.x |
|  3.0.x   | 1.16.x - 1.13.x |
|  2.16.x  | 1.16.x - 1.15.x |
|  2.15.x  | 1.15.x - 1.14.x |

##### 16.3.1 helm的安装

1. 首先我们要选择[需要的版本](https://github.com/helm/helm/tags)，我这边使用的是3.6.2版本，然后直接通过如下命令下载即可：

   ```shell
   wget https://get.helm.sh/helm-v3.6.2-linux-amd64.tar.gz
   ```

2. 下载完成后，解压文件：

   ```shell
   tar -zxvf helm-v3.6.2-linux-amd64.tar.gz
   ```

3. 解压完成后，会出现一个linux-amd64目录，我们需要将该目录下的二进制文件移动到指定路径下：

   ```shell
   mv linux-amd64/helm /usr/local/bin/helm
   ```

至此，helm就安装完成了，我们可以通过`helm version`命令查看helm版本号信息：

![image-20210815170932521](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815170932521.png) 

##### 16.3.2 helm仓库

类似linux中的yum源，helm也需要一个可用的helm仓库，所以helm安装完成后首先就是要添加一个稳定的仓库。直接使用如下命令添加仓库即可：

```shell
helm repo add bitnami https://charts.bitnami.com/bitnami
```

仓库添加完成之后，我们可以使用`helm repo list`命令进行查看：

![image-20210815172103978](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815172103978.png) 

如果我们想要删除仓库，可以使用`helm repo remove <仓库名>`命令进行移除，比如：`helm repo remove bitnami`。使用一段时间后，如果我们担心仓库内容和远程仓库不同步的话，可以使用`helm repo update`命令更新下。

仓库添加成功后，我们可以使用如下命令查看仓库中的资源：

```shell
helm search repo bitnami
```

![image-20210815174517364](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815174517364.png) 

##### 16.3.3 安装Chart示例

我们可以通过`helm install`命令来安装chart，如下所示：

```shell
# 下面的mynginx是自定义的名称
helm install mynginx bitnami/nginx
```

![image-20210815175513322](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815175513322.png) 

我们可以使用`helm list`命令或者`helm ls`命令查看安装的chart：

![image-20210815180100514](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815180100514.png) 

我们还可以使用k8s的kubectl命令查看资源的运行情况，可以发现，最终会以deployment的方式进行运行：

![image-20210815175937744](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815175937744.png) 

我们也可以使用如下命令进行卸载：

```shell
helm uninstall mynginx
```

卸载之后，之前下载的所以资源都将被删除：

![image-20210815180512589](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815180512589.png) 

##### 16.3.4 使用helm

###### 16.3.5.1 三大概念

- `Chart`：代表着helm包，它包含在Kubernetes集群内部运行应用程序、工具或服务所需的所有资源定义；
- `Repository`：它用来存放和共享charts的地方，它是供Kubernetes包所使用的一个仓库；
- `Release`：它是运行在Kubernetes集群中的chart的实例。一个chart通常可以在同一个集群中安装多次。每一次安装都会创建一个新的*release*。以MySQL chart为例，如果我们想在我们的集群中运行两个数据库，那我们可以安装该chart两次。每一个数据库都会拥有它自己的 *release* 和 *release name*。

在了解了上述这些概念以后，我们就可以这样来解释Helm：

Helm安装*charts*到k8s集群中，每次安装都会创建一个新的*release*。我们可以在Helm的chart *repositories* 中寻找新的chart。

###### 16.3.5.2 helm search：查找Charts

Helm自带一个强大的搜索命令，可以用来从两种来源中进行搜索：

- `helm search hub`：从[Artifact Hub](https://artifacthub.io/)中查找并列出helm charts，Artifact Hub中存放了大量不同的仓库；
- `helm search repo`：从我们添加(使用`helm repo add`)到本地helm客户端中的仓库中进行查找，该命令基于本地数据进行搜索，无需连接互联网。

比如我们要搜索wordpress，可以使用如下命令：

```shell
# 在公共仓库搜索
helm search hub wordpress

# 在添加到本地的仓库中搜索
helm search repo wordpress
```

![image-20210815183145519](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815183145519.png) 

###### 16.3.5.3 helm install：安装一个helm包

使用`helm install`命令来安装一个新的helm包。最简单的使用方法只需要传入两个参数：我们命名的release名字和我们想要安装的chart的名称，如下所示：

```shell
helm install nginx-test bitnami/nginx

# 如果想要自动生成一个名称，可以使用如下命令
helm install bitnami/nginx --generate-name
```

![image-20210815184702794](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815184702794.png) 

在安装过程中，`helm`客户端会打印一些有用的信息，其中包括：哪些资源已经被创建，release当前的状态，以及我们是否还需要执行额外的配置步骤等等。

Helm客户端不会等到所有资源都运行完成才退出，我们也可以通过`helm status <release-name>`命令来追踪release的状态。

我们在安装前也可以自定义chart，使用`helm show values`命令可以查看chart中的可配置选项，如下所示：

```shell
helm show values bitnami/nginx
```

我们可以自定义一个values.yaml文件，然后使用该文件中自定义的配置项覆盖官方默认配置项，然后在安装过程使用该文件即可，如下所示：

```shell
helm install -f values.yaml bitnami/nginx --generate-name
```

或者我们也可以使用`--set`选项通过命令行的方式对指定项进行覆盖，比如下面这样：

```shell
# 创建的service默认类型是LoadBalancer，这里是将其修改为NodePort类型
helm install mynginx bitnami/nginx --set service.type=NodePort
```

我们也可以使用`helm get values mynginx`命令查看是否生效：

![image-20210815200016994](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815200016994.png) 

###### 16.3.5.4 helm upgrade和 helm rollback

当我们想升级到chart的新版本，或是修改release的配置时，可以使用`helm upgrade`命令，如果是要回滚到之前的发布版本，可以使用`helm rollback`命令，具体操作如下：

```shell
# 先安装一个chart
helm install test-nginx bitnami/nginx

# 查看
helm list
```

![image-20210815201827616](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815201827616.png) 

```shell
# 设置副本数为2
helm upgrade test-nginx bitnami/nginx --set replicaCount=2
```

![image-20210815202146638](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815202146638.png) 

```shell
# 指定版本号进行回滚
helm rollback test-nginx 1
```

![image-20210815202638869](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815202638869.png) 

> **说明**：上面的命令将我们的`test-nginx`回滚到了它最初的版本。release版本其实是一个增量修订(revision)。每当发生了一次安装、升级或回滚操作，revision的值就会加1。第一次revision的值永远是1。

我们也可以使用`helm history`命令查看特定release的每个版本的历史记录：

```shell
helm history test-nginx
```

![image-20210815203120533](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815203120533.png) 

###### 16.3.5.5 helm uninstall：卸载 release

我们可以使用`helm uninstall`命令从集群中卸载一个release，比如下面这样：

```shell
helm uninstall test-nginx
```

在Helm 2版本中，当一个release被删除，会保留一条删除记录。而在Helm3 中，删除也会移除release的记录。 如果我们想要保留删除记录的话，可以使用`--keep-history`选项。然后我们可以使用`helm list --uninstalled`命令展示使用了`--keep-history`选项删除的release。`helm list -a`命令则会展示Helm保留的所有release记录，包括失败或删除的条目(即指定了`--keep-history`选项）。我们可以使用`helm delete`命令删除使用`--keep-history`选项保留的删除记录。

###### 16.3.5.6 helm repo：使用仓库

Helm 3不再附带一个默认的chart仓库。`helm repo`提供了一组命令用于添加、列出和移除仓库。

我们可以使用`helm repo list`来查看配置的仓库，使用`helm repo add`来添加新的仓库。

因为chart仓库经常在变化，所以我们可以在任何时候通过执行`helm repo update`命令来确保我们的Helm客户端是最新的。另外，我们可以使用`helm repo remove`命令来移除仓库。

###### 16.3.5.7 创建属于自己的 charts

[chart开发指南](https://helm.sh/zh/docs/topics/charts)介绍了如何开发我们自己的chart，但是我们也可以通过使用`helm create`命令来快速开始：

```shell
helm create gsl-test
```

```php
[root@k8s-01 ~]# tree gsl-test/
gsl-test/
├── charts                           # 包含chart依赖的其他chart
├── Chart.yaml                       # 包含了chart信息的yaml文件
├── templates                        # 模板目录，目录下的文件可通过变量的方式引用values.yaml中的内容
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── hpa.yaml
│   ├── ingress.yaml
│   ├── NOTES.txt                    # 可选: 包含简要使用说明的纯文本文件
│   ├── serviceaccount.yaml
│   ├── service.yaml
│   └── tests
│       └── test-connection.yaml
└── values.yaml                      # chart 默认的配置值
```

执行完以上`helm create`命令后，就会在当前目录创建一个**gsl-test**目录，并且在该目录下默认会有一个chart，我们可以编辑它并创建我们自己的模版。

在编辑chart时，可以通过`helm lint`验证格式是否正确，当准备将chart打包分发时，可以运行`helm package`命令：

```shell
helm package gsl-test
```

> **说明**：通过`helm package`命令打包后会生成一个[gsl-test-0.1.0.tgz](https://gitee.com/gongcqq/others/attach_files/802425/download/gsl-test-0.1.0.tgz)文件。

然后这个chart就可以很轻松地通过`helm install`命令进行安装了：

```shell
helm install gsl-test ./gsl-test-0.1.0.tgz
```

![image-20210815214706314](D:\Program Files (x86)\Typora\images\3.Kubernetes(k8s)入门教程\image-20210815214706314.png) 

> **说明**：对于打包好的chart包也可以上传到chart仓库中。

#### 16.4 



































































